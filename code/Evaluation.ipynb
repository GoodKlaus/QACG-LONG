{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d8137cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4c5efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from random import shuffle\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import collections\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model.CGBERT import *\n",
    "from model.QACGBERT import *\n",
    "from model.QACGLONG import *\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, precision_score, recall_score, cohen_kappa_score\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler, WeightedRandomSampler\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import LongformerTokenizer, LongformerConfig, LongformerForSequenceClassification\n",
    "\n",
    "from util.optimization import BERTAdam\n",
    "from util.processor import (Sentihood_NLI_M_Processor,\n",
    "                            Semeval_NLI_M_Processor,\n",
    "                            PersentV1_Processor,\n",
    "                            PersentV2_Processor,\n",
    "                            PersentV1_Para_Processor,\n",
    "                            PersentV2_Para_Processor,\n",
    "                            PersentV1_Longformer_Processor,\n",
    "                            PersentV2_Longformer_Processor\n",
    "                           )\n",
    "\n",
    "from util.tokenization import *\n",
    "\n",
    "from util.evaluation import *\n",
    "\n",
    "from util.train_helper import system_setups, getModelOptimizerTokenizer, convert_examples_to_features\n",
    "\n",
    "from util.processor import *\n",
    "\n",
    "from model.QACGLONG import *\n",
    "\n",
    "from util.args_parser import parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f969b",
   "metadata": {},
   "source": [
    "# Process logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4a0d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best otained strict accuracy is 0.3597938144329897\n",
      "The best otained test accuracy is 0.8470544918998527\n"
     ]
    }
   ],
   "source": [
    "def process_log(data):\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop(['s_auc'], axis=1, inplace=True)\n",
    "    data.columns = ['epoch', 'global_step', 'loss', 't_loss', 't_acc', 'strict_acc', 'f1', 'auc', 's_acc', 's_auc']\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    return data\n",
    "\n",
    "data1 = pd.read_csv('../results/persent_annotated/QACGBERT-7aspects-noNeuMix-30epochs_2e-5/log.txt', sep=\"\\t\")\n",
    "data1 = process_log(data1)\n",
    "\n",
    "df = data1.groupby(by=['epoch'], as_index=False).max()\n",
    "df['loss'] = df['loss']  * 8\n",
    "\n",
    "sns.lineplot(data=df, x=\"epoch\", y=\"f1\", color=\"red\", label=\"F1 score\")\n",
    "sns.lineplot(data=df, x=\"epoch\", y=\"strict_acc\", label=\"strict accuracy\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "print(f'The best otained strict accuracy is {max(df.strict_acc)}')\n",
    "print(f'The best otained test accuracy is {max(df.t_acc)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4d2c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>global_step</th>\n",
       "      <th>loss</th>\n",
       "      <th>t_loss</th>\n",
       "      <th>t_acc</th>\n",
       "      <th>strict_acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "      <th>s_acc</th>\n",
       "      <th>s_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>32500</td>\n",
       "      <td>0.030549</td>\n",
       "      <td>1.233507</td>\n",
       "      <td>0.843962</td>\n",
       "      <td>0.359794</td>\n",
       "      <td>0.490714</td>\n",
       "      <td>0.770757</td>\n",
       "      <td>0.717526</td>\n",
       "      <td>0.716904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  global_step      loss    t_loss     t_acc  strict_acc        f1  \\\n",
       "25     25        32500  0.030549  1.233507  0.843962    0.359794  0.490714   \n",
       "\n",
       "         auc     s_acc     s_auc  \n",
       "25  0.770757  0.717526  0.716904  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['strict_acc']==max(df.strict_acc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3e415",
   "metadata": {},
   "source": [
    "# Aspect level visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3de47",
   "metadata": {},
   "source": [
    "## Obtain predictions from best trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ccf323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Same argument format as the ones in Trial Run.ipynb file. \n",
    "But remember to swap the init_checkpoint with the best_checkpoint.bin from trained cases.\n",
    "'''\n",
    "\n",
    "\n",
    "args_init = ['--task_name', 'persentv2', \n",
    "             '--data_dir', '../datasets/persent_annotated/',\n",
    "             '--output_dir', '../results/persent_annotated/QACGLONG-reproduce22/',\n",
    "             '--model_type', 'QACGLONG',\n",
    "             '--do_lower_case',\n",
    "             '--max_seq_length', '2048',\n",
    "             '--train_batch_size', '8',\n",
    "             '--eval_batch_size', '12',\n",
    "             '--learning_rate', '1e-4',\n",
    "             '--num_train_epochs', '30',\n",
    "             '--vocab_file', 'BERT-Google/vocab.txt',\n",
    "             '--bert_config_file', 'Longformer/config.json',\n",
    "             '--init_checkpoint', '../results/persent_annotated/QACGLONG-7aspects-noNeuMix-30epochs_2e-5/best_checkpoint.bin', \n",
    "             '--seed', '123',\n",
    "             '--evaluate_interval', '25',\n",
    "             '--gradient_accumulation_steps', '8']\n",
    "\n",
    "args = parser.parse_args(args_init)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0672d6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/25/2022 22:38:26 - INFO - util.train_helper -   device cuda n_gpu 1 distributed training False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_log_file= ../results/persent_annotated/QACGLONG-reproduce22/log.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/25/2022 22:38:32 - INFO - util.train_helper -   *** Model Config ***\n",
      "11/25/2022 22:38:32 - INFO - util.train_helper -   {\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"full_pooler\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"sep_token_id\": 2,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "11/25/2022 22:38:32 - INFO - util.train_helper -   model = QACGLONG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_weight = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/25/2022 22:38:34 - INFO - util.train_helper -   retraining with saved model.\n",
      "11/25/2022 22:38:34 - INFO - util.train_helper -   loading a best checkpoint, not BERT pretrain.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/persent_annotated/QACGLONG-7aspects-noNeuMix-30epochs_2e-5/best_checkpoint.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QACGBertForSequenceClassification1(\n",
       "  (bert): ContextBertModel1(\n",
       "    (embeddings): BERTEmbeddings1(\n",
       "      (word_embeddings): Embedding(50265, 768)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BERTLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ContextBERTEncoder1(\n",
       "      (context_layer): ModuleList(\n",
       "        (0): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (1): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (3): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (4): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (5): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (6): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (7): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (8): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (9): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (10): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (11): Linear(in_features=1536, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer): ModuleList(\n",
       "        (0): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): ContextBERTPooler1(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (context_embeddings): Embedding(2375, 768)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Set up model, optimizer, tokenizer and data\n",
    "'''\n",
    "\n",
    "\n",
    "device, n_gpu, output_log_file= system_setups(args)\n",
    "\n",
    "processors = {\n",
    "    \"sentihood_NLI_M\":Sentihood_NLI_M_Processor,\n",
    "    \"semeval_NLI_M\":Semeval_NLI_M_Processor,\n",
    "    \"persentv1\":PersentV1_Processor,\n",
    "    \"persentv2\":PersentV2_Processor,\n",
    "    \"persentv1_para\":PersentV1_Para_Processor,\n",
    "    \"persentv2_para\":PersentV2_Para_Processor,\n",
    "    \"persentv1_longformer\":PersentV1_Longformer_Processor,\n",
    "    \"persentv2_longformer\":PersentV2_Longformer_Processor\n",
    "}\n",
    "\n",
    "processor = processors[args.task_name]()\n",
    "label_list = processor.get_labels()\n",
    "train_examples = None\n",
    "num_train_steps = None\n",
    "train_examples = processor.get_train_examples(args.data_dir)\n",
    "num_train_steps = int(len(train_examples) / args.train_batch_size * args.num_train_epochs)\n",
    "\n",
    "model1, optimizer, tokenizer = getModelOptimizerTokenizer(model_type=args.model_type,\n",
    "                                   vocab_file=args.vocab_file,\n",
    "                                   config_file=args.bert_config_file,\n",
    "                                   init_checkpoint=args.init_checkpoint,\n",
    "                                   label_list=label_list,\n",
    "                                   do_lower_case=True,\n",
    "                                   num_train_steps=num_train_steps,\n",
    "                                   learning_rate=args.learning_rate,\n",
    "                                   base_learning_rate=args.base_learning_rate,\n",
    "                                   warmup_proportion=args.warmup_proportion)\n",
    "\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44d0a50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7385/7385 [00:21<00:00, 340.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "\n",
    "test_examples = processor.get_combo_examples(args.data_dir)\n",
    "test_features = convert_examples_to_features(\n",
    "        test_examples, label_list, args.max_seq_length,\n",
    "        tokenizer, args.max_context_length,\n",
    "        args.context_standalone, args.task_name)\n",
    "\n",
    "input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "seq_len = torch.tensor([[f.seq_len] for f in test_features], dtype=torch.long)\n",
    "context_ids = torch.tensor([f.context_ids for f in test_features], dtype=torch.long)\n",
    "\n",
    "test_data = TensorDataset(input_ids, input_mask, segment_ids,\n",
    "                                label_ids, seq_len, context_ids)\n",
    "test_dataloader = DataLoader(test_data, batch_size=args.eval_batch_size, shuffle=False)\n",
    "\n",
    "context_ids_eval = torch.clone(context_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d2f7c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/616 [00:00<?, ?it/s]11/25/2022 22:39:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "/home/haoyu/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "Iteration:   0%|          | 1/616 [00:00<02:28,  4.13it/s]11/25/2022 22:39:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   0%|          | 2/616 [00:00<02:12,  4.64it/s]11/25/2022 22:39:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 411 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   0%|          | 3/616 [00:00<02:05,  4.89it/s]11/25/2022 22:39:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|          | 4/616 [00:00<02:09,  4.72it/s]11/25/2022 22:39:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|          | 5/616 [00:01<02:04,  4.93it/s]11/25/2022 22:39:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|          | 6/616 [00:01<02:00,  5.06it/s]11/25/2022 22:39:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|          | 7/616 [00:01<01:58,  5.14it/s]11/25/2022 22:39:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 210 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|▏         | 8/616 [00:01<01:57,  5.19it/s]11/25/2022 22:39:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|▏         | 9/616 [00:01<01:56,  5.23it/s]11/25/2022 22:39:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 561 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 10/616 [00:02<02:24,  4.20it/s]11/25/2022 22:39:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 564 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 11/616 [00:02<02:45,  3.67it/s]11/25/2022 22:39:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 12/616 [00:02<02:30,  4.01it/s]11/25/2022 22:39:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 13/616 [00:03<02:47,  3.60it/s]11/25/2022 22:39:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 14/616 [00:03<03:00,  3.34it/s]11/25/2022 22:39:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 1780 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 15/616 [00:04<04:08,  2.42it/s]11/25/2022 22:39:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 1781 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   3%|▎         | 16/616 [00:05<05:48,  1.72it/s]11/25/2022 22:39:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 1285 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   3%|▎         | 17/616 [00:05<06:10,  1.62it/s]11/25/2022 22:39:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   3%|▎         | 18/616 [00:05<04:55,  2.02it/s]11/25/2022 22:39:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:   3%|▎         | 19/616 [00:06<05:28,  1.82it/s]11/25/2022 22:39:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:   3%|▎         | 20/616 [00:07<06:44,  1.47it/s]11/25/2022 22:39:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   3%|▎         | 21/616 [00:07<05:22,  1.84it/s]11/25/2022 22:39:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 1106 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▎         | 22/616 [00:08<05:15,  1.88it/s]11/25/2022 22:39:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 882 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▎         | 23/616 [00:08<04:45,  2.08it/s]11/25/2022 22:39:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▍         | 24/616 [00:09<04:21,  2.26it/s]11/25/2022 22:39:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 515 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▍         | 25/616 [00:09<04:04,  2.41it/s]11/25/2022 22:39:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▍         | 26/616 [00:09<03:53,  2.53it/s]11/25/2022 22:39:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 950 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▍         | 27/616 [00:10<03:45,  2.61it/s]11/25/2022 22:39:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/25/2022 22:39:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 952 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   5%|▍         | 28/616 [00:10<03:39,  2.68it/s]11/25/2022 22:39:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 229 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   5%|▍         | 29/616 [00:10<03:08,  3.12it/s]11/25/2022 22:39:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 1026 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   5%|▍         | 30/616 [00:11<03:39,  2.67it/s]11/25/2022 22:39:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 1028 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   5%|▌         | 31/616 [00:11<04:03,  2.40it/s]11/25/2022 22:39:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 159 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   5%|▌         | 32/616 [00:11<03:26,  2.83it/s]11/25/2022 22:39:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 768 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   5%|▌         | 33/616 [00:12<03:25,  2.84it/s]11/25/2022 22:39:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▌         | 34/616 [00:12<03:24,  2.85it/s]11/25/2022 22:39:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▌         | 35/616 [00:12<03:24,  2.85it/s]11/25/2022 22:39:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▌         | 36/616 [00:13<02:56,  3.28it/s]11/25/2022 22:39:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▌         | 37/616 [00:13<03:03,  3.16it/s]11/25/2022 22:39:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▌         | 38/616 [00:13<02:42,  3.56it/s]11/25/2022 22:39:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▋         | 39/616 [00:13<02:26,  3.95it/s]11/25/2022 22:39:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▋         | 40/616 [00:14<02:15,  4.27it/s]11/25/2022 22:39:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   7%|▋         | 41/616 [00:14<02:07,  4.51it/s]11/25/2022 22:39:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   7%|▋         | 42/616 [00:14<02:01,  4.73it/s]11/25/2022 22:39:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 814 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   7%|▋         | 43/616 [00:14<02:24,  3.98it/s]11/25/2022 22:39:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   7%|▋         | 44/616 [00:14<02:14,  4.25it/s]11/25/2022 22:39:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   7%|▋         | 45/616 [00:15<02:06,  4.52it/s]11/25/2022 22:39:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 613 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   7%|▋         | 46/616 [00:15<02:27,  3.88it/s]11/25/2022 22:39:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 737 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 47/616 [00:15<02:43,  3.49it/s]11/25/2022 22:39:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 48/616 [00:16<02:53,  3.27it/s]11/25/2022 22:39:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 49/616 [00:16<02:35,  3.66it/s]11/25/2022 22:39:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 530 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 50/616 [00:16<02:46,  3.39it/s]11/25/2022 22:39:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 51/616 [00:17<02:56,  3.21it/s]11/25/2022 22:39:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 674 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 52/616 [00:17<03:02,  3.09it/s]11/25/2022 22:39:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 71 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▊         | 53/616 [00:17<02:40,  3.51it/s]11/25/2022 22:39:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 382 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▉         | 54/616 [00:17<02:23,  3.91it/s]11/25/2022 22:39:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/25/2022 22:39:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▉         | 55/616 [00:18<02:12,  4.25it/s]11/25/2022 22:39:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 680 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▉         | 56/616 [00:18<02:31,  3.71it/s]11/25/2022 22:39:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▉         | 57/616 [00:18<02:18,  4.04it/s]11/25/2022 22:39:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▉         | 58/616 [00:18<02:08,  4.35it/s]11/25/2022 22:39:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 811 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|▉         | 59/616 [00:19<02:27,  3.79it/s]11/25/2022 22:39:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 814 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|▉         | 60/616 [00:19<02:41,  3.44it/s]11/25/2022 22:39:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|▉         | 61/616 [00:19<02:25,  3.81it/s]11/25/2022 22:39:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|█         | 62/616 [00:19<02:39,  3.48it/s]11/25/2022 22:39:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 954 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|█         | 63/616 [00:20<02:50,  3.24it/s]11/25/2022 22:39:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|█         | 64/616 [00:20<02:31,  3.64it/s]11/25/2022 22:39:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 218 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█         | 65/616 [00:20<02:16,  4.02it/s]11/25/2022 22:39:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 538 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█         | 66/616 [00:21<02:32,  3.60it/s]11/25/2022 22:39:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 541 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█         | 67/616 [00:21<02:44,  3.34it/s]11/25/2022 22:39:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█         | 68/616 [00:21<02:53,  3.17it/s]11/25/2022 22:39:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 833 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█         | 69/616 [00:22<02:58,  3.06it/s]11/25/2022 22:39:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█▏        | 70/616 [00:22<02:37,  3.48it/s]11/25/2022 22:39:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▏        | 71/616 [00:22<02:21,  3.86it/s]11/25/2022 22:39:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▏        | 72/616 [00:22<02:09,  4.21it/s]11/25/2022 22:39:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▏        | 73/616 [00:23<02:26,  3.70it/s]11/25/2022 22:39:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 707 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▏        | 74/616 [00:23<02:39,  3.39it/s]11/25/2022 22:39:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 690 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▏        | 75/616 [00:23<02:49,  3.20it/s]11/25/2022 22:39:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▏        | 76/616 [00:23<02:29,  3.60it/s]11/25/2022 22:39:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:39:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▎        | 77/616 [00:24<02:15,  3.99it/s]11/25/2022 22:39:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  13%|█▎        | 78/616 [00:24<03:21,  2.66it/s]11/25/2022 22:39:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  13%|█▎        | 79/616 [00:25<04:47,  1.87it/s]11/25/2022 22:40:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  13%|█▎        | 80/616 [00:25<03:55,  2.27it/s]11/25/2022 22:40:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  13%|█▎        | 81/616 [00:26<03:15,  2.74it/s]11/25/2022 22:40:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 968 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  13%|█▎        | 82/616 [00:26<03:12,  2.77it/s]11/25/2022 22:40:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/25/2022 22:40:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  13%|█▎        | 83/616 [00:26<02:46,  3.20it/s]11/25/2022 22:40:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▎        | 84/616 [00:27<02:51,  3.11it/s]11/25/2022 22:40:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 414 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▍        | 85/616 [00:27<02:30,  3.53it/s]11/25/2022 22:40:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 426 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▍        | 86/616 [00:27<02:15,  3.92it/s]11/25/2022 22:40:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▍        | 87/616 [00:27<02:04,  4.26it/s]11/25/2022 22:40:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▍        | 88/616 [00:27<01:56,  4.53it/s]11/25/2022 22:40:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 776 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▍        | 89/616 [00:28<02:16,  3.87it/s]11/25/2022 22:40:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 777 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▍        | 90/616 [00:28<02:30,  3.49it/s]11/25/2022 22:40:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▍        | 91/616 [00:28<02:16,  3.85it/s]11/25/2022 22:40:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 1137 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▍        | 92/616 [00:29<02:54,  3.01it/s]11/25/2022 22:40:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 873 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▌        | 93/616 [00:29<02:58,  2.93it/s]11/25/2022 22:40:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 878 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▌        | 94/616 [00:29<02:59,  2.90it/s]11/25/2022 22:40:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 881 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▌        | 95/616 [00:30<03:00,  2.88it/s]11/25/2022 22:40:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▌        | 96/616 [00:30<02:37,  3.31it/s]11/25/2022 22:40:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 969 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▌        | 97/616 [00:30<02:43,  3.17it/s]11/25/2022 22:40:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▌        | 98/616 [00:30<02:25,  3.57it/s]11/25/2022 22:40:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▌        | 99/616 [00:31<02:10,  3.95it/s]11/25/2022 22:40:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 610 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▌        | 100/616 [00:31<02:24,  3.57it/s]11/25/2022 22:40:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 1412 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▋        | 101/616 [00:32<02:59,  2.87it/s]11/25/2022 22:40:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 1415 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 102/616 [00:32<03:28,  2.47it/s]11/25/2022 22:40:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 1027 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 103/616 [00:33<03:45,  2.28it/s]11/25/2022 22:40:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 1052 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 104/616 [00:33<03:56,  2.16it/s]11/25/2022 22:40:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 1054 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 105/616 [00:34<04:05,  2.08it/s]11/25/2022 22:40:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 261 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 106/616 [00:34<03:22,  2.52it/s]11/25/2022 22:40:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 107/616 [00:34<02:50,  2.98it/s]11/25/2022 22:40:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 529 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  18%|█▊        | 108/616 [00:34<02:51,  2.97it/s]11/25/2022 22:40:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 387 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  18%|█▊        | 109/616 [00:35<02:29,  3.39it/s]11/25/2022 22:40:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/25/2022 22:40:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  18%|█▊        | 110/616 [00:35<02:13,  3.80it/s]11/25/2022 22:40:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 721 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  18%|█▊        | 111/616 [00:35<02:25,  3.47it/s]11/25/2022 22:40:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  18%|█▊        | 112/616 [00:35<02:34,  3.26it/s]11/25/2022 22:40:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  18%|█▊        | 113/616 [00:36<03:30,  2.39it/s]11/25/2022 22:40:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  19%|█▊        | 114/616 [00:37<04:44,  1.76it/s]11/25/2022 22:40:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 858 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▊        | 115/616 [00:37<04:16,  1.96it/s]11/25/2022 22:40:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 861 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▉        | 116/616 [00:38<03:52,  2.15it/s]11/25/2022 22:40:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▉        | 117/616 [00:38<03:35,  2.32it/s]11/25/2022 22:40:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 896 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▉        | 118/616 [00:38<03:23,  2.45it/s]11/25/2022 22:40:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 477 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▉        | 119/616 [00:39<02:51,  2.90it/s]11/25/2022 22:40:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 861 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▉        | 120/616 [00:39<02:51,  2.90it/s]11/25/2022 22:40:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 862 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  20%|█▉        | 121/616 [00:39<02:51,  2.88it/s]11/25/2022 22:40:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  20%|█▉        | 122/616 [00:40<02:28,  3.32it/s]11/25/2022 22:40:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 257 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  20%|█▉        | 123/616 [00:40<02:13,  3.69it/s]11/25/2022 22:40:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 1546 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  20%|██        | 124/616 [00:40<03:11,  2.58it/s]11/25/2022 22:40:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  20%|██        | 125/616 [00:41<02:45,  2.97it/s]11/25/2022 22:40:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  20%|██        | 126/616 [00:41<02:23,  3.41it/s]11/25/2022 22:40:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  21%|██        | 127/616 [00:41<03:18,  2.46it/s]11/25/2022 22:40:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 1088 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  21%|██        | 128/616 [00:42<03:59,  2.04it/s]11/25/2022 22:40:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 465 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  21%|██        | 129/616 [00:42<03:17,  2.46it/s]11/25/2022 22:40:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  21%|██        | 130/616 [00:43<03:08,  2.57it/s]11/25/2022 22:40:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  21%|██▏       | 131/616 [00:43<03:03,  2.64it/s]11/25/2022 22:40:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 888 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  21%|██▏       | 132/616 [00:43<02:59,  2.70it/s]11/25/2022 22:40:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 890 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  22%|██▏       | 133/616 [00:44<02:56,  2.74it/s]11/25/2022 22:40:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  22%|██▏       | 134/616 [00:44<02:31,  3.18it/s]11/25/2022 22:40:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  22%|██▏       | 135/616 [00:44<02:35,  3.10it/s]11/25/2022 22:40:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  22%|██▏       | 136/616 [00:45<02:38,  3.02it/s]11/25/2022 22:40:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  22%|██▏       | 137/616 [00:45<02:41,  2.97it/s]11/25/2022 22:40:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  22%|██▏       | 138/616 [00:45<02:21,  3.39it/s]11/25/2022 22:40:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 139/616 [00:46<02:27,  3.23it/s]11/25/2022 22:40:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 140/616 [00:46<02:11,  3.62it/s]11/25/2022 22:40:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 141/616 [00:46<01:58,  4.00it/s]11/25/2022 22:40:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 881 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 142/616 [00:46<02:11,  3.59it/s]11/25/2022 22:40:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 653 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 143/616 [00:47<02:21,  3.33it/s]11/25/2022 22:40:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 144/616 [00:47<02:07,  3.71it/s]11/25/2022 22:40:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 418 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▎       | 145/616 [00:47<01:55,  4.08it/s]11/25/2022 22:40:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▎       | 146/616 [00:47<02:09,  3.64it/s]11/25/2022 22:40:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 669 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▍       | 147/616 [00:48<02:19,  3.35it/s]11/25/2022 22:40:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▍       | 148/616 [00:48<02:05,  3.72it/s]11/25/2022 22:40:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 1077 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▍       | 149/616 [00:48<02:38,  2.95it/s]11/25/2022 22:40:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 863 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▍       | 150/616 [00:49<02:41,  2.89it/s]11/25/2022 22:40:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 866 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▍       | 151/616 [00:49<02:41,  2.87it/s]11/25/2022 22:40:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 531 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▍       | 152/616 [00:50<02:41,  2.87it/s]11/25/2022 22:40:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▍       | 153/616 [00:50<02:41,  2.86it/s]11/25/2022 22:40:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 834 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▌       | 154/616 [00:50<02:41,  2.86it/s]11/25/2022 22:40:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▌       | 155/616 [00:50<02:20,  3.28it/s]11/25/2022 22:40:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 922 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▌       | 156/616 [00:51<02:25,  3.16it/s]11/25/2022 22:40:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 1250 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▌       | 157/616 [00:51<02:52,  2.67it/s]11/25/2022 22:40:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 1253 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▌       | 158/616 [00:52<03:11,  2.39it/s]11/25/2022 22:40:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▌       | 159/616 [00:52<02:41,  2.83it/s]11/25/2022 22:40:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▌       | 160/616 [00:52<02:40,  2.83it/s]11/25/2022 22:40:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▌       | 161/616 [00:53<02:40,  2.84it/s]11/25/2022 22:40:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 473 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▋       | 162/616 [00:53<02:18,  3.27it/s]11/25/2022 22:40:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 1016 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▋       | 163/616 [00:53<02:23,  3.15it/s]11/25/2022 22:40:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 1018 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  27%|██▋       | 164/616 [00:54<02:28,  3.05it/s]11/25/2022 22:40:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 231 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  27%|██▋       | 165/616 [00:54<02:10,  3.44it/s]11/25/2022 22:40:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 1144 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  27%|██▋       | 166/616 [00:54<02:39,  2.83it/s]11/25/2022 22:40:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  27%|██▋       | 167/616 [00:55<02:39,  2.81it/s]11/25/2022 22:40:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  27%|██▋       | 168/616 [00:55<02:18,  3.24it/s]11/25/2022 22:40:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 1154 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  27%|██▋       | 169/616 [00:55<02:43,  2.73it/s]11/25/2022 22:40:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 1011 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  28%|██▊       | 170/616 [00:56<02:42,  2.74it/s]11/25/2022 22:40:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 516 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  28%|██▊       | 171/616 [00:56<02:40,  2.77it/s]11/25/2022 22:40:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  28%|██▊       | 172/616 [00:56<02:18,  3.21it/s]11/25/2022 22:40:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 1776 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  28%|██▊       | 173/616 [00:57<03:05,  2.38it/s]11/25/2022 22:40:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 1062 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  28%|██▊       | 174/616 [00:58<03:38,  2.03it/s]11/25/2022 22:40:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  28%|██▊       | 175/616 [00:58<02:59,  2.45it/s]11/25/2022 22:40:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▊       | 176/616 [00:58<02:30,  2.92it/s]11/25/2022 22:40:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 602 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▊       | 177/616 [00:58<02:30,  2.92it/s]11/25/2022 22:40:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▉       | 178/616 [00:59<02:31,  2.90it/s]11/25/2022 22:40:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▉       | 179/616 [00:59<02:31,  2.88it/s]11/25/2022 22:40:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▉       | 180/616 [00:59<02:12,  3.30it/s]11/25/2022 22:40:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 1436 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▉       | 181/616 [01:00<02:37,  2.76it/s]11/25/2022 22:40:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 1438 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|██▉       | 182/616 [01:00<02:57,  2.44it/s]11/25/2022 22:40:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|██▉       | 183/616 [01:00<02:30,  2.88it/s]11/25/2022 22:40:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 1633 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|██▉       | 184/616 [01:01<03:12,  2.24it/s]11/25/2022 22:40:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|███       | 185/616 [01:01<02:43,  2.63it/s]11/25/2022 22:40:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|███       | 186/616 [01:02<02:19,  3.08it/s]11/25/2022 22:40:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|███       | 187/616 [01:02<02:02,  3.50it/s]11/25/2022 22:40:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███       | 188/616 [01:02<02:10,  3.29it/s]11/25/2022 22:40:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███       | 189/616 [01:02<02:15,  3.14it/s]11/25/2022 22:40:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███       | 190/616 [01:03<01:59,  3.55it/s]11/25/2022 22:40:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 712 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███       | 191/616 [01:03<02:07,  3.33it/s]11/25/2022 22:40:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  31%|███       | 192/616 [01:03<02:15,  3.12it/s]11/25/2022 22:40:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 779 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███▏      | 193/616 [01:04<02:21,  3.00it/s]11/25/2022 22:40:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 1080 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███▏      | 194/616 [01:04<02:43,  2.59it/s]11/25/2022 22:40:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 195/616 [01:04<02:19,  3.01it/s]11/25/2022 22:40:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 196/616 [01:05<02:02,  3.44it/s]11/25/2022 22:40:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 1194 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 197/616 [01:05<02:28,  2.83it/s]11/25/2022 22:40:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 1195 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 198/616 [01:06<02:48,  2.48it/s]11/25/2022 22:40:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 1080 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 199/616 [01:06<03:02,  2.29it/s]11/25/2022 22:40:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 1083 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 200/616 [01:07<03:12,  2.16it/s]11/25/2022 22:40:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 990 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 201/616 [01:07<02:59,  2.31it/s]11/25/2022 22:40:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 801 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 202/616 [01:07<02:49,  2.44it/s]11/25/2022 22:40:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 803 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 203/616 [01:08<02:41,  2.55it/s]11/25/2022 22:40:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 848 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 204/616 [01:08<02:36,  2.63it/s]11/25/2022 22:40:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 795 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 205/616 [01:08<02:33,  2.68it/s]11/25/2022 22:40:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 797 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 206/616 [01:09<02:30,  2.72it/s]11/25/2022 22:40:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 514 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▎      | 207/616 [01:09<02:28,  2.75it/s]11/25/2022 22:40:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 575 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▍      | 208/616 [01:10<02:26,  2.78it/s]11/25/2022 22:40:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▍      | 209/616 [01:10<02:25,  2.80it/s]11/25/2022 22:40:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 858 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▍      | 210/616 [01:10<02:24,  2.82it/s]11/25/2022 22:40:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▍      | 211/616 [01:10<02:04,  3.25it/s]11/25/2022 22:40:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 656 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▍      | 212/616 [01:11<02:08,  3.14it/s]11/25/2022 22:40:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 984 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▍      | 213/616 [01:11<02:12,  3.04it/s]11/25/2022 22:40:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▍      | 214/616 [01:11<02:15,  2.97it/s]11/25/2022 22:40:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 918 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▍      | 215/616 [01:12<02:16,  2.93it/s]11/25/2022 22:40:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 799 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▌      | 216/616 [01:12<02:17,  2.91it/s]11/25/2022 22:40:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 801 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▌      | 217/616 [01:13<02:18,  2.88it/s]11/25/2022 22:40:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 1105 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▌      | 218/616 [01:13<02:37,  2.53it/s]11/25/2022 22:40:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 1193 to 1536 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  36%|███▌      | 219/616 [01:14<02:51,  2.31it/s]11/25/2022 22:40:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  36%|███▌      | 220/616 [01:14<02:24,  2.75it/s]11/25/2022 22:40:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 1234 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  36%|███▌      | 221/616 [01:14<02:40,  2.46it/s]11/25/2022 22:40:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 1231 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  36%|███▌      | 222/616 [01:15<02:53,  2.27it/s]11/25/2022 22:40:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 769 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  36%|███▌      | 223/616 [01:15<02:43,  2.40it/s]11/25/2022 22:40:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 771 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  36%|███▋      | 224/616 [01:16<02:36,  2.50it/s]11/25/2022 22:40:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 946 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 225/616 [01:16<02:30,  2.59it/s]11/25/2022 22:40:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 226/616 [01:16<02:26,  2.66it/s]11/25/2022 22:40:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 1393 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 227/616 [01:17<02:41,  2.41it/s]11/25/2022 22:40:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 1396 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 228/616 [01:17<02:52,  2.24it/s]11/25/2022 22:40:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 229/616 [01:17<02:24,  2.68it/s]11/25/2022 22:40:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 230/616 [01:18<02:03,  3.14it/s]11/25/2022 22:40:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 231/616 [01:18<01:47,  3.57it/s]11/25/2022 22:40:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 232/616 [01:18<01:37,  3.95it/s]11/25/2022 22:40:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 233/616 [01:18<01:47,  3.57it/s]11/25/2022 22:40:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 906 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 234/616 [01:19<01:55,  3.31it/s]11/25/2022 22:40:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 235/616 [01:19<01:42,  3.70it/s]11/25/2022 22:40:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 236/616 [01:19<01:33,  4.07it/s]11/25/2022 22:40:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 1040 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 237/616 [01:20<02:02,  3.10it/s]11/25/2022 22:40:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▊      | 238/616 [01:20<01:49,  3.45it/s]11/25/2022 22:40:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▉      | 239/616 [01:20<01:55,  3.26it/s]11/25/2022 22:40:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 1163 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▉      | 240/616 [01:21<02:18,  2.71it/s]11/25/2022 22:40:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▉      | 241/616 [01:21<02:17,  2.72it/s]11/25/2022 22:40:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▉      | 242/616 [01:21<01:58,  3.16it/s]11/25/2022 22:40:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▉      | 243/616 [01:21<01:43,  3.59it/s]11/25/2022 22:40:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 1233 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|███▉      | 244/616 [01:22<02:08,  2.89it/s]11/25/2022 22:40:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 385 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|███▉      | 245/616 [01:22<01:52,  3.30it/s]11/25/2022 22:40:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  40%|███▉      | 246/616 [01:22<01:39,  3.71it/s]11/25/2022 22:40:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 1061 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|████      | 247/616 [01:23<02:05,  2.94it/s]11/25/2022 22:40:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 1063 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|████      | 248/616 [01:23<02:24,  2.54it/s]11/25/2022 22:40:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|████      | 249/616 [01:24<02:03,  2.97it/s]11/25/2022 22:40:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 558 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████      | 250/616 [01:24<02:04,  2.95it/s]11/25/2022 22:40:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 1329 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████      | 251/616 [01:24<02:22,  2.56it/s]11/25/2022 22:40:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████      | 252/616 [01:25<02:01,  2.99it/s]11/25/2022 22:40:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:40:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████      | 253/616 [01:25<02:02,  2.96it/s]11/25/2022 22:41:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 766 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████      | 254/616 [01:25<02:04,  2.91it/s]11/25/2022 22:41:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 1254 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████▏     | 255/616 [01:26<02:22,  2.54it/s]11/25/2022 22:41:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  42%|████▏     | 256/616 [01:26<02:18,  2.61it/s]11/25/2022 22:41:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 664 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  42%|████▏     | 257/616 [01:27<02:14,  2.68it/s]11/25/2022 22:41:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 759 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  42%|████▏     | 258/616 [01:27<02:11,  2.72it/s]11/25/2022 22:41:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  42%|████▏     | 259/616 [01:27<02:09,  2.75it/s]11/25/2022 22:41:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 1286 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  42%|████▏     | 260/616 [01:28<02:25,  2.45it/s]11/25/2022 22:41:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 1287 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  42%|████▏     | 261/616 [01:28<02:36,  2.27it/s]11/25/2022 22:41:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 999 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  43%|████▎     | 262/616 [01:29<02:27,  2.39it/s]11/25/2022 22:41:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 1002 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  43%|████▎     | 263/616 [01:29<02:20,  2.51it/s]11/25/2022 22:41:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  43%|████▎     | 264/616 [01:30<02:49,  2.07it/s]11/25/2022 22:41:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  43%|████▎     | 265/616 [01:31<03:34,  1.63it/s]11/25/2022 22:41:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  43%|████▎     | 266/616 [01:31<02:53,  2.02it/s]11/25/2022 22:41:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  43%|████▎     | 267/616 [01:32<03:20,  1.74it/s]11/25/2022 22:41:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  44%|████▎     | 268/616 [01:32<03:56,  1.47it/s]11/25/2022 22:41:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  44%|████▎     | 269/616 [01:33<03:07,  1.85it/s]11/25/2022 22:41:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  44%|████▍     | 270/616 [01:33<02:30,  2.29it/s]11/25/2022 22:41:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  44%|████▍     | 271/616 [01:33<02:21,  2.44it/s]11/25/2022 22:41:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  44%|████▍     | 272/616 [01:33<01:58,  2.89it/s]11/25/2022 22:41:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  44%|████▍     | 273/616 [01:34<01:42,  3.34it/s]11/25/2022 22:41:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  44%|████▍     | 274/616 [01:34<01:46,  3.20it/s]11/25/2022 22:41:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 1208 to 1536 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  45%|████▍     | 275/616 [01:34<02:07,  2.68it/s]11/25/2022 22:41:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 327 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  45%|████▍     | 276/616 [01:35<01:49,  3.10it/s]11/25/2022 22:41:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 620 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  45%|████▍     | 277/616 [01:35<01:54,  2.97it/s]11/25/2022 22:41:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 753 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  45%|████▌     | 278/616 [01:35<01:55,  2.92it/s]11/25/2022 22:41:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 653 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  45%|████▌     | 279/616 [01:36<01:56,  2.90it/s]11/25/2022 22:41:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 604 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  45%|████▌     | 280/616 [01:36<01:56,  2.88it/s]11/25/2022 22:41:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 977 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▌     | 281/616 [01:36<01:56,  2.87it/s]11/25/2022 22:41:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 887 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▌     | 282/616 [01:37<01:56,  2.86it/s]11/25/2022 22:41:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 995 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▌     | 283/616 [01:37<01:56,  2.86it/s]11/25/2022 22:41:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 998 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▌     | 284/616 [01:38<01:56,  2.84it/s]11/25/2022 22:41:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 631 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▋     | 285/616 [01:38<01:56,  2.85it/s]11/25/2022 22:41:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▋     | 286/616 [01:38<01:40,  3.28it/s]11/25/2022 22:41:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 287/616 [01:38<01:28,  3.70it/s]11/25/2022 22:41:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 884 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 288/616 [01:39<01:36,  3.42it/s]11/25/2022 22:41:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 885 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 289/616 [01:39<01:41,  3.22it/s]11/25/2022 22:41:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 290/616 [01:39<01:45,  3.09it/s]11/25/2022 22:41:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 291/616 [01:40<01:32,  3.50it/s]11/25/2022 22:41:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 292/616 [01:40<01:23,  3.90it/s]11/25/2022 22:41:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 640 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  48%|████▊     | 293/616 [01:40<01:31,  3.54it/s]11/25/2022 22:41:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 642 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  48%|████▊     | 294/616 [01:40<01:37,  3.30it/s]11/25/2022 22:41:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 423 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  48%|████▊     | 295/616 [01:41<01:27,  3.69it/s]11/25/2022 22:41:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  48%|████▊     | 296/616 [01:41<01:19,  4.03it/s]11/25/2022 22:41:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  48%|████▊     | 297/616 [01:41<01:59,  2.68it/s]11/25/2022 22:41:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  48%|████▊     | 298/616 [01:42<02:50,  1.87it/s]11/25/2022 22:41:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  49%|████▊     | 299/616 [01:43<02:19,  2.27it/s]11/25/2022 22:41:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 522 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  49%|████▊     | 300/616 [01:43<02:10,  2.42it/s]11/25/2022 22:41:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  49%|████▉     | 301/616 [01:43<02:04,  2.53it/s]11/25/2022 22:41:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 625 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  49%|████▉     | 302/616 [01:44<01:59,  2.62it/s]11/25/2022 22:41:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 544 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  49%|████▉     | 303/616 [01:44<01:56,  2.68it/s]11/25/2022 22:41:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 886 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  49%|████▉     | 304/616 [01:44<01:54,  2.71it/s]11/25/2022 22:41:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 1008 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|████▉     | 305/616 [01:45<01:53,  2.75it/s]11/25/2022 22:41:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  50%|████▉     | 306/616 [01:45<02:22,  2.18it/s]11/25/2022 22:41:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  50%|████▉     | 307/616 [01:46<03:03,  1.68it/s]11/25/2022 22:41:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 783 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|█████     | 308/616 [01:47<02:43,  1.88it/s]11/25/2022 22:41:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|█████     | 309/616 [01:47<02:27,  2.09it/s]11/25/2022 22:41:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 485 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|█████     | 310/616 [01:47<02:00,  2.54it/s]11/25/2022 22:41:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 1262 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|█████     | 311/616 [01:48<02:10,  2.34it/s]11/25/2022 22:41:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  51%|█████     | 312/616 [01:48<02:04,  2.45it/s]11/25/2022 22:41:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 732 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  51%|█████     | 313/616 [01:48<01:58,  2.55it/s]11/25/2022 22:41:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  51%|█████     | 314/616 [01:49<01:40,  2.99it/s]11/25/2022 22:41:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 230 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  51%|█████     | 315/616 [01:49<01:27,  3.44it/s]11/25/2022 22:41:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  51%|█████▏    | 316/616 [01:49<01:18,  3.82it/s]11/25/2022 22:41:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  51%|█████▏    | 317/616 [01:49<01:11,  4.17it/s]11/25/2022 22:41:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 230 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 318/616 [01:49<01:06,  4.45it/s]11/25/2022 22:41:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 202 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 319/616 [01:50<01:03,  4.68it/s]11/25/2022 22:41:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 320/616 [01:50<01:00,  4.85it/s]11/25/2022 22:41:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 321/616 [01:50<00:59,  4.96it/s]11/25/2022 22:41:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 322/616 [01:50<00:58,  5.04it/s]11/25/2022 22:41:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 323/616 [01:50<00:57,  5.13it/s]11/25/2022 22:41:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 324/616 [01:51<00:56,  5.17it/s]11/25/2022 22:41:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 325/616 [01:51<00:55,  5.21it/s]11/25/2022 22:41:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 326/616 [01:51<00:55,  5.24it/s]11/25/2022 22:41:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 685 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 327/616 [01:51<01:08,  4.21it/s]11/25/2022 22:41:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 965 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 328/616 [01:52<01:18,  3.66it/s]11/25/2022 22:41:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 329/616 [01:52<01:11,  4.00it/s]11/25/2022 22:41:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 1110 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  54%|█████▎    | 330/616 [01:52<01:33,  3.05it/s]11/25/2022 22:41:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 1117 to 1536 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  54%|█████▎    | 331/616 [01:53<01:49,  2.60it/s]11/25/2022 22:41:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 1040 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  54%|█████▍    | 332/616 [01:53<02:00,  2.35it/s]11/25/2022 22:41:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 538 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  54%|█████▍    | 333/616 [01:54<01:55,  2.46it/s]11/25/2022 22:41:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 1341 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  54%|█████▍    | 334/616 [01:54<02:03,  2.29it/s]11/25/2022 22:41:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 1342 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  54%|█████▍    | 335/616 [01:55<02:09,  2.17it/s]11/25/2022 22:41:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▍    | 336/616 [01:55<01:47,  2.60it/s]11/25/2022 22:41:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▍    | 337/616 [01:55<01:31,  3.06it/s]11/25/2022 22:41:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▍    | 338/616 [01:55<01:19,  3.50it/s]11/25/2022 22:41:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▌    | 339/616 [01:56<01:10,  3.90it/s]11/25/2022 22:41:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▌    | 340/616 [01:56<01:05,  4.24it/s]11/25/2022 22:41:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▌    | 341/616 [01:56<01:00,  4.52it/s]11/25/2022 22:41:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 1300 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▌    | 342/616 [01:56<01:23,  3.27it/s]11/25/2022 22:41:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 81 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▌    | 343/616 [01:57<01:15,  3.64it/s]11/25/2022 22:41:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▌    | 344/616 [01:57<01:08,  4.00it/s]11/25/2022 22:41:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 700 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▌    | 345/616 [01:57<01:15,  3.58it/s]11/25/2022 22:41:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 702 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▌    | 346/616 [01:58<01:21,  3.32it/s]11/25/2022 22:41:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▋    | 347/616 [01:58<01:25,  3.16it/s]11/25/2022 22:41:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 1754 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▋    | 348/616 [01:59<01:53,  2.35it/s]11/25/2022 22:41:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 1755 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 349/616 [01:59<02:32,  1.75it/s]11/25/2022 22:41:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 767 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 350/616 [02:00<02:16,  1.95it/s]11/25/2022 22:41:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 351/616 [02:00<02:03,  2.14it/s]11/25/2022 22:41:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 352/616 [02:00<01:41,  2.59it/s]11/25/2022 22:41:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 826 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 353/616 [02:01<01:38,  2.68it/s]11/25/2022 22:41:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 815 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 354/616 [02:01<01:36,  2.71it/s]11/25/2022 22:41:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 620 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  58%|█████▊    | 355/616 [02:01<01:34,  2.75it/s]11/25/2022 22:41:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 595 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  58%|█████▊    | 356/616 [02:02<01:33,  2.78it/s]11/25/2022 22:41:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  58%|█████▊    | 357/616 [02:02<01:32,  2.80it/s]11/25/2022 22:41:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 838 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  58%|█████▊    | 358/616 [02:02<01:31,  2.81it/s]11/25/2022 22:41:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 793 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  58%|█████▊    | 359/616 [02:03<01:31,  2.82it/s]11/25/2022 22:41:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  58%|█████▊    | 360/616 [02:03<01:30,  2.83it/s]11/25/2022 22:41:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▊    | 361/616 [02:03<01:18,  3.25it/s]11/25/2022 22:41:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 919 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▉    | 362/616 [02:04<01:21,  3.13it/s]11/25/2022 22:41:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 1119 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▉    | 363/616 [02:04<01:35,  2.66it/s]11/25/2022 22:41:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 1121 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▉    | 364/616 [02:05<01:45,  2.39it/s]11/25/2022 22:41:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▉    | 365/616 [02:05<01:28,  2.82it/s]11/25/2022 22:41:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▉    | 366/616 [02:05<01:16,  3.27it/s]11/25/2022 22:41:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 858 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|█████▉    | 367/616 [02:06<01:18,  3.15it/s]11/25/2022 22:41:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 861 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|█████▉    | 368/616 [02:06<01:21,  3.04it/s]11/25/2022 22:41:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|█████▉    | 369/616 [02:06<01:11,  3.45it/s]11/25/2022 22:41:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|██████    | 370/616 [02:06<01:03,  3.85it/s]11/25/2022 22:41:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|██████    | 371/616 [02:06<01:00,  4.05it/s]11/25/2022 22:41:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 1049 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|██████    | 372/616 [02:07<01:18,  3.09it/s]11/25/2022 22:41:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 1050 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  61%|██████    | 373/616 [02:07<01:32,  2.62it/s]11/25/2022 22:41:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 1083 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  61%|██████    | 374/616 [02:08<01:42,  2.37it/s]11/25/2022 22:41:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  61%|██████    | 375/616 [02:08<01:26,  2.80it/s]11/25/2022 22:41:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 841 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  61%|██████    | 376/616 [02:09<01:25,  2.82it/s]11/25/2022 22:41:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 1205 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  61%|██████    | 377/616 [02:09<01:35,  2.50it/s]11/25/2022 22:41:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 914 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  61%|██████▏   | 378/616 [02:09<01:32,  2.57it/s]11/25/2022 22:41:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 563 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 379/616 [02:10<01:29,  2.65it/s]11/25/2022 22:41:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 564 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 380/616 [02:10<01:27,  2.71it/s]11/25/2022 22:41:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 801 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 381/616 [02:10<01:25,  2.74it/s]11/25/2022 22:41:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 382/616 [02:11<01:24,  2.77it/s]11/25/2022 22:41:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 383/616 [02:11<01:23,  2.79it/s]11/25/2022 22:41:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 384/616 [02:11<01:11,  3.23it/s]11/25/2022 22:41:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 255 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  62%|██████▎   | 385/616 [02:12<01:03,  3.66it/s]11/25/2022 22:41:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 410 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 386/616 [02:12<00:56,  4.04it/s]11/25/2022 22:41:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 387/616 [02:12<00:52,  4.35it/s]11/25/2022 22:41:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 388/616 [02:12<00:49,  4.61it/s]11/25/2022 22:41:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 222 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 389/616 [02:12<00:47,  4.80it/s]11/25/2022 22:41:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 822 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 390/616 [02:13<00:56,  4.01it/s]11/25/2022 22:41:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 823 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 391/616 [02:13<01:03,  3.57it/s]11/25/2022 22:41:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  64%|██████▎   | 392/616 [02:13<00:57,  3.92it/s]11/25/2022 22:41:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  64%|██████▍   | 393/616 [02:13<00:52,  4.26it/s]11/25/2022 22:41:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 659 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  64%|██████▍   | 394/616 [02:14<00:59,  3.74it/s]11/25/2022 22:41:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 907 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  64%|██████▍   | 395/616 [02:14<01:04,  3.41it/s]11/25/2022 22:41:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 220 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  64%|██████▍   | 396/616 [02:14<00:58,  3.78it/s]11/25/2022 22:41:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  64%|██████▍   | 397/616 [02:15<01:03,  3.47it/s]11/25/2022 22:41:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▍   | 398/616 [02:15<00:56,  3.84it/s]11/25/2022 22:41:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▍   | 399/616 [02:15<00:51,  4.18it/s]11/25/2022 22:41:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▍   | 400/616 [02:15<00:48,  4.47it/s]11/25/2022 22:41:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 642 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▌   | 401/616 [02:16<00:55,  3.85it/s]11/25/2022 22:41:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 773 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▌   | 402/616 [02:16<01:01,  3.48it/s]11/25/2022 22:41:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▌   | 403/616 [02:16<00:55,  3.84it/s]11/25/2022 22:41:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▌   | 404/616 [02:16<00:50,  4.19it/s]11/25/2022 22:41:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▌   | 405/616 [02:16<00:47,  4.47it/s]11/25/2022 22:41:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▌   | 406/616 [02:17<00:44,  4.69it/s]11/25/2022 22:41:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 707 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▌   | 407/616 [02:17<00:52,  3.95it/s]11/25/2022 22:41:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▌   | 408/616 [02:17<00:58,  3.53it/s]11/25/2022 22:41:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 941 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▋   | 409/616 [02:18<01:03,  3.29it/s]11/25/2022 22:41:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 944 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  67%|██████▋   | 410/616 [02:18<01:05,  3.14it/s]11/25/2022 22:41:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 701 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  67%|██████▋   | 411/616 [02:18<01:07,  3.02it/s]11/25/2022 22:41:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 930 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  67%|██████▋   | 412/616 [02:19<01:08,  2.96it/s]11/25/2022 22:41:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 932 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  67%|██████▋   | 413/616 [02:19<01:09,  2.92it/s]11/25/2022 22:41:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 585 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  67%|██████▋   | 414/616 [02:20<01:09,  2.89it/s]11/25/2022 22:41:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 785 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  67%|██████▋   | 415/616 [02:20<01:09,  2.87it/s]11/25/2022 22:41:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  68%|██████▊   | 416/616 [02:20<01:00,  3.29it/s]11/25/2022 22:41:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  68%|██████▊   | 417/616 [02:20<00:53,  3.70it/s]11/25/2022 22:41:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 690 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  68%|██████▊   | 418/616 [02:21<00:58,  3.41it/s]11/25/2022 22:41:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  68%|██████▊   | 419/616 [02:21<01:01,  3.21it/s]11/25/2022 22:41:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  68%|██████▊   | 420/616 [02:21<01:03,  3.08it/s]11/25/2022 22:41:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  68%|██████▊   | 421/616 [02:22<00:55,  3.50it/s]11/25/2022 22:41:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 878 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▊   | 422/616 [02:22<00:58,  3.29it/s]11/25/2022 22:41:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 1044 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▊   | 423/616 [02:22<01:10,  2.72it/s]11/25/2022 22:41:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▉   | 424/616 [02:23<01:01,  3.15it/s]11/25/2022 22:41:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▉   | 425/616 [02:23<00:53,  3.57it/s]11/25/2022 22:41:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 578 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▉   | 426/616 [02:23<00:56,  3.34it/s]11/25/2022 22:41:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▉   | 427/616 [02:23<00:50,  3.73it/s]11/25/2022 22:41:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▉   | 428/616 [02:23<00:45,  4.09it/s]11/25/2022 22:41:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 720 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|██████▉   | 429/616 [02:24<00:51,  3.64it/s]11/25/2022 22:41:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|██████▉   | 430/616 [02:24<00:46,  3.96it/s]11/25/2022 22:41:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|██████▉   | 431/616 [02:24<00:51,  3.57it/s]11/25/2022 22:41:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 547 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|███████   | 432/616 [02:25<00:55,  3.32it/s]11/25/2022 22:41:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:41:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|███████   | 433/616 [02:25<00:49,  3.70it/s]11/25/2022 22:42:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 1474 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|███████   | 434/616 [02:25<01:01,  2.94it/s]11/25/2022 22:42:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 677 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  71%|███████   | 435/616 [02:26<01:02,  2.89it/s]11/25/2022 22:42:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  71%|███████   | 436/616 [02:26<01:02,  2.87it/s]11/25/2022 22:42:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  71%|███████   | 437/616 [02:26<01:02,  2.86it/s]11/25/2022 22:42:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  71%|███████   | 438/616 [02:27<00:54,  3.29it/s]11/25/2022 22:42:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 738 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  71%|███████▏  | 439/616 [02:27<00:56,  3.16it/s]11/25/2022 22:42:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  71%|███████▏  | 440/616 [02:27<00:57,  3.05it/s]11/25/2022 22:42:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 156 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 441/616 [02:28<00:50,  3.47it/s]11/25/2022 22:42:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 442/616 [02:28<00:44,  3.87it/s]11/25/2022 22:42:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 443/616 [02:28<00:41,  4.22it/s]11/25/2022 22:42:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 444/616 [02:28<00:38,  4.49it/s]11/25/2022 22:42:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 445/616 [02:28<00:36,  4.71it/s]11/25/2022 22:42:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 769 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 446/616 [02:29<00:42,  3.96it/s]11/25/2022 22:42:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 770 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 447/616 [02:29<00:47,  3.54it/s]11/25/2022 22:42:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 817 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 448/616 [02:29<00:50,  3.30it/s]11/25/2022 22:42:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 842 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 449/616 [02:30<00:53,  3.14it/s]11/25/2022 22:42:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 450/616 [02:30<00:46,  3.53it/s]11/25/2022 22:42:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 451/616 [02:30<00:42,  3.92it/s]11/25/2022 22:42:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 452/616 [02:30<00:46,  3.55it/s]11/25/2022 22:42:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  74%|███████▎  | 453/616 [02:31<00:49,  3.31it/s]11/25/2022 22:42:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  74%|███████▎  | 454/616 [02:31<00:43,  3.70it/s]11/25/2022 22:42:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 321 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  74%|███████▍  | 455/616 [02:31<00:39,  4.07it/s]11/25/2022 22:42:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  74%|███████▍  | 456/616 [02:32<00:44,  3.63it/s]11/25/2022 22:42:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 1100 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  74%|███████▍  | 457/616 [02:32<00:54,  2.90it/s]11/25/2022 22:42:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 1682 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  74%|███████▍  | 458/616 [02:33<01:13,  2.14it/s]11/25/2022 22:42:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 1685 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▍  | 459/616 [02:34<01:34,  1.66it/s]11/25/2022 22:42:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 464 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▍  | 460/616 [02:34<01:15,  2.05it/s]11/25/2022 22:42:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 2031 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▍  | 461/616 [02:35<01:27,  1.76it/s]11/25/2022 22:42:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▌  | 462/616 [02:35<01:11,  2.16it/s]11/25/2022 22:42:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▌  | 463/616 [02:35<00:58,  2.62it/s]11/25/2022 22:42:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 1137 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▌  | 464/616 [02:36<01:03,  2.38it/s]11/25/2022 22:42:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▌  | 465/616 [02:36<01:00,  2.49it/s]11/25/2022 22:42:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 679 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  76%|███████▌  | 466/616 [02:36<00:58,  2.58it/s]11/25/2022 22:42:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 724 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  76%|███████▌  | 467/616 [02:37<00:56,  2.65it/s]11/25/2022 22:42:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 1042 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  76%|███████▌  | 468/616 [02:37<01:02,  2.37it/s]11/25/2022 22:42:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 1044 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  76%|███████▌  | 469/616 [02:38<01:06,  2.22it/s]11/25/2022 22:42:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 548 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  76%|███████▋  | 470/616 [02:38<01:01,  2.36it/s]11/25/2022 22:42:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 750 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  76%|███████▋  | 471/616 [02:38<00:58,  2.47it/s]11/25/2022 22:42:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 752 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 472/616 [02:39<00:56,  2.57it/s]11/25/2022 22:42:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 367 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 473/616 [02:39<00:47,  3.01it/s]11/25/2022 22:42:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 1094 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 474/616 [02:40<00:54,  2.62it/s]11/25/2022 22:42:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 1095 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 475/616 [02:40<00:59,  2.36it/s]11/25/2022 22:42:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 476/616 [02:40<00:50,  2.79it/s]11/25/2022 22:42:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 590 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 477/616 [02:41<00:49,  2.82it/s]11/25/2022 22:42:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 620 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  78%|███████▊  | 478/616 [02:41<00:48,  2.82it/s]11/25/2022 22:42:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 661 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  78%|███████▊  | 479/616 [02:41<00:48,  2.83it/s]11/25/2022 22:42:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 1183 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  78%|███████▊  | 480/616 [02:42<00:54,  2.50it/s]11/25/2022 22:42:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 1180 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  78%|███████▊  | 481/616 [02:42<00:58,  2.30it/s]11/25/2022 22:42:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 870 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  78%|███████▊  | 482/616 [02:43<00:55,  2.42it/s]11/25/2022 22:42:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 504 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  78%|███████▊  | 483/616 [02:43<00:46,  2.87it/s]11/25/2022 22:42:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 216 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▊  | 484/616 [02:43<00:39,  3.33it/s]11/25/2022 22:42:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▊  | 485/616 [02:43<00:35,  3.73it/s]11/25/2022 22:42:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 712 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▉  | 486/616 [02:44<00:37,  3.43it/s]11/25/2022 22:42:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 554 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▉  | 487/616 [02:44<00:39,  3.23it/s]11/25/2022 22:42:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▉  | 488/616 [02:44<00:35,  3.63it/s]11/25/2022 22:42:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 480 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▉  | 489/616 [02:44<00:31,  3.99it/s]11/25/2022 22:42:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|███████▉  | 490/616 [02:45<00:35,  3.59it/s]11/25/2022 22:42:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|███████▉  | 491/616 [02:45<00:37,  3.33it/s]11/25/2022 22:42:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|███████▉  | 492/616 [02:45<00:39,  3.17it/s]11/25/2022 22:42:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  80%|████████  | 493/616 [02:46<00:34,  3.56it/s]11/25/2022 22:42:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|████████  | 494/616 [02:46<00:30,  3.95it/s]11/25/2022 22:42:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|████████  | 495/616 [02:46<00:28,  4.26it/s]11/25/2022 22:42:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 927 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████  | 496/616 [02:46<00:32,  3.74it/s]11/25/2022 22:42:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 1145 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████  | 497/616 [02:47<00:40,  2.94it/s]11/25/2022 22:42:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 1096 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████  | 498/616 [02:47<00:46,  2.54it/s]11/25/2022 22:42:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 895 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████  | 499/616 [02:48<00:44,  2.60it/s]11/25/2022 22:42:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 897 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████  | 500/616 [02:48<00:43,  2.66it/s]11/25/2022 22:42:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████▏ | 501/616 [02:48<00:37,  3.11it/s]11/25/2022 22:42:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 882 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████▏ | 502/616 [02:49<00:37,  3.04it/s]11/25/2022 22:42:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 883 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 503/616 [02:49<00:38,  2.97it/s]11/25/2022 22:42:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 504/616 [02:49<00:38,  2.93it/s]11/25/2022 22:42:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 796 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 505/616 [02:50<00:38,  2.91it/s]11/25/2022 22:42:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 1031 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 506/616 [02:50<00:43,  2.54it/s]11/25/2022 22:42:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 1033 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 507/616 [02:51<00:46,  2.32it/s]11/25/2022 22:42:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 508/616 [02:51<00:44,  2.44it/s]11/25/2022 22:42:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 764 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 509/616 [02:51<00:42,  2.54it/s]11/25/2022 22:42:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 765 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 510/616 [02:52<00:40,  2.62it/s]11/25/2022 22:42:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 511/616 [02:52<00:34,  3.07it/s]11/25/2022 22:42:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 512/616 [02:52<00:29,  3.48it/s]11/25/2022 22:42:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 827 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 513/616 [02:52<00:31,  3.28it/s]11/25/2022 22:42:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 829 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 514/616 [02:53<00:32,  3.13it/s]11/25/2022 22:42:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 224 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  84%|████████▎ | 515/616 [02:53<00:28,  3.54it/s]11/25/2022 22:42:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 216 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  84%|████████▍ | 516/616 [02:53<00:25,  3.93it/s]11/25/2022 22:42:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 1735 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  84%|████████▍ | 517/616 [02:54<00:37,  2.64it/s]11/25/2022 22:42:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  84%|████████▍ | 518/616 [02:54<00:32,  3.00it/s]11/25/2022 22:42:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 688 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  84%|████████▍ | 519/616 [02:54<00:32,  2.95it/s]11/25/2022 22:42:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 730 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  84%|████████▍ | 520/616 [02:55<00:33,  2.91it/s]11/25/2022 22:42:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▍ | 521/616 [02:55<00:28,  3.32it/s]11/25/2022 22:42:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▍ | 522/616 [02:55<00:25,  3.73it/s]11/25/2022 22:42:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 810 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▍ | 523/616 [02:56<00:27,  3.44it/s]11/25/2022 22:42:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 572 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▌ | 524/616 [02:56<00:28,  3.21it/s]11/25/2022 22:42:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 830 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▌ | 525/616 [02:56<00:29,  3.07it/s]11/25/2022 22:42:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 839 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▌ | 526/616 [02:57<00:30,  2.99it/s]11/25/2022 22:42:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 925 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▌ | 527/616 [02:57<00:30,  2.94it/s]11/25/2022 22:42:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▌ | 528/616 [02:57<00:30,  2.91it/s]11/25/2022 22:42:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▌ | 529/616 [02:58<00:26,  3.33it/s]11/25/2022 22:42:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▌ | 530/616 [02:58<00:22,  3.74it/s]11/25/2022 22:42:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▌ | 531/616 [02:58<00:24,  3.44it/s]11/25/2022 22:42:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▋ | 532/616 [02:58<00:25,  3.23it/s]11/25/2022 22:42:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 596 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 533/616 [02:59<00:26,  3.11it/s]11/25/2022 22:42:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 597 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 534/616 [02:59<00:27,  3.03it/s]11/25/2022 22:42:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 953 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 535/616 [02:59<00:27,  2.97it/s]11/25/2022 22:42:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 956 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 536/616 [03:00<00:27,  2.92it/s]11/25/2022 22:42:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 370 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 537/616 [03:00<00:23,  3.34it/s]11/25/2022 22:42:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 538/616 [03:00<00:20,  3.76it/s]11/25/2022 22:42:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 1377 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 539/616 [03:01<00:25,  2.97it/s]11/25/2022 22:42:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 351 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 540/616 [03:01<00:22,  3.36it/s]11/25/2022 22:42:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 541/616 [03:01<00:23,  3.21it/s]11/25/2022 22:42:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 542/616 [03:01<00:20,  3.61it/s]11/25/2022 22:42:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 543/616 [03:02<00:18,  3.99it/s]11/25/2022 22:42:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 766 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 544/616 [03:02<00:20,  3.58it/s]11/25/2022 22:42:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 545/616 [03:02<00:21,  3.32it/s]11/25/2022 22:42:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  89%|████████▊ | 546/616 [03:03<00:18,  3.71it/s]11/25/2022 22:42:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 1309 to 1536 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  89%|████████▉ | 547/616 [03:03<00:23,  2.95it/s]11/25/2022 22:42:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 1310 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  89%|████████▉ | 548/616 [03:04<00:26,  2.55it/s]11/25/2022 22:42:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 1086 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  89%|████████▉ | 549/616 [03:04<00:28,  2.32it/s]11/25/2022 22:42:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  89%|████████▉ | 550/616 [03:04<00:23,  2.76it/s]11/25/2022 22:42:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  89%|████████▉ | 551/616 [03:04<00:20,  3.21it/s]11/25/2022 22:42:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 646 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|████████▉ | 552/616 [03:05<00:20,  3.12it/s]11/25/2022 22:42:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 896 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|████████▉ | 553/616 [03:05<00:20,  3.03it/s]11/25/2022 22:42:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|████████▉ | 554/616 [03:05<00:17,  3.45it/s]11/25/2022 22:42:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 370 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|█████████ | 555/616 [03:06<00:15,  3.85it/s]11/25/2022 22:42:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|█████████ | 556/616 [03:06<00:14,  4.20it/s]11/25/2022 22:42:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|█████████ | 557/616 [03:06<00:13,  4.48it/s]11/25/2022 22:42:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 1148 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  91%|█████████ | 558/616 [03:06<00:17,  3.26it/s]11/25/2022 22:42:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 1149 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  91%|█████████ | 559/616 [03:07<00:21,  2.70it/s]11/25/2022 22:42:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 755 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  91%|█████████ | 560/616 [03:07<00:20,  2.72it/s]11/25/2022 22:42:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 727 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  91%|█████████ | 561/616 [03:08<00:20,  2.75it/s]11/25/2022 22:42:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 728 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  91%|█████████ | 562/616 [03:08<00:19,  2.77it/s]11/25/2022 22:42:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 1131 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  91%|█████████▏| 563/616 [03:09<00:21,  2.47it/s]11/25/2022 22:42:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 1134 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 564/616 [03:09<00:23,  2.25it/s]11/25/2022 22:42:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 565/616 [03:09<00:21,  2.38it/s]11/25/2022 22:42:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 566/616 [03:10<00:19,  2.51it/s]11/25/2022 22:42:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 567/616 [03:10<00:18,  2.60it/s]11/25/2022 22:42:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 568/616 [03:11<00:18,  2.66it/s]11/25/2022 22:42:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 569/616 [03:11<00:15,  3.11it/s]11/25/2022 22:42:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 1404 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  93%|█████████▎| 570/616 [03:11<00:17,  2.66it/s]11/25/2022 22:42:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 1407 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  93%|█████████▎| 571/616 [03:12<00:18,  2.39it/s]11/25/2022 22:42:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 1498 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  93%|█████████▎| 572/616 [03:12<00:19,  2.23it/s]11/25/2022 22:42:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  93%|█████████▎| 573/616 [03:12<00:16,  2.66it/s]11/25/2022 22:42:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  93%|█████████▎| 574/616 [03:13<00:13,  3.12it/s]11/25/2022 22:42:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/25/2022 22:42:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 1044 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  93%|█████████▎| 575/616 [03:13<00:15,  2.67it/s]11/25/2022 22:42:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 1189 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▎| 576/616 [03:14<00:16,  2.39it/s]11/25/2022 22:42:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 1061 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▎| 577/616 [03:14<00:17,  2.23it/s]11/25/2022 22:42:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 935 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▍| 578/616 [03:15<00:16,  2.37it/s]11/25/2022 22:42:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 899 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▍| 579/616 [03:15<00:14,  2.49it/s]11/25/2022 22:42:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 900 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▍| 580/616 [03:15<00:13,  2.59it/s]11/25/2022 22:42:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 648 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▍| 581/616 [03:16<00:13,  2.66it/s]11/25/2022 22:42:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 1240 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▍| 582/616 [03:16<00:14,  2.41it/s]11/25/2022 22:42:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▍| 583/616 [03:16<00:11,  2.84it/s]11/25/2022 22:42:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▍| 584/616 [03:17<00:11,  2.86it/s]11/25/2022 22:42:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▍| 585/616 [03:17<00:10,  2.85it/s]11/25/2022 22:42:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▌| 586/616 [03:17<00:09,  3.29it/s]11/25/2022 22:42:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 778 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▌| 587/616 [03:18<00:09,  3.16it/s]11/25/2022 22:42:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 780 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▌| 588/616 [03:18<00:09,  3.05it/s]11/25/2022 22:42:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 820 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▌| 589/616 [03:18<00:09,  2.98it/s]11/25/2022 22:42:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 821 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▌| 590/616 [03:19<00:08,  2.94it/s]11/25/2022 22:42:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▌| 591/616 [03:19<00:08,  2.90it/s]11/25/2022 22:42:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▌| 592/616 [03:19<00:07,  3.33it/s]11/25/2022 22:42:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▋| 593/616 [03:19<00:06,  3.74it/s]11/25/2022 22:42:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 957 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▋| 594/616 [03:20<00:06,  3.44it/s]11/25/2022 22:42:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 959 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 595/616 [03:20<00:06,  3.24it/s]11/25/2022 22:42:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 596/616 [03:20<00:05,  3.63it/s]11/25/2022 22:42:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 467 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 597/616 [03:20<00:04,  4.00it/s]11/25/2022 22:42:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 598/616 [03:21<00:05,  3.59it/s]11/25/2022 22:42:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 324 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 599/616 [03:21<00:04,  3.94it/s]11/25/2022 22:42:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 600/616 [03:21<00:04,  3.57it/s]11/25/2022 22:42:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  98%|█████████▊| 601/616 [03:22<00:04,  3.32it/s]11/25/2022 22:42:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/25/2022 22:42:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 1190 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  98%|█████████▊| 602/616 [03:22<00:05,  2.75it/s]11/25/2022 22:42:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 887 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  98%|█████████▊| 603/616 [03:23<00:04,  2.74it/s]11/25/2022 22:42:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 1131 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  98%|█████████▊| 604/616 [03:23<00:04,  2.45it/s]11/25/2022 22:42:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 1133 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  98%|█████████▊| 605/616 [03:24<00:04,  2.26it/s]11/25/2022 22:42:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  98%|█████████▊| 606/616 [03:24<00:03,  2.70it/s]11/25/2022 22:42:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 900 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▊| 607/616 [03:24<00:03,  2.75it/s]11/25/2022 22:42:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▊| 608/616 [03:24<00:02,  2.78it/s]11/25/2022 22:42:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:42:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▉| 609/616 [03:25<00:02,  2.80it/s]11/25/2022 22:43:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:43:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 1215 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▉| 610/616 [03:25<00:02,  2.48it/s]11/25/2022 22:43:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:43:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 1216 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▉| 611/616 [03:26<00:02,  2.28it/s]11/25/2022 22:43:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:43:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 1383 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▉| 612/616 [03:26<00:01,  2.16it/s]11/25/2022 22:43:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:43:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration: 100%|█████████▉| 613/616 [03:27<00:01,  2.60it/s]11/25/2022 22:43:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:43:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 576 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration: 100%|█████████▉| 614/616 [03:27<00:00,  2.68it/s]11/25/2022 22:43:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:43:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 855 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration: 100%|█████████▉| 615/616 [03:27<00:00,  2.72it/s]11/25/2022 22:43:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "11/25/2022 22:43:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 693 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration: 100%|██████████| 616/616 [03:27<00:00,  2.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Start the process to obtain predictions\n",
    "\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "pbar = tqdm(test_dataloader, desc=\"Iteration\")\n",
    "y_true, y_pred, score = [], [], []\n",
    "# we don't need gradient in this case.\n",
    "with torch.no_grad():\n",
    "        for _, batch in enumerate(pbar):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            # truncate to save space and computing resource\n",
    "            input_ids, input_mask, segment_ids, label_ids, seq_lens, \\\n",
    "                context_ids = batch\n",
    "            max_seq_lens = max(seq_lens)[0]\n",
    "            input_ids = input_ids[:,:max_seq_lens]\n",
    "            input_mask = input_mask[:,:max_seq_lens]\n",
    "            segment_ids = segment_ids[:,:max_seq_lens]\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "            seq_lens = seq_lens.to(device)\n",
    "            context_ids = context_ids.to(device)\n",
    "\n",
    "            if args.task_name == \"persentv1_longformer\" or args.task_name == \"persentv2_longformer\":\n",
    "                tmp_test_loss, logits = \\\n",
    "                    model1(input_ids, input_mask, labels=label_ids, return_dict=False)\n",
    "            else:\n",
    "                # intentially with gradient\n",
    "                tmp_test_loss, logits, _, _, _, _ = \\\n",
    "                    model1(input_ids, segment_ids, input_mask, seq_lens,\n",
    "                            device=device, labels=label_ids,\n",
    "                            context_ids=context_ids)\n",
    "\n",
    "            logits = F.softmax(logits, dim=-1)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            outputs = np.argmax(logits, axis=1)\n",
    "            tmp_test_accuracy=np.sum(outputs == label_ids)\n",
    "\n",
    "            y_true.append(label_ids)\n",
    "            y_pred.append(outputs)\n",
    "            score.append(logits)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "\n",
    "        test_loss = test_loss / nb_test_steps\n",
    "        test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "# we follow previous works in calculating the metrics\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "score = np.concatenate(score, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746993ea",
   "metadata": {},
   "source": [
    "## Results of QACG-BERT or QACG-LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8291bfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>total</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>work occupation</td>\n",
       "      <td>1055</td>\n",
       "      <td>0.721327</td>\n",
       "      <td>0.515517</td>\n",
       "      <td>0.531594</td>\n",
       "      <td>0.521436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crime justice system</td>\n",
       "      <td>1055</td>\n",
       "      <td>0.855924</td>\n",
       "      <td>0.583982</td>\n",
       "      <td>0.584745</td>\n",
       "      <td>0.569353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>digital online</td>\n",
       "      <td>1055</td>\n",
       "      <td>0.759242</td>\n",
       "      <td>0.412126</td>\n",
       "      <td>0.428594</td>\n",
       "      <td>0.418758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>social inequality human rights</td>\n",
       "      <td>1055</td>\n",
       "      <td>0.831280</td>\n",
       "      <td>0.428079</td>\n",
       "      <td>0.409813</td>\n",
       "      <td>0.406417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>economic issues</td>\n",
       "      <td>1055</td>\n",
       "      <td>0.942180</td>\n",
       "      <td>0.651518</td>\n",
       "      <td>0.537008</td>\n",
       "      <td>0.570574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>other not a social issue</td>\n",
       "      <td>1055</td>\n",
       "      <td>0.936493</td>\n",
       "      <td>0.498634</td>\n",
       "      <td>0.408708</td>\n",
       "      <td>0.433634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>public health</td>\n",
       "      <td>1055</td>\n",
       "      <td>0.951659</td>\n",
       "      <td>0.668927</td>\n",
       "      <td>0.662688</td>\n",
       "      <td>0.663789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            group  total  accuracy  precision    recall  \\\n",
       "0                 work occupation   1055  0.721327   0.515517  0.531594   \n",
       "1            crime justice system   1055  0.855924   0.583982  0.584745   \n",
       "2                  digital online   1055  0.759242   0.412126  0.428594   \n",
       "3  social inequality human rights   1055  0.831280   0.428079  0.409813   \n",
       "4                 economic issues   1055  0.942180   0.651518  0.537008   \n",
       "5        other not a social issue   1055  0.936493   0.498634  0.408708   \n",
       "6                   public health   1055  0.951659   0.668927  0.662688   \n",
       "\n",
       "         f1  \n",
       "0  0.521436  \n",
       "1  0.569353  \n",
       "2  0.418758  \n",
       "3  0.406417  \n",
       "4  0.570574  \n",
       "5  0.433634  \n",
       "6  0.663789  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "aspects_true4 = [[] for i in range(4)]\n",
    "aspects_pred4 = [[] for i in range(4)]\n",
    "\n",
    "aspects_true7 = [[] for i in range(7)]\n",
    "aspects_pred7 = [[] for i in range(7)]\n",
    "\n",
    "total, accuracy, precision, recall, f1 = [], [], [], [], []\n",
    "for i in range(len(y_true)):\n",
    "    \n",
    "# Comment 4 aspects commands (the next 2 commands) if the case is 7-aspect dataset\n",
    "\n",
    "#     aspects_true4[context_ids_eval[i][0]].append(y_true[i])\n",
    "#     aspects_pred4[context_ids_eval[i][0]].append(y_pred[i])\n",
    "\n",
    "# Comment 7 aspects commands (the next 2 commands) if the case is 4-aspect dataset\n",
    "    aspects_true7[context_ids_eval[i][0]].append(y_true[i])\n",
    "    aspects_pred7[context_ids_eval[i][0]].append(y_pred[i])\n",
    "    \n",
    "    \n",
    "# Comment the next for loop if the case is 7-aspect dataset\n",
    "# for j in range(4):\n",
    "#     total.append(len(aspects_true4[j]))\n",
    "#     accuracy.append(accuracy_score(aspects_true4[j], aspects_pred4[j]))\n",
    "#     precision.append(precision_score(aspects_true4[j], aspects_pred4[j], average='macro'))\n",
    "#     recall.append(recall_score(aspects_true4[j], aspects_pred4[j], average='macro'))\n",
    "#     f1.append(f1_score(aspects_true4[j], aspects_pred4[j], average='macro'))\n",
    "\n",
    "# Comment the next for loop if the case is 4-aspect dataset\n",
    "for j in range(7):\n",
    "    total.append(len(aspects_true7[j]))\n",
    "    accuracy.append(accuracy_score(aspects_true7[j], aspects_pred7[j]))\n",
    "    precision.append(precision_score(aspects_true7[j], aspects_pred7[j], average='macro'))\n",
    "    recall.append(recall_score(aspects_true7[j], aspects_pred7[j], average='macro'))\n",
    "    f1.append(f1_score(aspects_true7[j], aspects_pred7[j], average='macro'))\n",
    "\n",
    "group7V1 = ['politics', 'recreation', 'computer', 'religion', 'science', 'sale', 'general']\n",
    "group4V1 = ['politics', 'general', 'recreation', 'science']\n",
    "group7V2 = ['work occupation', 'crime justice system', 'digital online', 'social inequality human rights', 'economic issues', 'other not a social issue', 'public health']\n",
    "group4V2 = ['work occupation', 'crime justice system', 'digital online', 'social inequality human rights']\n",
    "\n",
    "# Remember to use correct aspect group for corresponding case\n",
    "df = pd.DataFrame({'group': group7V2, 'total': total, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1})\n",
    "df.sort_values(by=['total'], inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(['index'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa1b56de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax  = plt.subplots(1,1, figsize = (10,5))\n",
    "\n",
    "# Remember to set x ticks for different number of aspects\n",
    "x = np.arange(7)\n",
    "# x = np.arange(4)\n",
    "\n",
    "\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x-3*width/2, df['accuracy'], width, label='Accuracy')\n",
    "ax.bar(x-width/2, df['precision'], width, label='Precision')\n",
    "ax.bar(x+width/2, df['recall'], width, label='Recall')\n",
    "ax.bar(x+3*width/2, df['f1'], width, label='Macro-F1')\n",
    "ax.set_title('Aspect-level Metrics')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=15)\n",
    "ax.set_xticklabels(df['group'].astype(str).values)\n",
    "ax.legend(loc=(1,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ee4cf",
   "metadata": {},
   "source": [
    "## Results of Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5091c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "aspect7_true = [[], [], [], [], [], [], []]\n",
    "aspect7_pred = [[], [], [], [], [], [], []]\n",
    "\n",
    "aspect4_true = [[], [], [], []]\n",
    "aspect4_pred = [[], [], [], []]\n",
    "\n",
    "total, accuracy, precision, recall, f1 = [], [], [], [], []\n",
    "\n",
    "# Comment the next for loop if it is PerSent V2 dataset\n",
    "# for i in range(len(y_true)//4):\n",
    "#     if y_true[i*4] != 1:\n",
    "# # Comment 4 aspects commands (the next 2 commands) if the case is 7-aspect dataset\n",
    "\n",
    "# #         aspect4_true[i%4].append(y_true[i*4:i*4+4])\n",
    "# #         aspect4_pred[i%4].append(y_pred[i*4:i*4+4])\n",
    "\n",
    "# # Comment 7 aspects commands (the next 2 commands) if the case is 4-aspect dataset\n",
    "\n",
    "#         aspect7_true[i%7].append(y_true[i*4:i*4+4])\n",
    "#         aspect7_pred[i%7].append(y_pred[i*4:i*4+4])\n",
    "\n",
    "\n",
    "# Comment the next for loop if it is PerSent V1 dataset\n",
    "for i in range(len(y_true)//3):\n",
    "    if y_true[i*3] != 1:\n",
    "        \n",
    "# Comment 4 aspects commands (the next 2 commands) if the case is 7-aspect dataset\n",
    "\n",
    "#         aspect4_true[i%4].append(y_true[i*3:i*3+3])\n",
    "#         aspect4_pred[i%4].append(y_pred[i*3:i*3+3])\n",
    "\n",
    "# Comment 7 aspects commands (the next 2 commands) if the case is 4-aspect dataset\n",
    "        aspect7_true[i%7].append(y_true[i*3:i*3+3])\n",
    "        aspect7_pred[i%7].append(y_pred[i*3:i*3+3])\n",
    "        \n",
    "# Comment the next for loop if the case is 7-aspect dataset\n",
    "# for j in range(4):\n",
    "#     total.append(len(aspect4_true[j]))\n",
    "#     accuracy.append(accuracy_score(aspect4_true[j], aspect4_pred[j]))\n",
    "#     precision.append(precision_score(aspect4_true[j], aspect4_pred[j], average='macro'))\n",
    "#     recall.append(recall_score(aspect4_true[j], aspect4_pred[j], average='macro'))\n",
    "#     f1.append(f1_score(aspect4_true[j], aspect4_pred[j], average='macro'))\n",
    "\n",
    "# Comment the next for loop if the case is 4-aspect dataset\n",
    "for j in range(7):\n",
    "    total.append(len(aspect7_true[j]))\n",
    "    accuracy.append(accuracy_score(aspect7_true[j], aspect7_pred[j]))\n",
    "    precision.append(precision_score(aspect7_true[j], aspect7_pred[j], average='macro'))\n",
    "    recall.append(recall_score(aspect7_true[j], aspect7_pred[j], average='macro'))\n",
    "    f1.append(f1_score(aspect7_true[j], aspect7_pred[j], average='macro'))\n",
    "\n",
    "group7V1 = ['politics', 'recreation', 'computer', 'religion', 'science', 'sale', 'general']\n",
    "group4V1 = ['politics', 'general', 'recreation', 'science']\n",
    "group7V2 = ['work occupation', 'crime justice system', 'digital online', 'social inequality human rights', 'economic issues', 'other not a social issue', 'public health']\n",
    "group4V2 = ['work occupation', 'crime justice system', 'digital online', 'social inequality human rights']\n",
    "\n",
    "# Remember to use correct aspect group for corresponding case\n",
    "df = pd.DataFrame({'group': group7V2, 'total': total, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1})\n",
    "\n",
    "df.sort_values(by=['total'], inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(['index'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c606569",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax  = plt.subplots(1,1, figsize = (8,4))\n",
    "\n",
    "# Remember to set x ticks for different number of aspects\n",
    "x = np.arange(7)\n",
    "# x = np.arange(4)\n",
    "\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x-3*width/2, df['accuracy'], width, label='Accuracy')\n",
    "ax.bar(x-width/2, df['precision'], width, label='Precision')\n",
    "ax.bar(x+width/2, df['recall'], width, label='Recall')\n",
    "ax.bar(x+3*width/2, df['f1'], width, label='Macro-F1')\n",
    "ax.set_title('Aspect-level Metrics')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=15)\n",
    "ax.set_xticklabels(df['group'].astype(str).values)\n",
    "ax.legend(loc=(1,0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
