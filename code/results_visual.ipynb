{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72c2063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from random import shuffle\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import collections\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from util.args_parser import parser\n",
    "from model.QACGBERT import BertConfig\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, precision_score, recall_score\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler, WeightedRandomSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from util.optimization import BERTAdam\n",
    "from util.processor import (Sentihood_NLI_M_Processor,\n",
    "                            Semeval_NLI_M_Processor,\n",
    "                            Persent_Processor)\n",
    "\n",
    "from util.tokenization import *\n",
    "\n",
    "from util.evaluation import *\n",
    "\n",
    "from util.train_helper import *\n",
    "\n",
    "from model.QACGLONG import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d697dd1",
   "metadata": {},
   "source": [
    "## Logbook Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07fabc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best otained strict accuracy is 0.3039723661485319\n",
      "The best otained test accuracy is 0.8497409326424871\n"
     ]
    }
   ],
   "source": [
    "def process_log(data):\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop(['s_auc'], axis=1, inplace=True)\n",
    "    data.columns = ['epoch', 'global_step', 'loss', 't_loss', 't_acc', 'strict_acc', 'f1', 'auc', 's_acc', 's_auc']\n",
    "    data.drop(['loss', 'auc', 's_acc', 's_auc'], axis=1, inplace=True)\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    return data\n",
    "\n",
    "data1 = pd.read_csv('../results/persent/7aspects_lr_1e-4+allsamples/7aspects_lr_1e-4+allsamples_noerror_Pretrained_20epoch/log.txt', sep=\"\\t\")\n",
    "data1 = process_log(data1)\n",
    "\n",
    "# data2 = pd.read_csv('../results/persent/lr_1e-4+7000sample2/log.txt', sep=\"\\t\")\n",
    "# data2 = process_log(data2)\n",
    "\n",
    "# data3 = pd.read_csv('../results/persent/lr_1e-4+7000sample3/log.txt', sep=\"\\t\")\n",
    "# data3 = process_log(data3)\n",
    "\n",
    "# data4 = pd.read_csv('../results/persent/lr_1e-4+7000sample4/log.txt', sep=\"\\t\")\n",
    "# data4 = process_log(data4)\n",
    "\n",
    "# data5 = pd.read_csv('../results/persent/lr_1e-4+7000sample5/log.txt', sep=\"\\t\")\n",
    "# data5 = process_log(data5)\n",
    "\n",
    "# data = pd.concat([data1, data2, data3, data4, data5])\n",
    "df = data1\n",
    "\n",
    "sns.lineplot(data=df, x=\"global_step\", y=\"strict_acc\", color=\"red\", label=\"strict accuracy\")\n",
    "sns.lineplot(data=df, x=\"global_step\", y=\"t_acc\", label=\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "print(f'The best otained strict accuracy is {max(df.strict_acc)}')\n",
    "print(f'The best otained test accuracy is {max(df.t_acc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e3c61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77809c47",
   "metadata": {},
   "source": [
    "## Aspect-level Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22a1325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_init = ['--task_name', 'persent', \n",
    "             '--data_dir', '../datasets/persent/',\n",
    "             '--output_dir', '../results/persent/QACGLONG-reproduce2/',\n",
    "             '--model_type', 'QACGLONG',\n",
    "             '--do_lower_case',\n",
    "             '--max_seq_length', '2048',\n",
    "             '--train_batch_size', '64',\n",
    "             '--eval_batch_size', '10',\n",
    "             '--learning_rate', '1e-4',\n",
    "             '--num_train_epochs', '10',\n",
    "             '--vocab_file', 'BERT-Google/vocab.txt',\n",
    "             '--bert_config_file', 'Longformer/config.json',\n",
    "             '--init_checkpoint', '../results/persent/7aspects_lr_1e-4+allsamples/7aspects_lr_1e-4+allsamples_noerror_Pretrained_20epoch/best_checkpoint.bin', \n",
    "             '--seed', '123',\n",
    "             '--evaluate_interval', '150',\n",
    "             '--accumulate_gradients', '8',\n",
    "             '--gradient_accumulation_steps', '8']\n",
    "\n",
    "# args_init = ['--task_name', 'persent', \n",
    "#              '--data_dir', '../datasets/persent/',\n",
    "#              '--output_dir', '../results/persent/QACGBERT-reproduce5/',\n",
    "#              '--model_type', 'QACGBERT',\n",
    "#              '--do_lower_case',\n",
    "#              '--max_seq_length', '128',\n",
    "#              '--train_batch_size', '32',\n",
    "#              '--eval_batch_size', '64',\n",
    "#              '--learning_rate', '2e-5',\n",
    "#              '--num_train_epochs', '30',\n",
    "#              '--vocab_file', 'BERT-Google/vocab.txt',\n",
    "#              '--bert_config_file', 'BERT-Google/config.json',\n",
    "#              '--init_checkpoint', '../results/persent/4aspects_para_BERT_30epoch_128tokens/checkpoint_26800.bin', \n",
    "#              '--seed', '123',\n",
    "#              '--evaluate_interval', '200']\n",
    "\n",
    "args = parser.parse_args(args_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671844a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2022 10:36:17 - INFO - util.train_helper -   device cuda n_gpu 4 distributed training False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_log_file= ../results/persent/QACGLONG-reproduce2/log.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2022 10:36:23 - INFO - util.train_helper -   *** Model Config ***\n",
      "07/27/2022 10:36:23 - INFO - util.train_helper -   {\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"full_pooler\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"sep_token_id\": 2,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "07/27/2022 10:36:23 - INFO - util.train_helper -   model = QACGLONG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_weight = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2022 10:36:26 - INFO - util.train_helper -   retraining with saved model.\n",
      "07/27/2022 10:36:26 - INFO - util.train_helper -   loading a best checkpoint, not BERT pretrain.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/persent/7aspects_lr_1e-4+allsamples/7aspects_lr_1e-4+allsamples_noerror_Pretrained_20epoch/best_checkpoint.bin\n"
     ]
    }
   ],
   "source": [
    "device, n_gpu, output_log_file= system_setups(args)\n",
    "\n",
    "processors = {\n",
    "    \"sentihood_NLI_M\":Sentihood_NLI_M_Processor,\n",
    "    \"semeval_NLI_M\":Semeval_NLI_M_Processor,\n",
    "    \"persent\":Persent_Processor\n",
    "}\n",
    "\n",
    "processor = processors[args.task_name]()\n",
    "label_list = processor.get_labels()\n",
    "train_examples = None\n",
    "num_train_steps = None\n",
    "train_examples = processor.get_train_examples(args.data_dir)\n",
    "num_train_steps = int(len(train_examples) / args.train_batch_size * args.num_train_epochs)\n",
    "\n",
    "model1, optimizer, tokenizer = getModelOptimizerTokenizer(model_type=args.model_type,\n",
    "                                   vocab_file=args.vocab_file,\n",
    "                                   config_file=args.bert_config_file,\n",
    "                                   init_checkpoint=args.init_checkpoint,\n",
    "                                   label_list=label_list,\n",
    "                                   do_lower_case=True,\n",
    "                                   num_train_steps=num_train_steps,\n",
    "                                   learning_rate=args.learning_rate,\n",
    "                                   base_learning_rate=args.base_learning_rate,\n",
    "                                   warmup_proportion=args.warmup_proportion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfe41de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8099/8099 [00:24<00:00, 336.35it/s]\n"
     ]
    }
   ],
   "source": [
    "test_examples = processor.get_combo_examples(args.data_dir)\n",
    "test_features = convert_examples_to_features(\n",
    "        test_examples, label_list, args.max_seq_length,\n",
    "        tokenizer, args.max_context_length,\n",
    "        args.context_standalone, args)\n",
    "\n",
    "input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)\n",
    "seq_len = torch.tensor([[f.seq_len] for f in test_features], dtype=torch.long)\n",
    "context_ids = torch.tensor([f.context_ids for f in test_features], dtype=torch.long)\n",
    "\n",
    "test_data = TensorDataset(input_ids, input_mask, segment_ids,\n",
    "                                label_ids, seq_len, context_ids)\n",
    "test_dataloader = DataLoader(test_data, batch_size=args.eval_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68a4f174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QACGBertForSequenceClassification1(\n",
       "  (bert): ContextBertModel1(\n",
       "    (embeddings): BERTEmbeddings1(\n",
       "      (word_embeddings): Embedding(50265, 768)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BERTLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ContextBERTEncoder1(\n",
       "      (context_layer): ModuleList(\n",
       "        (0): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (1): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (3): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (4): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (5): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (6): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (7): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (8): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (9): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (10): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (11): Linear(in_features=1536, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer): ModuleList(\n",
       "        (0): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): ContextBERTPooler1(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (context_embeddings): Embedding(2375, 768)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6adacb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/810 [00:00<?, ?it/s]07/27/2022 10:48:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "/home/haoyu/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "Iteration:   0%|          | 1/810 [00:00<03:05,  4.37it/s]07/27/2022 10:48:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   0%|          | 2/810 [00:00<02:34,  5.21it/s]07/27/2022 10:48:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   0%|          | 3/810 [00:00<02:23,  5.61it/s]07/27/2022 10:48:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   0%|          | 4/810 [00:00<02:20,  5.74it/s]07/27/2022 10:48:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 322 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|          | 5/810 [00:00<02:17,  5.87it/s]07/27/2022 10:48:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|          | 6/810 [00:01<02:17,  5.86it/s]07/27/2022 10:48:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 199 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|          | 7/810 [00:01<02:23,  5.58it/s]07/27/2022 10:48:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|          | 8/810 [00:01<02:20,  5.70it/s]07/27/2022 10:48:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|          | 9/810 [00:01<02:19,  5.74it/s]07/27/2022 10:48:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|          | 10/810 [00:01<02:16,  5.84it/s]07/27/2022 10:48:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 278 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|▏         | 11/810 [00:01<02:16,  5.87it/s]07/27/2022 10:48:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   1%|▏         | 12/810 [00:02<02:46,  4.79it/s]07/27/2022 10:48:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 13/810 [00:02<03:07,  4.25it/s]07/27/2022 10:48:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 14/810 [00:02<02:52,  4.62it/s]07/27/2022 10:48:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 338 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 15/810 [00:02<02:39,  4.99it/s]07/27/2022 10:48:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 654 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 16/810 [00:03<03:00,  4.40it/s]07/27/2022 10:48:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 1483 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 17/810 [00:03<03:49,  3.45it/s]07/27/2022 10:48:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 950 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 18/810 [00:03<03:54,  3.38it/s]07/27/2022 10:48:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 1777 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 19/810 [00:04<04:58,  2.65it/s]07/27/2022 10:48:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 639 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   2%|▏         | 20/810 [00:04<04:43,  2.78it/s]07/27/2022 10:48:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 1281 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   3%|▎         | 21/810 [00:05<05:01,  2.62it/s]07/27/2022 10:48:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   3%|▎         | 22/810 [00:05<04:13,  3.11it/s]07/27/2022 10:48:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 1325 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   3%|▎         | 23/810 [00:05<04:38,  2.83it/s]07/27/2022 10:48:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:   3%|▎         | 24/810 [00:06<05:32,  2.37it/s]07/27/2022 10:48:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:   3%|▎         | 25/810 [00:06<06:12,  2.11it/s]07/27/2022 10:48:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 583 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   3%|▎         | 26/810 [00:07<05:34,  2.34it/s]07/27/2022 10:48:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 1102 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   3%|▎         | 27/810 [00:07<05:38,  2.31it/s]07/27/2022 10:48:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2022 10:48:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 1102 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   3%|▎         | 28/810 [00:08<05:40,  2.29it/s]07/27/2022 10:48:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 878 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▎         | 29/810 [00:08<05:10,  2.51it/s]07/27/2022 10:48:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 858 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▎         | 30/810 [00:08<04:47,  2.71it/s]07/27/2022 10:48:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 858 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▍         | 31/810 [00:09<04:31,  2.86it/s]07/27/2022 10:48:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▍         | 32/810 [00:09<03:50,  3.38it/s]07/27/2022 10:48:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▍         | 33/810 [00:09<03:48,  3.40it/s]07/27/2022 10:48:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 671 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▍         | 34/810 [00:09<03:49,  3.37it/s]07/27/2022 10:48:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 176 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▍         | 35/810 [00:10<03:21,  3.84it/s]07/27/2022 10:48:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 948 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   4%|▍         | 36/810 [00:10<03:29,  3.70it/s]07/27/2022 10:48:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 73 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   5%|▍         | 37/810 [00:10<03:06,  4.15it/s]07/27/2022 10:48:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   5%|▍         | 38/810 [00:10<02:47,  4.60it/s]07/27/2022 10:48:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 299 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   5%|▍         | 39/810 [00:10<02:35,  4.96it/s]07/27/2022 10:48:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:   5%|▍         | 40/810 [00:11<02:57,  4.35it/s]07/27/2022 10:48:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   5%|▌         | 41/810 [00:11<02:42,  4.73it/s]07/27/2022 10:48:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 155 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   5%|▌         | 42/810 [00:11<02:31,  5.08it/s]07/27/2022 10:48:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 764 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   5%|▌         | 43/810 [00:11<02:53,  4.43it/s]07/27/2022 10:48:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 334 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   5%|▌         | 44/810 [00:11<02:40,  4.78it/s]07/27/2022 10:48:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▌         | 45/810 [00:12<02:58,  4.28it/s]07/27/2022 10:48:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▌         | 46/810 [00:12<03:13,  3.94it/s]07/27/2022 10:48:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▌         | 47/810 [00:12<02:55,  4.36it/s]07/27/2022 10:48:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▌         | 48/810 [00:12<03:09,  4.03it/s]07/27/2022 10:48:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 523 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▌         | 49/810 [00:13<03:20,  3.80it/s]07/27/2022 10:48:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▌         | 50/810 [00:13<02:58,  4.25it/s]07/27/2022 10:48:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▋         | 51/810 [00:13<02:42,  4.68it/s]07/27/2022 10:48:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   6%|▋         | 52/810 [00:13<03:00,  4.20it/s]07/27/2022 10:48:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 754 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   7%|▋         | 53/810 [00:14<03:13,  3.91it/s]07/27/2022 10:48:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   7%|▋         | 54/810 [00:14<02:54,  4.33it/s]07/27/2022 10:48:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   7%|▋         | 55/810 [00:14<02:38,  4.76it/s]07/27/2022 10:48:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 810 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   7%|▋         | 56/810 [00:14<02:56,  4.27it/s]07/27/2022 10:48:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   7%|▋         | 57/810 [00:15<02:42,  4.64it/s]07/27/2022 10:48:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   7%|▋         | 58/810 [00:15<02:30,  4.99it/s]07/27/2022 10:48:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   7%|▋         | 59/810 [00:15<02:22,  5.28it/s]07/27/2022 10:48:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   7%|▋         | 60/810 [00:15<02:17,  5.44it/s]07/27/2022 10:48:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 61/810 [00:15<02:41,  4.63it/s]07/27/2022 10:48:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 62/810 [00:16<03:01,  4.13it/s]07/27/2022 10:48:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 63/810 [00:16<03:17,  3.78it/s]07/27/2022 10:48:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 388 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 64/810 [00:16<02:56,  4.23it/s]07/27/2022 10:48:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 65/810 [00:16<03:08,  3.96it/s]07/27/2022 10:48:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 526 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 66/810 [00:17<03:17,  3.77it/s]07/27/2022 10:48:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 67/810 [00:17<02:56,  4.21it/s]07/27/2022 10:48:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 670 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   8%|▊         | 68/810 [00:17<03:09,  3.91it/s]07/27/2022 10:48:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 96 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▊         | 69/810 [00:17<02:50,  4.35it/s]07/27/2022 10:48:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 67 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▊         | 70/810 [00:17<02:36,  4.74it/s]07/27/2022 10:48:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 378 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▉         | 71/810 [00:18<02:25,  5.09it/s]07/27/2022 10:48:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 296 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▉         | 72/810 [00:18<02:17,  5.35it/s]07/27/2022 10:48:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▉         | 73/810 [00:18<02:41,  4.58it/s]07/27/2022 10:48:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 676 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▉         | 74/810 [00:18<02:58,  4.13it/s]07/27/2022 10:48:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▉         | 75/810 [00:19<02:42,  4.52it/s]07/27/2022 10:48:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:   9%|▉         | 76/810 [00:19<02:29,  4.90it/s]07/27/2022 10:48:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 482 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|▉         | 77/810 [00:19<02:20,  5.21it/s]07/27/2022 10:48:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 810 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|▉         | 78/810 [00:19<02:42,  4.51it/s]07/27/2022 10:48:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 440 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|▉         | 79/810 [00:19<02:31,  4.84it/s]07/27/2022 10:48:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|▉         | 80/810 [00:20<02:50,  4.27it/s]07/27/2022 10:48:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|█         | 81/810 [00:20<03:04,  3.95it/s]07/27/2022 10:48:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 950 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  10%|█         | 82/810 [00:20<03:14,  3.74it/s]07/27/2022 10:48:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 504 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|█         | 83/810 [00:20<02:53,  4.20it/s]07/27/2022 10:48:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|█         | 84/810 [00:21<02:36,  4.64it/s]07/27/2022 10:48:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 193 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  10%|█         | 85/810 [00:21<02:24,  5.01it/s]07/27/2022 10:48:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 537 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█         | 86/810 [00:21<02:44,  4.41it/s]07/27/2022 10:48:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 721 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█         | 87/810 [00:21<02:59,  4.03it/s]07/27/2022 10:48:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 721 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█         | 88/810 [00:22<03:10,  3.78it/s]07/27/2022 10:48:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:48:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 829 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█         | 89/810 [00:22<03:18,  3.64it/s]07/27/2022 10:49:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█         | 90/810 [00:22<02:55,  4.10it/s]07/27/2022 10:49:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 490 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█         | 91/810 [00:22<02:37,  4.55it/s]07/27/2022 10:49:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█▏        | 92/810 [00:22<02:25,  4.94it/s]07/27/2022 10:49:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  11%|█▏        | 93/810 [00:23<02:16,  5.26it/s]07/27/2022 10:49:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▏        | 94/810 [00:23<02:37,  4.54it/s]07/27/2022 10:49:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 703 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▏        | 95/810 [00:23<02:53,  4.12it/s]07/27/2022 10:49:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 689 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▏        | 96/810 [00:23<03:05,  3.85it/s]07/27/2022 10:49:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▏        | 97/810 [00:24<02:46,  4.28it/s]07/27/2022 10:49:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▏        | 98/810 [00:24<02:58,  3.98it/s]07/27/2022 10:49:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▏        | 99/810 [00:24<02:42,  4.38it/s]07/27/2022 10:49:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  12%|█▏        | 100/810 [00:24<02:28,  4.77it/s]07/27/2022 10:49:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  12%|█▏        | 101/810 [00:25<03:43,  3.18it/s]07/27/2022 10:49:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 1046 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  13%|█▎        | 102/810 [00:25<04:13,  2.79it/s]07/27/2022 10:49:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  13%|█▎        | 103/810 [00:25<03:34,  3.29it/s]07/27/2022 10:49:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  13%|█▎        | 104/810 [00:26<03:06,  3.79it/s]07/27/2022 10:49:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 964 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  13%|█▎        | 105/810 [00:26<03:12,  3.66it/s]07/27/2022 10:49:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  13%|█▎        | 106/810 [00:26<02:50,  4.13it/s]07/27/2022 10:49:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 493 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  13%|█▎        | 107/810 [00:26<02:33,  4.58it/s]07/27/2022 10:49:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  13%|█▎        | 108/810 [00:27<02:48,  4.16it/s]07/27/2022 10:49:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 658 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  13%|█▎        | 109/810 [00:27<02:59,  3.89it/s]07/27/2022 10:49:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2022 10:49:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▎        | 110/810 [00:27<02:43,  4.28it/s]07/27/2022 10:49:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▎        | 111/810 [00:27<02:28,  4.71it/s]07/27/2022 10:49:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 371 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▍        | 112/810 [00:27<02:18,  5.05it/s]07/27/2022 10:49:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▍        | 113/810 [00:28<02:12,  5.27it/s]07/27/2022 10:49:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▍        | 114/810 [00:28<02:33,  4.55it/s]07/27/2022 10:49:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▍        | 115/810 [00:28<02:48,  4.12it/s]07/27/2022 10:49:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 773 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▍        | 116/810 [00:28<02:59,  3.86it/s]07/27/2022 10:49:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 773 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  14%|█▍        | 117/810 [00:29<03:07,  3.69it/s]07/27/2022 10:49:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▍        | 118/810 [00:29<02:46,  4.15it/s]07/27/2022 10:49:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 1133 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▍        | 119/810 [00:29<03:25,  3.36it/s]07/27/2022 10:49:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▍        | 120/810 [00:30<03:27,  3.33it/s]07/27/2022 10:49:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 871 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▍        | 121/810 [00:30<03:26,  3.33it/s]07/27/2022 10:49:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 877 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▌        | 122/810 [00:30<03:26,  3.33it/s]07/27/2022 10:49:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 877 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▌        | 123/810 [00:31<03:25,  3.34it/s]07/27/2022 10:49:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▌        | 124/810 [00:31<02:58,  3.84it/s]07/27/2022 10:49:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 965 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  15%|█▌        | 125/810 [00:31<03:04,  3.70it/s]07/27/2022 10:49:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 965 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▌        | 126/810 [00:31<03:10,  3.59it/s]07/27/2022 10:49:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▌        | 127/810 [00:31<02:48,  4.06it/s]07/27/2022 10:49:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 386 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▌        | 128/810 [00:32<02:31,  4.51it/s]07/27/2022 10:49:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 606 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▌        | 129/810 [00:32<02:53,  3.93it/s]07/27/2022 10:49:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 556 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▌        | 130/810 [00:32<03:01,  3.74it/s]07/27/2022 10:49:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 1411 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▌        | 131/810 [00:33<03:35,  3.15it/s]07/27/2022 10:49:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 1411 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▋        | 132/810 [00:33<04:01,  2.81it/s]07/27/2022 10:49:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 1026 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  16%|█▋        | 133/810 [00:34<04:18,  2.62it/s]07/27/2022 10:49:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 134/810 [00:34<03:36,  3.12it/s]07/27/2022 10:49:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 135/810 [00:34<03:05,  3.64it/s]07/27/2022 10:49:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 1050 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 136/810 [00:34<03:35,  3.13it/s]07/27/2022 10:49:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2022 10:49:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 1050 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 137/810 [00:35<04:00,  2.80it/s]07/27/2022 10:49:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 869 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 138/810 [00:35<03:50,  2.92it/s]07/27/2022 10:49:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 139/810 [00:35<03:15,  3.43it/s]07/27/2022 10:49:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 140/810 [00:35<02:49,  3.96it/s]07/27/2022 10:49:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 525 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  17%|█▋        | 141/810 [00:36<02:56,  3.79it/s]07/27/2022 10:49:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  18%|█▊        | 142/810 [00:36<02:37,  4.24it/s]07/27/2022 10:49:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 383 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  18%|█▊        | 143/810 [00:36<02:23,  4.65it/s]07/27/2022 10:49:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  18%|█▊        | 144/810 [00:36<02:38,  4.21it/s]07/27/2022 10:49:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 717 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  18%|█▊        | 145/810 [00:37<02:49,  3.91it/s]07/27/2022 10:49:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  18%|█▊        | 146/810 [00:37<02:58,  3.73it/s]07/27/2022 10:49:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  18%|█▊        | 147/810 [00:38<03:57,  2.79it/s]07/27/2022 10:49:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  18%|█▊        | 148/810 [00:38<04:44,  2.33it/s]07/27/2022 10:49:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  18%|█▊        | 149/810 [00:38<03:56,  2.80it/s]07/27/2022 10:49:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 857 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▊        | 150/810 [00:39<03:44,  2.94it/s]07/27/2022 10:49:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 857 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▊        | 151/810 [00:39<03:36,  3.05it/s]07/27/2022 10:49:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▉        | 152/810 [00:39<03:31,  3.11it/s]07/27/2022 10:49:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▉        | 153/810 [00:40<03:26,  3.18it/s]07/27/2022 10:49:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▉        | 154/810 [00:40<03:23,  3.22it/s]07/27/2022 10:49:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▉        | 155/810 [00:40<02:55,  3.72it/s]07/27/2022 10:49:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 858 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▉        | 156/810 [00:40<03:00,  3.62it/s]07/27/2022 10:49:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 858 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  19%|█▉        | 157/810 [00:41<03:04,  3.53it/s]07/27/2022 10:49:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  20%|█▉        | 158/810 [00:41<02:42,  4.01it/s]07/27/2022 10:49:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  20%|█▉        | 159/810 [00:41<02:25,  4.48it/s]07/27/2022 10:49:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 1542 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  20%|█▉        | 160/810 [00:42<03:37,  2.99it/s]07/27/2022 10:49:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 1542 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  20%|█▉        | 161/810 [00:42<04:27,  2.42it/s]07/27/2022 10:49:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 207 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  20%|██        | 162/810 [00:42<03:43,  2.90it/s]07/27/2022 10:49:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 315 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  20%|██        | 163/810 [00:42<03:08,  3.43it/s]07/27/2022 10:49:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  20%|██        | 164/810 [00:43<02:45,  3.90it/s]07/27/2022 10:49:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  20%|██        | 165/810 [00:43<03:44,  2.87it/s]07/27/2022 10:49:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  20%|██        | 166/810 [00:44<04:32,  2.37it/s]07/27/2022 10:49:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 776 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  21%|██        | 167/810 [00:44<04:11,  2.56it/s]07/27/2022 10:49:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  21%|██        | 168/810 [00:44<03:28,  3.08it/s]07/27/2022 10:49:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  21%|██        | 169/810 [00:44<02:58,  3.59it/s]07/27/2022 10:49:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  21%|██        | 170/810 [00:45<03:00,  3.54it/s]07/27/2022 10:49:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  21%|██        | 171/810 [00:45<03:03,  3.49it/s]07/27/2022 10:49:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  21%|██        | 172/810 [00:45<02:40,  3.97it/s]07/27/2022 10:49:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 886 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  21%|██▏       | 173/810 [00:46<02:48,  3.78it/s]07/27/2022 10:49:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  21%|██▏       | 174/810 [00:46<02:29,  4.24it/s]07/27/2022 10:49:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  22%|██▏       | 175/810 [00:46<02:16,  4.66it/s]07/27/2022 10:49:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 584 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  22%|██▏       | 176/810 [00:46<02:30,  4.20it/s]07/27/2022 10:49:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  22%|██▏       | 177/810 [00:46<02:18,  4.58it/s]07/27/2022 10:49:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  22%|██▏       | 178/810 [00:47<02:31,  4.16it/s]07/27/2022 10:49:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  22%|██▏       | 179/810 [00:47<02:18,  4.55it/s]07/27/2022 10:49:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  22%|██▏       | 180/810 [00:47<02:07,  4.94it/s]07/27/2022 10:49:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  22%|██▏       | 181/810 [00:47<02:01,  5.18it/s]07/27/2022 10:49:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  22%|██▏       | 182/810 [00:47<01:55,  5.45it/s]07/27/2022 10:49:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 474 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 183/810 [00:47<01:50,  5.65it/s]07/27/2022 10:49:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 425 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 184/810 [00:48<01:49,  5.71it/s]07/27/2022 10:49:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 877 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 185/810 [00:48<02:11,  4.75it/s]07/27/2022 10:49:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 186/810 [00:48<02:27,  4.24it/s]07/27/2022 10:49:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 187/810 [00:48<02:38,  3.93it/s]07/27/2022 10:49:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 188/810 [00:49<02:22,  4.36it/s]07/27/2022 10:49:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 289 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 189/810 [00:49<02:10,  4.77it/s]07/27/2022 10:49:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  23%|██▎       | 190/810 [00:49<02:01,  5.11it/s]07/27/2022 10:49:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▎       | 191/810 [00:49<02:18,  4.46it/s]07/27/2022 10:49:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 665 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▎       | 192/810 [00:50<02:32,  4.06it/s]07/27/2022 10:49:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▍       | 193/810 [00:50<02:17,  4.49it/s]07/27/2022 10:49:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2022 10:49:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 1073 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▍       | 194/810 [00:50<02:54,  3.53it/s]07/27/2022 10:49:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 709 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▍       | 195/810 [00:50<02:59,  3.43it/s]07/27/2022 10:49:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▍       | 196/810 [00:51<02:36,  3.92it/s]07/27/2022 10:49:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 862 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▍       | 197/810 [00:51<02:43,  3.75it/s]07/27/2022 10:49:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  24%|██▍       | 198/810 [00:51<02:26,  4.17it/s]07/27/2022 10:49:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 528 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▍       | 199/810 [00:51<02:35,  3.92it/s]07/27/2022 10:49:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 714 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▍       | 200/810 [00:52<02:44,  3.72it/s]07/27/2022 10:49:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 830 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▍       | 201/810 [00:52<02:49,  3.58it/s]07/27/2022 10:49:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 830 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▍       | 202/810 [00:52<02:53,  3.50it/s]07/27/2022 10:49:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▌       | 203/810 [00:52<02:32,  3.98it/s]07/27/2022 10:49:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▌       | 204/810 [00:53<02:16,  4.46it/s]07/27/2022 10:49:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 920 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▌       | 205/810 [00:53<02:28,  4.08it/s]07/27/2022 10:49:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 1249 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  25%|██▌       | 206/810 [00:53<03:03,  3.29it/s]07/27/2022 10:49:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 1249 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▌       | 207/810 [00:54<03:28,  2.89it/s]07/27/2022 10:49:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▌       | 208/810 [00:54<02:57,  3.39it/s]07/27/2022 10:49:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▌       | 209/810 [00:54<02:56,  3.40it/s]07/27/2022 10:49:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 725 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▌       | 210/810 [00:55<02:57,  3.39it/s]07/27/2022 10:49:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 888 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▌       | 211/810 [00:55<02:57,  3.37it/s]07/27/2022 10:49:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 469 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▌       | 212/810 [00:55<02:34,  3.87it/s]07/27/2022 10:49:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 1014 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▋       | 213/810 [00:55<02:40,  3.72it/s]07/27/2022 10:49:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 1014 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  26%|██▋       | 214/810 [00:56<02:45,  3.60it/s]07/27/2022 10:49:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 230 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  27%|██▋       | 215/810 [00:56<02:25,  4.08it/s]07/27/2022 10:49:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 1140 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  27%|██▋       | 216/810 [00:56<02:57,  3.34it/s]07/27/2022 10:49:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 1140 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  27%|██▋       | 217/810 [00:57<03:23,  2.92it/s]07/27/2022 10:49:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  27%|██▋       | 218/810 [00:57<02:53,  3.42it/s]07/27/2022 10:49:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 350 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  27%|██▋       | 219/810 [00:57<02:30,  3.92it/s]07/27/2022 10:49:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 1150 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  27%|██▋       | 220/810 [00:57<03:00,  3.27it/s]07/27/2022 10:49:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2022 10:49:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  27%|██▋       | 221/810 [00:58<02:39,  3.70it/s]07/27/2022 10:49:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 1007 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  27%|██▋       | 222/810 [00:58<02:43,  3.60it/s]07/27/2022 10:49:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  28%|██▊       | 223/810 [00:58<02:24,  4.07it/s]07/27/2022 10:49:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  28%|██▊       | 224/810 [00:58<02:09,  4.53it/s]07/27/2022 10:49:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 1772 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  28%|██▊       | 225/810 [00:59<03:08,  3.10it/s]07/27/2022 10:49:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 1772 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  28%|██▊       | 226/810 [00:59<03:56,  2.47it/s]07/27/2022 10:49:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 1058 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  28%|██▊       | 227/810 [01:00<04:06,  2.37it/s]07/27/2022 10:49:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  28%|██▊       | 228/810 [01:00<03:45,  2.58it/s]07/27/2022 10:49:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  28%|██▊       | 229/810 [01:00<03:30,  2.76it/s]07/27/2022 10:49:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  28%|██▊       | 230/810 [01:01<02:56,  3.28it/s]07/27/2022 10:49:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▊       | 231/810 [01:01<02:31,  3.82it/s]07/27/2022 10:49:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▊       | 232/810 [01:01<02:36,  3.69it/s]07/27/2022 10:49:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▉       | 233/810 [01:01<02:40,  3.59it/s]07/27/2022 10:49:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▉       | 234/810 [01:02<02:44,  3.50it/s]07/27/2022 10:49:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 405 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▉       | 235/810 [01:02<02:23,  3.99it/s]07/27/2022 10:49:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 967 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▉       | 236/810 [01:02<02:31,  3.79it/s]07/27/2022 10:49:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 1434 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▉       | 237/810 [01:03<03:00,  3.18it/s]07/27/2022 10:49:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 1434 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  29%|██▉       | 238/810 [01:03<03:26,  2.77it/s]07/27/2022 10:49:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|██▉       | 239/810 [01:03<02:54,  3.26it/s]07/27/2022 10:49:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 1629 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|██▉       | 240/810 [01:04<03:38,  2.61it/s]07/27/2022 10:49:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 1629 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|██▉       | 241/810 [01:04<04:14,  2.24it/s]07/27/2022 10:49:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 437 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|██▉       | 242/810 [01:05<03:29,  2.71it/s]07/27/2022 10:49:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 498 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|███       | 243/810 [01:05<02:54,  3.24it/s]07/27/2022 10:49:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|███       | 244/810 [01:05<02:30,  3.76it/s]07/27/2022 10:49:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|███       | 245/810 [01:05<02:14,  4.20it/s]07/27/2022 10:49:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|███       | 246/810 [01:05<02:23,  3.94it/s]07/27/2022 10:49:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 641 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  30%|███       | 247/810 [01:06<02:30,  3.74it/s]07/27/2022 10:49:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 337 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███       | 248/810 [01:06<02:13,  4.20it/s]07/27/2022 10:49:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2022 10:49:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███       | 249/810 [01:06<02:00,  4.64it/s]07/27/2022 10:49:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███       | 250/810 [01:06<02:13,  4.20it/s]07/27/2022 10:49:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███       | 251/810 [01:07<02:01,  4.59it/s]07/27/2022 10:49:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 775 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███       | 252/810 [01:07<02:13,  4.17it/s]07/27/2022 10:49:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 1076 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███       | 253/810 [01:07<02:46,  3.35it/s]07/27/2022 10:49:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███▏      | 254/810 [01:07<02:25,  3.82it/s]07/27/2022 10:49:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  31%|███▏      | 255/810 [01:08<02:09,  4.29it/s]07/27/2022 10:49:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 256/810 [01:08<01:57,  4.71it/s]07/27/2022 10:49:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 1191 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 257/810 [01:08<02:33,  3.61it/s]07/27/2022 10:49:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 258/810 [01:08<02:37,  3.49it/s]07/27/2022 10:49:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 259/810 [01:09<02:18,  3.98it/s]07/27/2022 10:49:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 1079 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 260/810 [01:09<02:47,  3.28it/s]07/27/2022 10:49:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 986 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 261/810 [01:09<02:47,  3.27it/s]07/27/2022 10:49:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 986 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 262/810 [01:10<02:46,  3.29it/s]07/27/2022 10:49:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 799 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  32%|███▏      | 263/810 [01:10<02:45,  3.31it/s]07/27/2022 10:49:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 799 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 264/810 [01:10<02:44,  3.33it/s]07/27/2022 10:49:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 844 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 265/810 [01:11<02:43,  3.33it/s]07/27/2022 10:49:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 844 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 266/810 [01:11<02:43,  3.34it/s]07/27/2022 10:49:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 267/810 [01:11<02:42,  3.34it/s]07/27/2022 10:49:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 793 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 268/810 [01:11<02:41,  3.35it/s]07/27/2022 10:49:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 793 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 269/810 [01:12<02:42,  3.34it/s]07/27/2022 10:49:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 270/810 [01:12<02:21,  3.83it/s]07/27/2022 10:49:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 572 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  33%|███▎      | 271/810 [01:12<02:25,  3.70it/s]07/27/2022 10:49:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 572 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▎      | 272/810 [01:13<02:29,  3.59it/s]07/27/2022 10:49:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 427 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▎      | 273/810 [01:13<02:11,  4.08it/s]07/27/2022 10:49:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 854 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▍      | 274/810 [01:13<02:19,  3.85it/s]07/27/2022 10:49:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 438 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▍      | 275/810 [01:13<02:04,  4.31it/s]07/27/2022 10:49:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2022 10:49:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 652 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▍      | 276/810 [01:13<02:13,  4.01it/s]07/27/2022 10:49:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▍      | 277/810 [01:14<02:21,  3.76it/s]07/27/2022 10:49:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 980 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▍      | 278/810 [01:14<02:26,  3.62it/s]07/27/2022 10:49:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 917 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  34%|███▍      | 279/810 [01:14<02:30,  3.53it/s]07/27/2022 10:49:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▍      | 280/810 [01:15<02:11,  4.02it/s]07/27/2022 10:49:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 797 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▍      | 281/810 [01:15<02:18,  3.83it/s]07/27/2022 10:49:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 797 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▍      | 282/810 [01:15<02:24,  3.67it/s]07/27/2022 10:49:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 1102 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▍      | 283/810 [01:16<02:49,  3.12it/s]07/27/2022 10:49:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 1189 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▌      | 284/810 [01:16<03:08,  2.80it/s]07/27/2022 10:49:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 1189 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▌      | 285/810 [01:16<03:21,  2.61it/s]07/27/2022 10:49:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 504 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▌      | 286/810 [01:17<02:48,  3.11it/s]07/27/2022 10:49:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 1230 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  35%|███▌      | 287/810 [01:17<03:05,  2.82it/s]07/27/2022 10:49:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  36%|███▌      | 288/810 [01:17<02:37,  3.32it/s]07/27/2022 10:49:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  36%|███▌      | 289/810 [01:17<02:15,  3.85it/s]07/27/2022 10:49:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 767 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  36%|███▌      | 290/810 [01:18<02:20,  3.71it/s]07/27/2022 10:49:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 767 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  36%|███▌      | 291/810 [01:18<02:24,  3.60it/s]07/27/2022 10:49:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 942 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  36%|███▌      | 292/810 [01:18<02:27,  3.52it/s]07/27/2022 10:49:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  36%|███▌      | 293/810 [01:19<02:29,  3.46it/s]07/27/2022 10:49:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 592 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  36%|███▋      | 294/810 [01:19<02:30,  3.43it/s]07/27/2022 10:49:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 1392 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  36%|███▋      | 295/810 [01:19<02:52,  2.99it/s]07/27/2022 10:49:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 1392 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 296/810 [01:20<03:08,  2.72it/s]07/27/2022 10:49:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 433 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 297/810 [01:20<02:39,  3.21it/s]07/27/2022 10:49:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 298/810 [01:20<02:17,  3.73it/s]07/27/2022 10:49:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 419 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 299/810 [01:20<02:00,  4.23it/s]07/27/2022 10:49:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 300/810 [01:20<01:49,  4.66it/s]07/27/2022 10:49:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 218 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 301/810 [01:21<01:41,  5.03it/s]07/27/2022 10:49:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 302/810 [01:21<01:55,  4.41it/s]07/27/2022 10:49:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2022 10:49:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  37%|███▋      | 303/810 [01:21<02:05,  4.04it/s]07/27/2022 10:49:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 902 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 304/810 [01:21<02:13,  3.79it/s]07/27/2022 10:49:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 305/810 [01:22<02:00,  4.20it/s]07/27/2022 10:49:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 306/810 [01:22<01:48,  4.64it/s]07/27/2022 10:49:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:49:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 1036 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 307/810 [01:22<02:20,  3.59it/s]07/27/2022 10:50:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 1036 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 308/810 [01:23<02:44,  3.05it/s]07/27/2022 10:50:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 309/810 [01:23<02:25,  3.44it/s]07/27/2022 10:50:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 876 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 310/810 [01:23<02:25,  3.43it/s]07/27/2022 10:50:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 1159 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  38%|███▊      | 311/810 [01:24<02:46,  2.99it/s]07/27/2022 10:50:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▊      | 312/810 [01:24<02:42,  3.07it/s]07/27/2022 10:50:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▊      | 313/810 [01:24<02:37,  3.15it/s]07/27/2022 10:50:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▉      | 314/810 [01:24<02:15,  3.66it/s]07/27/2022 10:50:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 492 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▉      | 315/810 [01:25<01:58,  4.16it/s]07/27/2022 10:50:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 1229 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▉      | 316/810 [01:25<02:27,  3.35it/s]07/27/2022 10:50:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 1229 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▉      | 317/810 [01:25<02:49,  2.91it/s]07/27/2022 10:50:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 381 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▉      | 318/810 [01:26<02:24,  3.41it/s]07/27/2022 10:50:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  39%|███▉      | 319/810 [01:26<02:05,  3.91it/s]07/27/2022 10:50:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|███▉      | 320/810 [01:26<02:10,  3.76it/s]07/27/2022 10:50:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 1059 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|███▉      | 321/810 [01:27<02:35,  3.15it/s]07/27/2022 10:50:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 1059 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|███▉      | 322/810 [01:27<02:53,  2.82it/s]07/27/2022 10:50:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|███▉      | 323/810 [01:27<02:27,  3.30it/s]07/27/2022 10:50:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|████      | 324/810 [01:27<02:07,  3.82it/s]07/27/2022 10:50:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 1325 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|████      | 325/810 [01:28<02:30,  3.21it/s]07/27/2022 10:50:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 1325 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|████      | 326/810 [01:28<02:49,  2.85it/s]07/27/2022 10:50:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|████      | 327/810 [01:28<02:43,  2.96it/s]07/27/2022 10:50:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  40%|████      | 328/810 [01:29<02:36,  3.07it/s]07/27/2022 10:50:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████      | 329/810 [01:29<02:32,  3.16it/s]07/27/2022 10:50:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2022 10:50:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 1250 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████      | 330/810 [01:29<02:48,  2.84it/s]07/27/2022 10:50:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 1250 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████      | 331/810 [01:30<03:03,  2.62it/s]07/27/2022 10:50:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 663 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████      | 332/810 [01:30<02:51,  2.78it/s]07/27/2022 10:50:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████      | 333/810 [01:30<02:24,  3.30it/s]07/27/2022 10:50:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████      | 334/810 [01:31<02:22,  3.34it/s]07/27/2022 10:50:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 757 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████▏     | 335/810 [01:31<02:21,  3.35it/s]07/27/2022 10:50:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  41%|████▏     | 336/810 [01:31<02:03,  3.84it/s]07/27/2022 10:50:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 1283 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  42%|████▏     | 337/810 [01:32<02:26,  3.22it/s]07/27/2022 10:50:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 1283 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  42%|████▏     | 338/810 [01:32<02:45,  2.85it/s]07/27/2022 10:50:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  42%|████▏     | 339/810 [01:32<02:20,  3.35it/s]07/27/2022 10:50:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 998 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  42%|████▏     | 340/810 [01:33<02:19,  3.36it/s]07/27/2022 10:50:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 998 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  42%|████▏     | 341/810 [01:33<02:19,  3.36it/s]07/27/2022 10:50:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  42%|████▏     | 342/810 [01:33<02:57,  2.64it/s]07/27/2022 10:50:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  42%|████▏     | 343/810 [01:34<03:29,  2.22it/s]07/27/2022 10:50:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 241 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  42%|████▏     | 344/810 [01:34<02:53,  2.69it/s]07/27/2022 10:50:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  43%|████▎     | 345/810 [01:34<02:24,  3.22it/s]07/27/2022 10:50:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  43%|████▎     | 346/810 [01:35<02:59,  2.58it/s]07/27/2022 10:50:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  43%|████▎     | 347/810 [01:35<02:49,  2.73it/s]07/27/2022 10:50:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  43%|████▎     | 348/810 [01:35<02:22,  3.25it/s]07/27/2022 10:50:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  43%|████▎     | 349/810 [01:36<02:02,  3.77it/s]07/27/2022 10:50:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  43%|████▎     | 350/810 [01:36<01:47,  4.27it/s]07/27/2022 10:50:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  43%|████▎     | 351/810 [01:36<01:55,  3.98it/s]07/27/2022 10:50:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  43%|████▎     | 352/810 [01:36<01:44,  4.40it/s]07/27/2022 10:50:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  44%|████▎     | 353/810 [01:36<01:34,  4.81it/s]07/27/2022 10:50:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  44%|████▎     | 354/810 [01:37<01:46,  4.30it/s]07/27/2022 10:50:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  44%|████▍     | 355/810 [01:37<01:54,  3.96it/s]07/27/2022 10:50:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 1204 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  44%|████▍     | 356/810 [01:37<02:19,  3.25it/s]07/27/2022 10:50:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 1204 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  44%|████▍     | 357/810 [01:38<02:37,  2.87it/s]07/27/2022 10:50:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  44%|████▍     | 358/810 [01:38<02:31,  2.98it/s]07/27/2022 10:50:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 719 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  44%|████▍     | 359/810 [01:38<02:26,  3.08it/s]07/27/2022 10:50:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  44%|████▍     | 360/810 [01:39<02:22,  3.16it/s]07/27/2022 10:50:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  45%|████▍     | 361/810 [01:39<02:19,  3.22it/s]07/27/2022 10:50:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 649 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  45%|████▍     | 362/810 [01:39<02:17,  3.27it/s]07/27/2022 10:50:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 600 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  45%|████▍     | 363/810 [01:40<02:15,  3.29it/s]07/27/2022 10:50:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 973 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  45%|████▍     | 364/810 [01:40<02:15,  3.30it/s]07/27/2022 10:50:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  45%|████▌     | 365/810 [01:40<01:57,  3.80it/s]07/27/2022 10:50:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  45%|████▌     | 366/810 [01:40<01:43,  4.29it/s]07/27/2022 10:50:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 885 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  45%|████▌     | 367/810 [01:41<01:51,  3.98it/s]07/27/2022 10:50:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 994 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  45%|████▌     | 368/810 [01:41<01:57,  3.76it/s]07/27/2022 10:50:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 994 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▌     | 369/810 [01:41<02:01,  3.62it/s]07/27/2022 10:50:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 627 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▌     | 370/810 [01:41<02:04,  3.54it/s]07/27/2022 10:50:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▌     | 371/810 [01:42<01:49,  4.01it/s]07/27/2022 10:50:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▌     | 372/810 [01:42<01:37,  4.47it/s]07/27/2022 10:50:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▌     | 373/810 [01:42<01:29,  4.87it/s]07/27/2022 10:50:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 881 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▌     | 374/810 [01:42<01:40,  4.32it/s]07/27/2022 10:50:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▋     | 375/810 [01:42<01:32,  4.68it/s]07/27/2022 10:50:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 584 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  46%|████▋     | 376/810 [01:43<01:42,  4.22it/s]07/27/2022 10:50:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 377/810 [01:43<01:34,  4.59it/s]07/27/2022 10:50:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 378/810 [01:43<01:26,  4.97it/s]07/27/2022 10:50:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 311 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 379/810 [01:43<01:21,  5.28it/s]07/27/2022 10:50:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 380/810 [01:44<01:34,  4.55it/s]07/27/2022 10:50:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 381/810 [01:44<01:44,  4.09it/s]07/27/2022 10:50:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 382/810 [01:44<01:35,  4.50it/s]07/27/2022 10:50:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 420 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 383/810 [01:44<01:27,  4.89it/s]07/27/2022 10:50:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 449 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  47%|████▋     | 384/810 [01:44<01:21,  5.21it/s]07/27/2022 10:50:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  48%|████▊     | 385/810 [01:45<02:08,  3.31it/s]07/27/2022 10:50:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2022 10:50:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  48%|████▊     | 386/810 [01:45<01:53,  3.73it/s]07/27/2022 10:50:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  48%|████▊     | 387/810 [01:45<01:40,  4.20it/s]07/27/2022 10:50:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 518 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  48%|████▊     | 388/810 [01:46<01:47,  3.92it/s]07/27/2022 10:50:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  48%|████▊     | 389/810 [01:46<01:53,  3.71it/s]07/27/2022 10:50:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 699 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  48%|████▊     | 390/810 [01:46<01:56,  3.60it/s]07/27/2022 10:50:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  48%|████▊     | 391/810 [01:46<01:58,  3.53it/s]07/27/2022 10:50:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 540 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  48%|████▊     | 392/810 [01:47<02:00,  3.48it/s]07/27/2022 10:50:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 495 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  49%|████▊     | 393/810 [01:47<01:45,  3.96it/s]07/27/2022 10:50:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 882 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  49%|████▊     | 394/810 [01:47<01:50,  3.78it/s]07/27/2022 10:50:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 1004 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  49%|████▉     | 395/810 [01:47<01:54,  3.63it/s]07/27/2022 10:50:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 1004 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  49%|████▉     | 396/810 [01:48<01:57,  3.52it/s]07/27/2022 10:50:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "Iteration:  49%|████▉     | 397/810 [01:48<02:32,  2.71it/s]07/27/2022 10:50:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 779 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  49%|████▉     | 398/810 [01:49<02:25,  2.83it/s]07/27/2022 10:50:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 779 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  49%|████▉     | 399/810 [01:49<02:19,  2.95it/s]07/27/2022 10:50:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 566 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  49%|████▉     | 400/810 [01:49<02:13,  3.06it/s]07/27/2022 10:50:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 481 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|████▉     | 401/810 [01:49<01:54,  3.58it/s]07/27/2022 10:50:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 1258 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|████▉     | 402/810 [01:50<02:12,  3.09it/s]07/27/2022 10:50:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 1258 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|████▉     | 403/810 [01:50<02:26,  2.78it/s]07/27/2022 10:50:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 731 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|████▉     | 404/810 [01:51<02:19,  2.91it/s]07/27/2022 10:50:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|█████     | 405/810 [01:51<02:13,  3.03it/s]07/27/2022 10:50:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 360 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|█████     | 406/810 [01:51<01:54,  3.53it/s]07/27/2022 10:50:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|█████     | 407/810 [01:51<01:39,  4.05it/s]07/27/2022 10:50:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|█████     | 408/810 [01:51<01:28,  4.52it/s]07/27/2022 10:50:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  50%|█████     | 409/810 [01:52<01:21,  4.91it/s]07/27/2022 10:50:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 396 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  51%|█████     | 410/810 [01:52<01:16,  5.22it/s]07/27/2022 10:50:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  51%|█████     | 411/810 [01:52<01:13,  5.45it/s]07/27/2022 10:50:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  51%|█████     | 412/810 [01:52<01:10,  5.65it/s]07/27/2022 10:50:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  51%|█████     | 413/810 [01:52<01:08,  5.78it/s]07/27/2022 10:50:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  51%|█████     | 414/810 [01:52<01:07,  5.89it/s]07/27/2022 10:50:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 306 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  51%|█████     | 415/810 [01:53<01:06,  5.97it/s]07/27/2022 10:50:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  51%|█████▏    | 416/810 [01:53<01:05,  6.03it/s]07/27/2022 10:50:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  51%|█████▏    | 417/810 [01:53<01:05,  6.03it/s]07/27/2022 10:50:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 418/810 [01:53<01:04,  6.06it/s]07/27/2022 10:50:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 419/810 [01:53<01:05,  6.00it/s]07/27/2022 10:50:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 420/810 [01:53<01:04,  6.04it/s]07/27/2022 10:50:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 343 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 421/810 [01:54<01:04,  6.07it/s]07/27/2022 10:50:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 681 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 422/810 [01:54<01:19,  4.89it/s]07/27/2022 10:50:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 961 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 423/810 [01:54<01:30,  4.29it/s]07/27/2022 10:50:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 961 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 424/810 [01:54<01:37,  3.96it/s]07/27/2022 10:50:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 1106 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  52%|█████▏    | 425/810 [01:55<01:58,  3.26it/s]07/27/2022 10:50:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 1106 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 426/810 [01:55<02:13,  2.87it/s]07/27/2022 10:50:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 1113 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 427/810 [01:56<02:24,  2.65it/s]07/27/2022 10:50:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 1036 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 428/810 [01:56<02:32,  2.51it/s]07/27/2022 10:50:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 994 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 429/810 [01:57<02:21,  2.69it/s]07/27/2022 10:50:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 534 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 430/810 [01:57<02:12,  2.86it/s]07/27/2022 10:50:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 1338 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 431/810 [01:57<02:22,  2.67it/s]07/27/2022 10:50:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 1338 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 432/810 [01:58<02:29,  2.53it/s]07/27/2022 10:50:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 509 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  53%|█████▎    | 433/810 [01:58<02:04,  3.03it/s]07/27/2022 10:50:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 472 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  54%|█████▎    | 434/810 [01:58<01:45,  3.55it/s]07/27/2022 10:50:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  54%|█████▎    | 435/810 [01:58<01:32,  4.07it/s]07/27/2022 10:50:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 234 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  54%|█████▍    | 436/810 [01:58<01:22,  4.53it/s]07/27/2022 10:50:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  54%|█████▍    | 437/810 [01:59<01:15,  4.92it/s]07/27/2022 10:50:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  54%|█████▍    | 438/810 [01:59<01:11,  5.23it/s]07/27/2022 10:50:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  54%|█████▍    | 439/810 [01:59<01:07,  5.48it/s]07/27/2022 10:50:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  54%|█████▍    | 440/810 [01:59<01:05,  5.65it/s]07/27/2022 10:50:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  54%|█████▍    | 441/810 [01:59<01:04,  5.76it/s]07/27/2022 10:50:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▍    | 442/810 [01:59<01:02,  5.87it/s]07/27/2022 10:50:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 213 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▍    | 443/810 [01:59<01:01,  5.94it/s]07/27/2022 10:50:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 1296 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▍    | 444/810 [02:00<01:30,  4.05it/s]07/27/2022 10:50:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▍    | 445/810 [02:00<01:22,  4.42it/s]07/27/2022 10:50:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 295 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▌    | 446/810 [02:00<01:15,  4.81it/s]07/27/2022 10:50:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 194 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▌    | 447/810 [02:00<01:10,  5.14it/s]07/27/2022 10:50:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 194 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▌    | 448/810 [02:01<01:07,  5.38it/s]07/27/2022 10:50:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 328 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  55%|█████▌    | 449/810 [02:01<01:04,  5.60it/s]07/27/2022 10:50:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 339 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▌    | 450/810 [02:01<01:02,  5.75it/s]07/27/2022 10:50:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▌    | 451/810 [02:01<01:14,  4.79it/s]07/27/2022 10:50:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▌    | 452/810 [02:01<01:10,  5.07it/s]07/27/2022 10:50:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 668 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▌    | 453/810 [02:02<01:20,  4.43it/s]07/27/2022 10:50:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 528 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▌    | 454/810 [02:02<01:27,  4.05it/s]07/27/2022 10:50:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 1751 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▌    | 455/810 [02:03<02:01,  2.91it/s]07/27/2022 10:50:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▋    | 456/810 [02:03<01:58,  2.98it/s]07/27/2022 10:50:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 763 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  56%|█████▋    | 457/810 [02:03<01:54,  3.07it/s]07/27/2022 10:50:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 622 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 458/810 [02:03<01:51,  3.16it/s]07/27/2022 10:50:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 181 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 459/810 [02:04<01:35,  3.66it/s]07/27/2022 10:50:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 460/810 [02:04<01:24,  4.16it/s]07/27/2022 10:50:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 461/810 [02:04<01:15,  4.60it/s]07/27/2022 10:50:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 502 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 462/810 [02:04<01:10,  4.96it/s]07/27/2022 10:50:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 822 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 463/810 [02:04<01:19,  4.37it/s]07/27/2022 10:50:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 822 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 464/810 [02:05<01:26,  3.99it/s]07/27/2022 10:50:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 811 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  57%|█████▋    | 465/810 [02:05<01:31,  3.78it/s]07/27/2022 10:50:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 647 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  58%|█████▊    | 466/810 [02:05<01:35,  3.62it/s]07/27/2022 10:50:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 297 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  58%|█████▊    | 467/810 [02:05<01:23,  4.08it/s]07/27/2022 10:50:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  58%|█████▊    | 468/810 [02:06<01:28,  3.86it/s]07/27/2022 10:50:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 616 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  58%|█████▊    | 469/810 [02:06<01:32,  3.70it/s]07/27/2022 10:50:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  58%|█████▊    | 470/810 [02:06<01:34,  3.58it/s]07/27/2022 10:50:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 591 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  58%|█████▊    | 471/810 [02:07<01:36,  3.51it/s]07/27/2022 10:50:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 834 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  58%|█████▊    | 472/810 [02:07<01:37,  3.46it/s]07/27/2022 10:50:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 834 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  58%|█████▊    | 473/810 [02:07<01:38,  3.41it/s]07/27/2022 10:50:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 789 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▊    | 474/810 [02:08<01:38,  3.40it/s]07/27/2022 10:50:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▊    | 475/810 [02:08<01:39,  3.37it/s]07/27/2022 10:50:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 605 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▉    | 476/810 [02:08<01:38,  3.38it/s]07/27/2022 10:50:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▉    | 477/810 [02:08<01:39,  3.35it/s]07/27/2022 10:50:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 915 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▉    | 478/810 [02:09<01:39,  3.35it/s]07/27/2022 10:50:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 1117 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▉    | 479/810 [02:09<01:52,  2.95it/s]07/27/2022 10:50:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 1117 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▉    | 480/810 [02:10<02:02,  2.69it/s]07/27/2022 10:50:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  59%|█████▉    | 481/810 [02:10<01:43,  3.19it/s]07/27/2022 10:50:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|█████▉    | 482/810 [02:10<01:31,  3.57it/s]07/27/2022 10:50:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|█████▉    | 483/810 [02:10<01:19,  4.09it/s]07/27/2022 10:50:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 857 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|█████▉    | 484/810 [02:10<01:24,  3.86it/s]07/27/2022 10:50:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 857 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|█████▉    | 485/810 [02:11<01:28,  3.68it/s]07/27/2022 10:50:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|██████    | 486/810 [02:11<01:17,  4.16it/s]07/27/2022 10:50:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|██████    | 487/810 [02:11<01:10,  4.60it/s]07/27/2022 10:50:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|██████    | 488/810 [02:11<01:04,  4.97it/s]07/27/2022 10:50:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 309 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|██████    | 489/810 [02:11<01:00,  5.28it/s]07/27/2022 10:50:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  60%|██████    | 490/810 [02:12<00:58,  5.52it/s]07/27/2022 10:50:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 1046 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  61%|██████    | 491/810 [02:12<01:21,  3.93it/s]07/27/2022 10:50:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  61%|██████    | 492/810 [02:12<01:13,  4.32it/s]07/27/2022 10:50:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 1079 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  61%|██████    | 493/810 [02:13<01:32,  3.44it/s]07/27/2022 10:50:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 353 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  61%|██████    | 494/810 [02:13<01:21,  3.89it/s]07/27/2022 10:50:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 837 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  61%|██████    | 495/810 [02:13<01:24,  3.72it/s]07/27/2022 10:50:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 1201 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  61%|██████    | 496/810 [02:14<01:39,  3.14it/s]07/27/2022 10:50:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 1201 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  61%|██████▏   | 497/810 [02:14<01:51,  2.80it/s]07/27/2022 10:50:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  61%|██████▏   | 498/810 [02:14<01:46,  2.93it/s]07/27/2022 10:50:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 559 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 499/810 [02:15<01:42,  3.05it/s]07/27/2022 10:50:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 560 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 500/810 [02:15<01:38,  3.14it/s]07/27/2022 10:50:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 797 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 501/810 [02:15<01:36,  3.19it/s]07/27/2022 10:50:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 797 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 502/810 [02:15<01:34,  3.24it/s]07/27/2022 10:50:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 503/810 [02:16<01:33,  3.27it/s]07/27/2022 10:50:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 300 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 504/810 [02:16<01:21,  3.77it/s]07/27/2022 10:50:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 505/810 [02:16<01:11,  4.27it/s]07/27/2022 10:50:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 280 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  62%|██████▏   | 506/810 [02:16<01:04,  4.69it/s]07/27/2022 10:50:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 507/810 [02:16<00:59,  5.06it/s]07/27/2022 10:50:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 406 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 508/810 [02:17<00:56,  5.34it/s]07/27/2022 10:50:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 509/810 [02:17<00:54,  5.55it/s]07/27/2022 10:50:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 510/810 [02:17<00:52,  5.71it/s]07/27/2022 10:50:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 287 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 511/810 [02:17<00:51,  5.84it/s]07/27/2022 10:50:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 634 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 512/810 [02:17<01:01,  4.83it/s]07/27/2022 10:50:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 513/810 [02:18<01:09,  4.27it/s]07/27/2022 10:50:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 819 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  63%|██████▎   | 514/810 [02:18<01:14,  3.95it/s]07/27/2022 10:50:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  64%|██████▎   | 515/810 [02:18<01:07,  4.37it/s]07/27/2022 10:50:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  64%|██████▎   | 516/810 [02:18<01:01,  4.78it/s]07/27/2022 10:50:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  64%|██████▍   | 517/810 [02:18<00:57,  5.11it/s]07/27/2022 10:50:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  64%|██████▍   | 518/810 [02:19<00:54,  5.36it/s]07/27/2022 10:50:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 655 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  64%|██████▍   | 519/810 [02:19<01:03,  4.59it/s]07/27/2022 10:50:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  64%|██████▍   | 520/810 [02:19<01:10,  4.14it/s]07/27/2022 10:50:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 903 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  64%|██████▍   | 521/810 [02:20<01:14,  3.86it/s]07/27/2022 10:50:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 903 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  64%|██████▍   | 522/810 [02:20<01:18,  3.66it/s]07/27/2022 10:50:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▍   | 523/810 [02:20<01:20,  3.57it/s]07/27/2022 10:50:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▍   | 524/810 [02:20<01:21,  3.50it/s]07/27/2022 10:50:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▍   | 525/810 [02:21<01:11,  3.98it/s]07/27/2022 10:50:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▍   | 526/810 [02:21<01:03,  4.45it/s]07/27/2022 10:50:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▌   | 527/810 [02:21<00:58,  4.85it/s]07/27/2022 10:50:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▌   | 528/810 [02:21<01:05,  4.32it/s]07/27/2022 10:50:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▌   | 529/810 [02:22<01:10,  3.98it/s]07/27/2022 10:50:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 769 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  65%|██████▌   | 530/810 [02:22<01:14,  3.77it/s]07/27/2022 10:50:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:50:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▌   | 531/810 [02:22<01:05,  4.23it/s]07/27/2022 10:51:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▌   | 532/810 [02:22<00:59,  4.66it/s]07/27/2022 10:51:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 594 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▌   | 533/810 [02:22<01:05,  4.21it/s]07/27/2022 10:51:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▌   | 534/810 [02:23<01:00,  4.60it/s]07/27/2022 10:51:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▌   | 535/810 [02:23<00:55,  4.98it/s]07/27/2022 10:51:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 342 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▌   | 536/810 [02:23<00:51,  5.28it/s]07/27/2022 10:51:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 704 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▋   | 537/810 [02:23<01:00,  4.55it/s]07/27/2022 10:51:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  66%|██████▋   | 538/810 [02:24<01:06,  4.11it/s]07/27/2022 10:51:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 198 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  67%|██████▋   | 539/810 [02:24<01:00,  4.50it/s]07/27/2022 10:51:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 940 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  67%|██████▋   | 540/810 [02:24<01:05,  4.10it/s]07/27/2022 10:51:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  67%|██████▋   | 541/810 [02:24<00:59,  4.50it/s]07/27/2022 10:51:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 698 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  67%|██████▋   | 542/810 [02:24<01:05,  4.11it/s]07/27/2022 10:51:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  67%|██████▋   | 543/810 [02:25<01:09,  3.84it/s]07/27/2022 10:51:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 928 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  67%|██████▋   | 544/810 [02:25<01:12,  3.68it/s]07/27/2022 10:51:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 582 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  67%|██████▋   | 545/810 [02:25<01:14,  3.56it/s]07/27/2022 10:51:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 781 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  67%|██████▋   | 546/810 [02:26<01:15,  3.50it/s]07/27/2022 10:51:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  68%|██████▊   | 547/810 [02:26<01:06,  3.98it/s]07/27/2022 10:51:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 252 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  68%|██████▊   | 548/810 [02:26<00:58,  4.45it/s]07/27/2022 10:51:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  68%|██████▊   | 549/810 [02:26<01:03,  4.08it/s]07/27/2022 10:51:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 686 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  68%|██████▊   | 550/810 [02:27<01:07,  3.83it/s]07/27/2022 10:51:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  68%|██████▊   | 551/810 [02:27<01:10,  3.68it/s]07/27/2022 10:51:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 682 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  68%|██████▊   | 552/810 [02:27<01:12,  3.57it/s]07/27/2022 10:51:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  68%|██████▊   | 553/810 [02:27<01:03,  4.04it/s]07/27/2022 10:51:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 874 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  68%|██████▊   | 554/810 [02:28<01:06,  3.83it/s]07/27/2022 10:51:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 874 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▊   | 555/810 [02:28<01:09,  3.67it/s]07/27/2022 10:51:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 1040 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▊   | 556/810 [02:28<01:21,  3.11it/s]07/27/2022 10:51:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 391 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▉   | 557/810 [02:29<01:10,  3.59it/s]07/27/2022 10:51:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▉   | 558/810 [02:29<01:01,  4.09it/s]07/27/2022 10:51:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▉   | 559/810 [02:29<01:04,  3.87it/s]07/27/2022 10:51:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 574 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▉   | 560/810 [02:29<01:07,  3.69it/s]07/27/2022 10:51:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 314 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▉   | 561/810 [02:29<00:59,  4.15it/s]07/27/2022 10:51:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  69%|██████▉   | 562/810 [02:30<00:54,  4.58it/s]07/27/2022 10:51:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|██████▉   | 563/810 [02:30<00:59,  4.15it/s]07/27/2022 10:51:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|██████▉   | 564/810 [02:30<01:03,  3.88it/s]07/27/2022 10:51:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 487 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|██████▉   | 565/810 [02:30<00:56,  4.32it/s]07/27/2022 10:51:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 373 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|██████▉   | 566/810 [02:31<00:51,  4.74it/s]07/27/2022 10:51:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|███████   | 567/810 [02:31<00:57,  4.24it/s]07/27/2022 10:51:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|███████   | 568/810 [02:31<00:52,  4.62it/s]07/27/2022 10:51:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 379 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|███████   | 569/810 [02:31<00:48,  4.97it/s]07/27/2022 10:51:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 1470 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|███████   | 570/810 [02:32<01:04,  3.72it/s]07/27/2022 10:51:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 1470 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  70%|███████   | 571/810 [02:32<01:16,  3.11it/s]07/27/2022 10:51:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  71%|███████   | 572/810 [02:32<01:15,  3.16it/s]07/27/2022 10:51:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 729 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  71%|███████   | 573/810 [02:33<01:13,  3.22it/s]07/27/2022 10:51:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 566 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  71%|███████   | 574/810 [02:33<01:13,  3.19it/s]07/27/2022 10:51:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  71%|███████   | 575/810 [02:33<01:03,  3.70it/s]07/27/2022 10:51:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 372 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  71%|███████   | 576/810 [02:33<00:55,  4.20it/s]07/27/2022 10:51:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 735 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  71%|███████   | 577/810 [02:34<00:59,  3.94it/s]07/27/2022 10:51:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 263 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  71%|███████▏  | 578/810 [02:34<00:53,  4.37it/s]07/27/2022 10:51:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 678 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  71%|███████▏  | 579/810 [02:34<00:57,  4.04it/s]07/27/2022 10:51:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 580/810 [02:34<00:51,  4.46it/s]07/27/2022 10:51:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 501 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 581/810 [02:34<00:47,  4.86it/s]07/27/2022 10:51:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 582/810 [02:35<00:43,  5.19it/s]07/27/2022 10:51:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 202 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 583/810 [02:35<00:41,  5.45it/s]07/27/2022 10:51:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 584/810 [02:35<00:40,  5.63it/s]07/27/2022 10:51:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 585/810 [02:35<00:38,  5.78it/s]07/27/2022 10:51:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 766 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 586/810 [02:35<00:46,  4.79it/s]07/27/2022 10:51:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  72%|███████▏  | 587/810 [02:36<00:52,  4.24it/s]07/27/2022 10:51:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 813 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 588/810 [02:36<00:56,  3.93it/s]07/27/2022 10:51:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 838 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 589/810 [02:36<00:59,  3.73it/s]07/27/2022 10:51:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 590/810 [02:36<00:52,  4.19it/s]07/27/2022 10:51:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 591/810 [02:37<00:47,  4.58it/s]07/27/2022 10:51:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 592/810 [02:37<00:44,  4.95it/s]07/27/2022 10:51:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 593/810 [02:37<00:49,  4.37it/s]07/27/2022 10:51:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 594/810 [02:37<00:45,  4.72it/s]07/27/2022 10:51:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 491 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  73%|███████▎  | 595/810 [02:37<00:42,  5.07it/s]07/27/2022 10:51:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  74%|███████▎  | 596/810 [02:38<00:40,  5.35it/s]07/27/2022 10:51:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  74%|███████▎  | 597/810 [02:38<00:38,  5.52it/s]07/27/2022 10:51:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  74%|███████▍  | 598/810 [02:38<00:37,  5.70it/s]07/27/2022 10:51:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 624 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  74%|███████▍  | 599/810 [02:38<00:44,  4.76it/s]07/27/2022 10:51:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 1098 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  74%|███████▍  | 600/810 [02:39<00:58,  3.61it/s]07/27/2022 10:51:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 1098 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  74%|███████▍  | 601/810 [02:39<01:08,  3.06it/s]07/27/2022 10:51:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 1681 to 2048 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  74%|███████▍  | 602/810 [02:40<01:24,  2.47it/s]07/27/2022 10:51:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  74%|███████▍  | 603/810 [02:40<01:10,  2.94it/s]07/27/2022 10:51:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▍  | 604/810 [02:40<00:59,  3.47it/s]07/27/2022 10:51:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 2027 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▍  | 605/810 [02:41<01:16,  2.69it/s]07/27/2022 10:51:18 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:18 - INFO - model.QACGLONG -   Input ids are automatically padded from 2027 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▍  | 606/810 [02:41<01:29,  2.28it/s]07/27/2022 10:51:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▍  | 607/810 [02:41<01:13,  2.75it/s]07/27/2022 10:51:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▌  | 608/810 [02:42<01:01,  3.28it/s]07/27/2022 10:51:19 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:19 - INFO - model.QACGLONG -   Input ids are automatically padded from 1133 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▌  | 609/810 [02:42<01:08,  2.92it/s]07/27/2022 10:51:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 650 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▌  | 610/810 [02:42<01:06,  3.01it/s]07/27/2022 10:51:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  75%|███████▌  | 611/810 [02:43<01:04,  3.11it/s]07/27/2022 10:51:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 675 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  76%|███████▌  | 612/810 [02:43<01:03,  3.10it/s]07/27/2022 10:51:20 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:20 - INFO - model.QACGLONG -   Input ids are automatically padded from 721 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  76%|███████▌  | 613/810 [02:43<01:02,  3.17it/s]07/27/2022 10:51:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 721 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  76%|███████▌  | 614/810 [02:43<01:00,  3.23it/s]07/27/2022 10:51:21 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:21 - INFO - model.QACGLONG -   Input ids are automatically padded from 1040 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  76%|███████▌  | 615/810 [02:44<01:07,  2.88it/s]07/27/2022 10:51:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 1040 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  76%|███████▌  | 616/810 [02:44<01:12,  2.66it/s]07/27/2022 10:51:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  76%|███████▌  | 617/810 [02:45<01:01,  3.16it/s]07/27/2022 10:51:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 460 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  76%|███████▋  | 618/810 [02:45<00:52,  3.68it/s]07/27/2022 10:51:22 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:22 - INFO - model.QACGLONG -   Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  76%|███████▋  | 619/810 [02:45<00:52,  3.61it/s]07/27/2022 10:51:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 620/810 [02:45<00:53,  3.53it/s]07/27/2022 10:51:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 748 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 621/810 [02:46<00:54,  3.48it/s]07/27/2022 10:51:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 363 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 622/810 [02:46<00:47,  3.95it/s]07/27/2022 10:51:23 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:23 - INFO - model.QACGLONG -   Input ids are automatically padded from 217 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 623/810 [02:46<00:42,  4.42it/s]07/27/2022 10:51:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 1091 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 624/810 [02:46<00:53,  3.50it/s]07/27/2022 10:51:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 959 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 625/810 [02:47<00:54,  3.42it/s]07/27/2022 10:51:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 626/810 [02:47<00:47,  3.91it/s]07/27/2022 10:51:24 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:24 - INFO - model.QACGLONG -   Input ids are automatically padded from 587 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  77%|███████▋  | 627/810 [02:47<00:48,  3.76it/s]07/27/2022 10:51:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  78%|███████▊  | 628/810 [02:47<00:50,  3.62it/s]07/27/2022 10:51:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 618 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  78%|███████▊  | 629/810 [02:48<00:51,  3.53it/s]07/27/2022 10:51:25 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:25 - INFO - model.QACGLONG -   Input ids are automatically padded from 660 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  78%|███████▊  | 630/810 [02:48<00:51,  3.48it/s]07/27/2022 10:51:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 1179 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  78%|███████▊  | 631/810 [02:48<00:59,  3.02it/s]07/27/2022 10:51:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 866 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  78%|███████▊  | 632/810 [02:49<00:57,  3.07it/s]07/27/2022 10:51:26 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:26 - INFO - model.QACGLONG -   Input ids are automatically padded from 866 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  78%|███████▊  | 633/810 [02:49<00:56,  3.14it/s]07/27/2022 10:51:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 500 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  78%|███████▊  | 634/810 [02:49<00:48,  3.65it/s]07/27/2022 10:51:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  78%|███████▊  | 635/810 [02:49<00:42,  4.15it/s]07/27/2022 10:51:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▊  | 636/810 [02:50<00:37,  4.60it/s]07/27/2022 10:51:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▊  | 637/810 [02:50<00:34,  4.97it/s]07/27/2022 10:51:27 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:27 - INFO - model.QACGLONG -   Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▉  | 638/810 [02:50<00:39,  4.38it/s]07/27/2022 10:51:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 708 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▉  | 639/810 [02:50<00:42,  4.02it/s]07/27/2022 10:51:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 550 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▉  | 640/810 [02:51<00:44,  3.80it/s]07/27/2022 10:51:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 454 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▉  | 641/810 [02:51<00:39,  4.23it/s]07/27/2022 10:51:28 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:28 - INFO - model.QACGLONG -   Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▉  | 642/810 [02:51<00:36,  4.66it/s]07/27/2022 10:51:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  79%|███████▉  | 643/810 [02:51<00:39,  4.21it/s]07/27/2022 10:51:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 588 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|███████▉  | 644/810 [02:52<00:42,  3.91it/s]07/27/2022 10:51:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|███████▉  | 645/810 [02:52<00:44,  3.73it/s]07/27/2022 10:51:29 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:29 - INFO - model.QACGLONG -   Input ids are automatically padded from 546 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|███████▉  | 646/810 [02:52<00:45,  3.61it/s]07/27/2022 10:51:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 452 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|███████▉  | 647/810 [02:52<00:39,  4.09it/s]07/27/2022 10:51:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|████████  | 648/810 [02:52<00:35,  4.55it/s]07/27/2022 10:51:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 319 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|████████  | 649/810 [02:53<00:32,  4.93it/s]07/27/2022 10:51:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 265 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|████████  | 650/810 [02:53<00:30,  5.24it/s]07/27/2022 10:51:30 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:30 - INFO - model.QACGLONG -   Input ids are automatically padded from 923 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|████████  | 651/810 [02:53<00:35,  4.51it/s]07/27/2022 10:51:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 1141 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  80%|████████  | 652/810 [02:53<00:45,  3.50it/s]07/27/2022 10:51:31 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:31 - INFO - model.QACGLONG -   Input ids are automatically padded from 1141 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████  | 653/810 [02:54<00:52,  3.00it/s]07/27/2022 10:51:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 1092 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████  | 654/810 [02:54<00:57,  2.73it/s]07/27/2022 10:51:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 564 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████  | 655/810 [02:55<00:54,  2.87it/s]07/27/2022 10:51:32 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:32 - INFO - model.QACGLONG -   Input ids are automatically padded from 893 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  81%|████████  | 656/810 [02:55<00:51,  2.99it/s]07/27/2022 10:51:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 421 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████  | 657/810 [02:55<00:43,  3.52it/s]07/27/2022 10:51:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████  | 658/810 [02:55<00:37,  4.02it/s]07/27/2022 10:51:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████▏ | 659/810 [02:56<00:39,  3.82it/s]07/27/2022 10:51:33 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:33 - INFO - model.QACGLONG -   Input ids are automatically padded from 879 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  81%|████████▏ | 660/810 [02:56<00:41,  3.64it/s]07/27/2022 10:51:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 661/810 [02:56<00:42,  3.54it/s]07/27/2022 10:51:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 938 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 662/810 [02:57<00:42,  3.48it/s]07/27/2022 10:51:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 663/810 [02:57<00:42,  3.44it/s]07/27/2022 10:51:34 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:34 - INFO - model.QACGLONG -   Input ids are automatically padded from 638 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 664/810 [02:57<00:42,  3.42it/s]07/27/2022 10:51:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 1029 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 665/810 [02:58<00:48,  2.97it/s]07/27/2022 10:51:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 570 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 666/810 [02:58<00:47,  3.05it/s]07/27/2022 10:51:35 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:35 - INFO - model.QACGLONG -   Input ids are automatically padded from 667 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 667/810 [02:58<00:45,  3.14it/s]07/27/2022 10:51:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  82%|████████▏ | 668/810 [02:58<00:44,  3.20it/s]07/27/2022 10:51:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 761 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 669/810 [02:59<00:43,  3.24it/s]07/27/2022 10:51:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:36 - INFO - model.QACGLONG -   Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 670/810 [02:59<00:42,  3.28it/s]07/27/2022 10:51:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 549 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 671/810 [02:59<00:42,  3.30it/s]07/27/2022 10:51:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 672/810 [03:00<00:36,  3.79it/s]07/27/2022 10:51:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 637 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 673/810 [03:00<00:37,  3.68it/s]07/27/2022 10:51:37 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:37 - INFO - model.QACGLONG -   Input ids are automatically padded from 825 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 674/810 [03:00<00:38,  3.57it/s]07/27/2022 10:51:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 825 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 675/810 [03:00<00:38,  3.50it/s]07/27/2022 10:51:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 220 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  83%|████████▎ | 676/810 [03:01<00:33,  3.98it/s]07/27/2022 10:51:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 212 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  84%|████████▎ | 677/810 [03:01<00:29,  4.45it/s]07/27/2022 10:51:38 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:38 - INFO - model.QACGLONG -   Input ids are automatically padded from 1731 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  84%|████████▎ | 678/810 [03:01<00:42,  3.07it/s]07/27/2022 10:51:39 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:39 - INFO - model.QACGLONG -   Input ids are automatically padded from 1731 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  84%|████████▍ | 679/810 [03:02<00:53,  2.46it/s]07/27/2022 10:51:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  84%|████████▍ | 680/810 [03:02<00:49,  2.63it/s]07/27/2022 10:51:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 684 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  84%|████████▍ | 681/810 [03:03<00:45,  2.80it/s]07/27/2022 10:51:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 726 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  84%|████████▍ | 682/810 [03:03<00:43,  2.95it/s]07/27/2022 10:51:40 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:40 - INFO - model.QACGLONG -   Input ids are automatically padded from 347 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  84%|████████▍ | 683/810 [03:03<00:36,  3.47it/s]07/27/2022 10:51:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 290 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  84%|████████▍ | 684/810 [03:03<00:31,  3.99it/s]07/27/2022 10:51:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 806 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▍ | 685/810 [03:03<00:33,  3.78it/s]07/27/2022 10:51:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 806 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▍ | 686/810 [03:04<00:34,  3.64it/s]07/27/2022 10:51:41 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:41 - INFO - model.QACGLONG -   Input ids are automatically padded from 568 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▍ | 687/810 [03:04<00:34,  3.55it/s]07/27/2022 10:51:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 826 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▍ | 688/810 [03:04<00:35,  3.48it/s]07/27/2022 10:51:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 835 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▌ | 689/810 [03:05<00:35,  3.43it/s]07/27/2022 10:51:42 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:42 - INFO - model.QACGLONG -   Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▌ | 690/810 [03:05<00:35,  3.40it/s]07/27/2022 10:51:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▌ | 691/810 [03:05<00:35,  3.38it/s]07/27/2022 10:51:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 518 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  85%|████████▌ | 692/810 [03:06<00:35,  3.37it/s]07/27/2022 10:51:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 242 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▌ | 693/810 [03:06<00:30,  3.86it/s]07/27/2022 10:51:43 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:43 - INFO - model.QACGLONG -   Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▌ | 694/810 [03:06<00:26,  4.34it/s]07/27/2022 10:51:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 721 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▌ | 695/810 [03:06<00:28,  4.02it/s]07/27/2022 10:51:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 721 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▌ | 696/810 [03:06<00:30,  3.79it/s]07/27/2022 10:51:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 511 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▌ | 697/810 [03:07<00:26,  4.23it/s]07/27/2022 10:51:44 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:44 - INFO - model.QACGLONG -   Input ids are automatically padded from 593 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▌ | 698/810 [03:07<00:28,  3.96it/s]07/27/2022 10:51:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▋ | 699/810 [03:07<00:25,  4.37it/s]07/27/2022 10:51:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 443 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  86%|████████▋ | 700/810 [03:07<00:22,  4.78it/s]07/27/2022 10:51:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 1260 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 701/810 [03:08<00:29,  3.65it/s]07/27/2022 10:51:45 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:45 - INFO - model.QACGLONG -   Input ids are automatically padded from 1260 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 702/810 [03:08<00:35,  3.08it/s]07/27/2022 10:51:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 952 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 703/810 [03:08<00:34,  3.11it/s]07/27/2022 10:51:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 369 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 704/810 [03:09<00:29,  3.62it/s]07/27/2022 10:51:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 312 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 705/810 [03:09<00:25,  4.12it/s]07/27/2022 10:51:46 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:46 - INFO - model.QACGLONG -   Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 706/810 [03:09<00:22,  4.57it/s]07/27/2022 10:51:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 707/810 [03:09<00:20,  4.94it/s]07/27/2022 10:51:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 1373 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  87%|████████▋ | 708/810 [03:10<00:27,  3.71it/s]07/27/2022 10:51:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 348 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 709/810 [03:10<00:24,  4.12it/s]07/27/2022 10:51:47 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:47 - INFO - model.QACGLONG -   Input ids are automatically padded from 630 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  88%|████████▊ | 710/810 [03:10<00:25,  3.87it/s]07/27/2022 10:51:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 413 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 711/810 [03:10<00:23,  4.30it/s]07/27/2022 10:51:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 712/810 [03:10<00:20,  4.70it/s]07/27/2022 10:51:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 713/810 [03:11<00:22,  4.23it/s]07/27/2022 10:51:48 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:48 - INFO - model.QACGLONG -   Input ids are automatically padded from 762 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 714/810 [03:11<00:24,  3.93it/s]07/27/2022 10:51:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 715/810 [03:11<00:25,  3.74it/s]07/27/2022 10:51:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 409 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  88%|████████▊ | 716/810 [03:11<00:22,  4.20it/s]07/27/2022 10:51:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 598 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  89%|████████▊ | 717/810 [03:12<00:23,  3.93it/s]07/27/2022 10:51:49 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:49 - INFO - model.QACGLONG -   Input ids are automatically padded from 1306 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  89%|████████▊ | 718/810 [03:12<00:28,  3.24it/s]07/27/2022 10:51:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 1306 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  89%|████████▉ | 719/810 [03:13<00:31,  2.87it/s]07/27/2022 10:51:50 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:50 - INFO - model.QACGLONG -   Input ids are automatically padded from 1082 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  89%|████████▉ | 720/810 [03:13<00:33,  2.65it/s]07/27/2022 10:51:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 1082 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  89%|████████▉ | 721/810 [03:13<00:35,  2.51it/s]07/27/2022 10:51:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  89%|████████▉ | 722/810 [03:14<00:29,  3.01it/s]07/27/2022 10:51:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 336 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  89%|████████▉ | 723/810 [03:14<00:24,  3.52it/s]07/27/2022 10:51:51 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:51 - INFO - model.QACGLONG -   Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  89%|████████▉ | 724/810 [03:14<00:24,  3.49it/s]07/27/2022 10:51:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|████████▉ | 725/810 [03:14<00:24,  3.44it/s]07/27/2022 10:51:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 892 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|████████▉ | 726/810 [03:15<00:24,  3.41it/s]07/27/2022 10:51:52 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:52 - INFO - model.QACGLONG -   Input ids are automatically padded from 358 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|████████▉ | 727/810 [03:15<00:21,  3.89it/s]07/27/2022 10:51:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|████████▉ | 728/810 [03:15<00:18,  4.37it/s]07/27/2022 10:51:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 302 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|█████████ | 729/810 [03:15<00:17,  4.53it/s]07/27/2022 10:51:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|█████████ | 730/810 [03:15<00:16,  4.91it/s]07/27/2022 10:51:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|█████████ | 731/810 [03:16<00:15,  5.21it/s]07/27/2022 10:51:53 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:53 - INFO - model.QACGLONG -   Input ids are automatically padded from 1145 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|█████████ | 732/810 [03:16<00:20,  3.81it/s]07/27/2022 10:51:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 1145 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  90%|█████████ | 733/810 [03:16<00:24,  3.15it/s]07/27/2022 10:51:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  91%|█████████ | 734/810 [03:17<00:23,  3.18it/s]07/27/2022 10:51:54 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:54 - INFO - model.QACGLONG -   Input ids are automatically padded from 751 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  91%|█████████ | 735/810 [03:17<00:23,  3.23it/s]07/27/2022 10:51:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 724 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  91%|█████████ | 736/810 [03:17<00:22,  3.27it/s]07/27/2022 10:51:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 724 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  91%|█████████ | 737/810 [03:18<00:22,  3.30it/s]07/27/2022 10:51:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 458 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  91%|█████████ | 738/810 [03:18<00:19,  3.77it/s]07/27/2022 10:51:55 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:55 - INFO - model.QACGLONG -   Input ids are automatically padded from 422 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  91%|█████████ | 739/810 [03:18<00:16,  4.26it/s]07/27/2022 10:51:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 1130 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  91%|█████████▏| 740/810 [03:18<00:20,  3.42it/s]07/27/2022 10:51:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  91%|█████████▏| 741/810 [03:19<00:20,  3.38it/s]07/27/2022 10:51:56 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:56 - INFO - model.QACGLONG -   Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 742/810 [03:19<00:20,  3.37it/s]07/27/2022 10:51:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 503 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 743/810 [03:19<00:17,  3.86it/s]07/27/2022 10:51:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 744/810 [03:19<00:17,  3.71it/s]07/27/2022 10:51:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 612 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 745/810 [03:20<00:18,  3.60it/s]07/27/2022 10:51:57 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:57 - INFO - model.QACGLONG -   Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 746/810 [03:20<00:18,  3.50it/s]07/27/2022 10:51:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 921 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 747/810 [03:20<00:18,  3.45it/s]07/27/2022 10:51:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 748/810 [03:21<00:15,  3.94it/s]07/27/2022 10:51:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 434 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  92%|█████████▏| 749/810 [03:21<00:13,  4.40it/s]07/27/2022 10:51:58 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:58 - INFO - model.QACGLONG -   Input ids are automatically padded from 1353 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  93%|█████████▎| 750/810 [03:21<00:17,  3.49it/s]07/27/2022 10:51:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 1403 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  93%|█████████▎| 751/810 [03:22<00:19,  2.96it/s]07/27/2022 10:51:59 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:51:59 - INFO - model.QACGLONG -   Input ids are automatically padded from 1494 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  93%|█████████▎| 752/810 [03:22<00:21,  2.70it/s]07/27/2022 10:52:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 1494 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  93%|█████████▎| 753/810 [03:23<00:22,  2.55it/s]07/27/2022 10:52:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  93%|█████████▎| 754/810 [03:23<00:18,  3.05it/s]07/27/2022 10:52:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 508 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  93%|█████████▎| 755/810 [03:23<00:15,  3.58it/s]07/27/2022 10:52:00 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:00 - INFO - model.QACGLONG -   Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  93%|█████████▎| 756/810 [03:23<00:15,  3.53it/s]07/27/2022 10:52:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 1185 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  93%|█████████▎| 757/810 [03:24<00:17,  3.04it/s]07/27/2022 10:52:01 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:01 - INFO - model.QACGLONG -   Input ids are automatically padded from 1185 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▎| 758/810 [03:24<00:18,  2.75it/s]07/27/2022 10:52:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 1057 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▎| 759/810 [03:24<00:19,  2.58it/s]07/27/2022 10:52:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 931 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▍| 760/810 [03:25<00:18,  2.75it/s]07/27/2022 10:52:02 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:02 - INFO - model.QACGLONG -   Input ids are automatically padded from 896 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▍| 761/810 [03:25<00:16,  2.90it/s]07/27/2022 10:52:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 896 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▍| 762/810 [03:25<00:15,  3.02it/s]07/27/2022 10:52:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 651 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▍| 763/810 [03:26<00:15,  3.11it/s]07/27/2022 10:52:03 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:03 - INFO - model.QACGLONG -   Input ids are automatically padded from 644 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  94%|█████████▍| 764/810 [03:26<00:14,  3.16it/s]07/27/2022 10:52:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 1236 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  94%|█████████▍| 765/810 [03:26<00:15,  2.85it/s]07/27/2022 10:52:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 1236 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▍| 766/810 [03:27<00:16,  2.64it/s]07/27/2022 10:52:04 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:04 - INFO - model.QACGLONG -   Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▍| 767/810 [03:27<00:13,  3.14it/s]07/27/2022 10:52:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▍| 768/810 [03:27<00:13,  3.22it/s]07/27/2022 10:52:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 545 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▍| 769/810 [03:28<00:12,  3.26it/s]07/27/2022 10:52:05 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:05 - INFO - model.QACGLONG -   Input ids are automatically padded from 841 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▌| 770/810 [03:28<00:12,  3.27it/s]07/27/2022 10:52:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▌| 771/810 [03:28<00:10,  3.76it/s]07/27/2022 10:52:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 361 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▌| 772/810 [03:28<00:08,  4.25it/s]07/27/2022 10:52:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 776 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  95%|█████████▌| 773/810 [03:29<00:09,  3.97it/s]07/27/2022 10:52:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 776 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▌| 774/810 [03:29<00:09,  3.77it/s]07/27/2022 10:52:06 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:06 - INFO - model.QACGLONG -   Input ids are automatically padded from 817 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▌| 775/810 [03:29<00:09,  3.64it/s]07/27/2022 10:52:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 817 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▌| 776/810 [03:29<00:09,  3.55it/s]07/27/2022 10:52:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 710 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▌| 777/810 [03:30<00:09,  3.49it/s]07/27/2022 10:52:07 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:07 - INFO - model.QACGLONG -   Input ids are automatically padded from 282 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▌| 778/810 [03:30<00:08,  3.96it/s]07/27/2022 10:52:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▌| 779/810 [03:30<00:06,  4.43it/s]07/27/2022 10:52:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 456 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▋| 780/810 [03:30<00:06,  4.82it/s]07/27/2022 10:52:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 568 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  96%|█████████▋| 781/810 [03:31<00:06,  4.30it/s]07/27/2022 10:52:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 955 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 782/810 [03:31<00:07,  3.96it/s]07/27/2022 10:52:08 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:08 - INFO - model.QACGLONG -   Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 783/810 [03:31<00:06,  4.39it/s]07/27/2022 10:52:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 463 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 784/810 [03:31<00:05,  4.80it/s]07/27/2022 10:52:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 335 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 785/810 [03:31<00:04,  5.12it/s]07/27/2022 10:52:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 635 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 786/810 [03:32<00:05,  4.45it/s]07/27/2022 10:52:09 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:09 - INFO - model.QACGLONG -   Input ids are automatically padded from 635 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 787/810 [03:32<00:05,  4.03it/s]07/27/2022 10:52:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 788/810 [03:32<00:04,  4.45it/s]07/27/2022 10:52:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 532 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  97%|█████████▋| 789/810 [03:32<00:05,  4.09it/s]07/27/2022 10:52:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  98%|█████████▊| 790/810 [03:33<00:05,  3.84it/s]07/27/2022 10:52:10 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:10 - INFO - model.QACGLONG -   Input ids are automatically padded from 542 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  98%|█████████▊| 791/810 [03:33<00:05,  3.68it/s]07/27/2022 10:52:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 1186 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  98%|█████████▊| 792/810 [03:33<00:05,  3.12it/s]07/27/2022 10:52:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 1186 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  98%|█████████▊| 793/810 [03:34<00:06,  2.79it/s]07/27/2022 10:52:11 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:11 - INFO - model.QACGLONG -   Input ids are automatically padded from 884 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  98%|█████████▊| 794/810 [03:34<00:05,  2.91it/s]07/27/2022 10:52:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 1129 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  98%|█████████▊| 795/810 [03:35<00:05,  2.70it/s]07/27/2022 10:52:12 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:12 - INFO - model.QACGLONG -   Input ids are automatically padded from 1129 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  98%|█████████▊| 796/810 [03:35<00:05,  2.55it/s]07/27/2022 10:52:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 359 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  98%|█████████▊| 797/810 [03:35<00:04,  3.05it/s]07/27/2022 10:52:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 896 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▊| 798/810 [03:36<00:03,  3.13it/s]07/27/2022 10:52:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▊| 799/810 [03:36<00:03,  3.19it/s]07/27/2022 10:52:13 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:13 - INFO - model.QACGLONG -   Input ids are automatically padded from 536 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▉| 800/810 [03:36<00:03,  3.24it/s]07/27/2022 10:52:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 1112 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▉| 801/810 [03:37<00:03,  2.89it/s]07/27/2022 10:52:14 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:14 - INFO - model.QACGLONG -   Input ids are automatically padded from 1212 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▉| 802/810 [03:37<00:03,  2.62it/s]07/27/2022 10:52:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 1212 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▉| 803/810 [03:37<00:02,  2.50it/s]07/27/2022 10:52:15 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:15 - INFO - model.QACGLONG -   Input ids are automatically padded from 1379 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▉| 804/810 [03:38<00:02,  2.42it/s]07/27/2022 10:52:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 1379 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration:  99%|█████████▉| 805/810 [03:38<00:02,  2.37it/s]07/27/2022 10:52:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 273 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Iteration: 100%|█████████▉| 806/810 [03:39<00:01,  2.86it/s]07/27/2022 10:52:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 573 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration: 100%|█████████▉| 807/810 [03:39<00:00,  3.01it/s]07/27/2022 10:52:16 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:16 - INFO - model.QACGLONG -   Input ids are automatically padded from 851 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration: 100%|█████████▉| 808/810 [03:39<00:00,  3.10it/s]07/27/2022 10:52:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 689 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Iteration: 100%|█████████▉| 809/810 [03:39<00:00,  3.17it/s]07/27/2022 10:52:17 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n",
      "07/27/2022 10:52:17 - INFO - model.QACGLONG -   Input ids are automatically padded from 1205 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Iteration: 100%|██████████| 810/810 [03:40<00:00,  3.68it/s]\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "pbar = tqdm(test_dataloader, desc=\"Iteration\")\n",
    "y_true, y_pred, score = [], [], []\n",
    "# we don't need gradient in this case.\n",
    "with torch.no_grad():\n",
    "        for _, batch in enumerate(pbar):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            # truncate to save space and computing resource\n",
    "            input_ids, input_mask, segment_ids, label_ids, seq_lens, \\\n",
    "                context_ids = batch\n",
    "            max_seq_lens = max(seq_lens)[0]\n",
    "            input_ids = input_ids[:,:max_seq_lens]\n",
    "            input_mask = input_mask[:,:max_seq_lens]\n",
    "            segment_ids = segment_ids[:,:max_seq_lens]\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "            seq_lens = seq_lens.to(device)\n",
    "            context_ids = context_ids.to(device)\n",
    "\n",
    "            # intentially with gradient\n",
    "            tmp_test_loss, logits, _, _, _, _ = \\\n",
    "                model1(input_ids, segment_ids, input_mask, seq_lens,\n",
    "                        device=device, labels=label_ids,\n",
    "                        context_ids=context_ids)\n",
    "\n",
    "            logits = F.softmax(logits, dim=-1)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            outputs = np.argmax(logits, axis=1)\n",
    "            tmp_test_accuracy=np.sum(outputs == label_ids)\n",
    "\n",
    "            y_true.append(label_ids)\n",
    "            y_pred.append(outputs)\n",
    "            score.append(logits)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "\n",
    "        test_loss = test_loss / nb_test_steps\n",
    "        test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "# we follow previous works in calculating the metrics\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "score = np.concatenate(score, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ddb3cf22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>total</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sale</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>computer</td>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>religion</td>\n",
       "      <td>61</td>\n",
       "      <td>0.147541</td>\n",
       "      <td>0.271978</td>\n",
       "      <td>0.081169</td>\n",
       "      <td>0.119118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>science</td>\n",
       "      <td>79</td>\n",
       "      <td>0.126582</td>\n",
       "      <td>0.185714</td>\n",
       "      <td>0.060390</td>\n",
       "      <td>0.090744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recreation</td>\n",
       "      <td>203</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.286033</td>\n",
       "      <td>0.237304</td>\n",
       "      <td>0.259369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>general</td>\n",
       "      <td>207</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>0.264618</td>\n",
       "      <td>0.168626</td>\n",
       "      <td>0.205929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politics</td>\n",
       "      <td>589</td>\n",
       "      <td>0.443124</td>\n",
       "      <td>0.309148</td>\n",
       "      <td>0.274492</td>\n",
       "      <td>0.277761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        group  total  accuracy  precision    recall        f1\n",
       "5        sale      2  0.000000   0.000000  0.000000  0.000000\n",
       "2    computer     16  0.000000   0.000000  0.000000  0.000000\n",
       "3    religion     61  0.147541   0.271978  0.081169  0.119118\n",
       "4     science     79  0.126582   0.185714  0.060390  0.090744\n",
       "1  recreation    203  0.517241   0.286033  0.237304  0.259369\n",
       "6     general    207  0.362319   0.264618  0.168626  0.205929\n",
       "0    politics    589  0.443124   0.309148  0.274492  0.277761"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "aspect7_true = [[], [], [], [], [], [], []]\n",
    "aspect7_pred = [[], [], [], [], [], [], []]\n",
    "total, accuracy, precision, recall, f1 = [], [], [], [], []\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i] != 0:\n",
    "        aspect7_true[i%7].append(y_true[i])\n",
    "        aspect7_pred[i%7].append(y_pred[i])\n",
    "\n",
    "for j in range(7):\n",
    "    total.append(len(aspect7_true[j]))\n",
    "    accuracy.append(accuracy_score(aspect7_true[j], aspect7_pred[j]))\n",
    "    precision.append(precision_score(aspect7_true[j], aspect7_pred[j], average='macro'))\n",
    "    recall.append(recall_score(aspect7_true[j], aspect7_pred[j], average='macro'))\n",
    "    f1.append(f1_score(aspect7_true[j], aspect7_pred[j], average='macro'))\n",
    "\n",
    "df = pd.DataFrame({'group': ['politics', 'recreation', 'computer', 'religion', 'science', 'sale', 'general'], 'total': total, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1})\n",
    "df.sort_values(by=['total'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db4cdc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax  = plt.subplots(1,1, figsize = (10,5))\n",
    "x = np.arange(7)\n",
    "\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x-3*width/2, df['accuracy'], width, label='Accuracy')\n",
    "ax.bar(x-width/2, df['precision'], width, label='Precision')\n",
    "ax.bar(x+width/2, df['recall'], width, label='Recall')\n",
    "ax.bar(x+3*width/2, df['f1'], width, label='Macro-F1')\n",
    "ax.set_title('Aspect-level Metrics')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df['group'].astype(str).values)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db78d81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
