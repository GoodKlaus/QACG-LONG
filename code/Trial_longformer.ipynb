{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19191af8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/08/gnkh3z114gd2wd6cgc3mm8sr0000gn/T/ipykernel_14038/3972608322.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from random import shuffle\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "\n",
    "from util.args_parser import parser\n",
    "from model.QACGBERT import BertConfig\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler, WeightedRandomSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from util.optimization import BERTAdam\n",
    "from util.processor import (Sentihood_NLI_M_Processor,\n",
    "                            Semeval_NLI_M_Processor)\n",
    "\n",
    "from util.tokenization import *\n",
    "\n",
    "from util.evaluation import *\n",
    "\n",
    "from model.QACGLONG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4920d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT_INDEX</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TARGET_ENTITY</th>\n",
       "      <th>DOCUMENT</th>\n",
       "      <th>MASKED_DOCUMENT</th>\n",
       "      <th>TRUE_SENTIMENT</th>\n",
       "      <th>Paragraph0</th>\n",
       "      <th>Paragraph1</th>\n",
       "      <th>Paragraph2</th>\n",
       "      <th>Paragraph3</th>\n",
       "      <th>...</th>\n",
       "      <th>Paragraph10</th>\n",
       "      <th>Paragraph11</th>\n",
       "      <th>Paragraph12</th>\n",
       "      <th>Paragraph13</th>\n",
       "      <th>Paragraph14</th>\n",
       "      <th>Paragraph15</th>\n",
       "      <th>prob_max</th>\n",
       "      <th>topic</th>\n",
       "      <th>group</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3360</td>\n",
       "      <td>AS VALUES DROP MORE HOMEOWNERS WALKING AWAY FR...</td>\n",
       "      <td>Benjamin Koellmann</td>\n",
       "      <td>In 2006  Benjamin Koellmann bought a condomini...</td>\n",
       "      <td>In 2006  [TGT] bought a condominium in Miami B...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.261895</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>general</td>\n",
       "      <td>[TGT] - general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3360</td>\n",
       "      <td>AS VALUES DROP MORE HOMEOWNERS WALKING AWAY FR...</td>\n",
       "      <td>Benjamin Koellmann</td>\n",
       "      <td>In 2006  Benjamin Koellmann bought a condomini...</td>\n",
       "      <td>In 2006  [TGT] bought a condominium in Miami B...</td>\n",
       "      <td>None</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.261895</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>politics</td>\n",
       "      <td>[TGT] - politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3360</td>\n",
       "      <td>AS VALUES DROP MORE HOMEOWNERS WALKING AWAY FR...</td>\n",
       "      <td>Benjamin Koellmann</td>\n",
       "      <td>In 2006  Benjamin Koellmann bought a condomini...</td>\n",
       "      <td>In 2006  [TGT] bought a condominium in Miami B...</td>\n",
       "      <td>None</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.261895</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>reaction</td>\n",
       "      <td>[TGT] - reaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3360</td>\n",
       "      <td>AS VALUES DROP MORE HOMEOWNERS WALKING AWAY FR...</td>\n",
       "      <td>Benjamin Koellmann</td>\n",
       "      <td>In 2006  Benjamin Koellmann bought a condomini...</td>\n",
       "      <td>In 2006  [TGT] bought a condominium in Miami B...</td>\n",
       "      <td>None</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.261895</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>computer</td>\n",
       "      <td>[TGT] - computer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3360</td>\n",
       "      <td>AS VALUES DROP MORE HOMEOWNERS WALKING AWAY FR...</td>\n",
       "      <td>Benjamin Koellmann</td>\n",
       "      <td>In 2006  Benjamin Koellmann bought a condomini...</td>\n",
       "      <td>In 2006  [TGT] bought a condominium in Miami B...</td>\n",
       "      <td>None</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.261895</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>religion</td>\n",
       "      <td>[TGT] - religion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOCUMENT_INDEX                                              TITLE  \\\n",
       "0            3360  AS VALUES DROP MORE HOMEOWNERS WALKING AWAY FR...   \n",
       "1            3360  AS VALUES DROP MORE HOMEOWNERS WALKING AWAY FR...   \n",
       "2            3360  AS VALUES DROP MORE HOMEOWNERS WALKING AWAY FR...   \n",
       "3            3360  AS VALUES DROP MORE HOMEOWNERS WALKING AWAY FR...   \n",
       "4            3360  AS VALUES DROP MORE HOMEOWNERS WALKING AWAY FR...   \n",
       "\n",
       "        TARGET_ENTITY                                           DOCUMENT  \\\n",
       "0  Benjamin Koellmann  In 2006  Benjamin Koellmann bought a condomini...   \n",
       "1  Benjamin Koellmann  In 2006  Benjamin Koellmann bought a condomini...   \n",
       "2  Benjamin Koellmann  In 2006  Benjamin Koellmann bought a condomini...   \n",
       "3  Benjamin Koellmann  In 2006  Benjamin Koellmann bought a condomini...   \n",
       "4  Benjamin Koellmann  In 2006  Benjamin Koellmann bought a condomini...   \n",
       "\n",
       "                                     MASKED_DOCUMENT TRUE_SENTIMENT  \\\n",
       "0  In 2006  [TGT] bought a condominium in Miami B...        Neutral   \n",
       "1  In 2006  [TGT] bought a condominium in Miami B...           None   \n",
       "2  In 2006  [TGT] bought a condominium in Miami B...           None   \n",
       "3  In 2006  [TGT] bought a condominium in Miami B...           None   \n",
       "4  In 2006  [TGT] bought a condominium in Miami B...           None   \n",
       "\n",
       "  Paragraph0 Paragraph1 Paragraph2 Paragraph3  ... Paragraph10 Paragraph11  \\\n",
       "0    Neutral    Neutral    Neutral    Neutral  ...         NaN         NaN   \n",
       "1    Neutral    Neutral    Neutral    Neutral  ...         NaN         NaN   \n",
       "2    Neutral    Neutral    Neutral    Neutral  ...         NaN         NaN   \n",
       "3    Neutral    Neutral    Neutral    Neutral  ...         NaN         NaN   \n",
       "4    Neutral    Neutral    Neutral    Neutral  ...         NaN         NaN   \n",
       "\n",
       "  Paragraph12 Paragraph13 Paragraph14 Paragraph15  prob_max      topic  \\\n",
       "0         NaN         NaN         NaN         NaN  0.261895  rec.autos   \n",
       "1         NaN         NaN         NaN         NaN  0.261895  rec.autos   \n",
       "2         NaN         NaN         NaN         NaN  0.261895  rec.autos   \n",
       "3         NaN         NaN         NaN         NaN  0.261895  rec.autos   \n",
       "4         NaN         NaN         NaN         NaN  0.261895  rec.autos   \n",
       "\n",
       "      group           context  \n",
       "0   general   [TGT] - general  \n",
       "1  politics  [TGT] - politics  \n",
       "2  reaction  [TGT] - reaction  \n",
       "3  computer  [TGT] - computer  \n",
       "4  religion  [TGT] - religion  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/persent/dev_longformer_topic.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92ba6bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0038</td>\n",
       "      <td>3.003272</td>\n",
       "      <td>0.050319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>3.0036</td>\n",
       "      <td>3.002958</td>\n",
       "      <td>0.050186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>3.0098</td>\n",
       "      <td>3.001984</td>\n",
       "      <td>0.051248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>3.0084</td>\n",
       "      <td>3.000390</td>\n",
       "      <td>0.051912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>3.0111</td>\n",
       "      <td>2.998500</td>\n",
       "      <td>0.052841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Step  Training Loss  Validation Loss  Accuracy\n",
       "0     4         3.0038         3.003272  0.050319\n",
       "1     8         3.0036         3.002958  0.050186\n",
       "2    12         3.0098         3.001984  0.051248\n",
       "3    16         3.0084         3.000390  0.051912\n",
       "4    20         3.0111         2.998500  0.052841"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"../datasets/longformer_20news.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c8c920c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Accuracy'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bec2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = {\n",
    "    \"sentihood_NLI_M\":Sentihood_NLI_M_Processor,\n",
    "    \"semeval_NLI_M\":Semeval_NLI_M_Processor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba65a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10833589",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "data_test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7526885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n",
      "Ignored unknown kwarg option direction\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length=2048)\n",
    "\n",
    "train_encodings = tokenizer(data_train.data, truncation=True, padding=True, max_length=2048)\n",
    "valid_encodings = tokenizer(data_test.data, truncation=True, padding=True, max_length=2048)\n",
    "\n",
    "class NewsGroupsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = NewsGroupsDataset(train_encodings, data_train.target)\n",
    "valid_dataset = NewsGroupsDataset(valid_encodings, data_test.target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40c94bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = train_dataset[:]['input_ids']\n",
    "attn_masks = train_dataset[:]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a8e908b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels=20, gradient_checkpointing=False, attention_window = 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attn_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d4eeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerForSequenceClassification(\n",
       "  (longformer): LongformerModel(\n",
       "    (embeddings): LongformerEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): LongformerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): LongformerClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58ae5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ecc4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = '../results/sentihood/NewsGroups/',\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 8,    \n",
    "    per_device_eval_batch_size= 1,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 4,\n",
    "    logging_dir='../results/sentihood/NewsGroups/logs',\n",
    "    dataloader_num_workers = 0,\n",
    "    run_name = 'longformer-classification-updated-rtx3090_paper_replication_2_warm'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6acf932",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=valid_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5265b636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haoyuzhu/opt/anaconda3/envs/torch1/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11314\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 7070\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/08/gnkh3z114gd2wd6cgc3mm8sr0000gn/T/ipykernel_2705/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/torch1/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m                 if (\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch1/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1956\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch1/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch1/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e966898",
   "metadata": {},
   "source": [
    "### Set up system (device and GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12eb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def system_setups(args):\n",
    "    # system related setups\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "    logger.info(\"device %s n_gpu %d distributed training %r\", device, n_gpu, bool(args.local_rank != -1))\n",
    "\n",
    "    if args.accumulate_gradients < 1:\n",
    "        raise ValueError(\"Invalid accumulate_gradients parameter: {}, should be >= 1\".format(\n",
    "                            args.accumulate_gradients))\n",
    "\n",
    "    args.train_batch_size = int(args.train_batch_size / args.accumulate_gradients)\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    if args.bert_config_file is not None:\n",
    "        bert_config = BertConfig.from_json_file(args.bert_config_file)\n",
    "        if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "            raise ValueError(\n",
    "                \"Cannot use sequence length {} because the BERT model was only trained up to sequence length {}\".format(\n",
    "                args.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "    # not preloading\n",
    "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n",
    "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    output_log_file = os.path.join(args.output_dir, \"log.txt\")\n",
    "    print(\"output_log_file=\",output_log_file)\n",
    "\n",
    "    if args.task_name == \"sentihood_NLI_M\":\n",
    "        with open(output_log_file, \"w\") as writer:\n",
    "            writer.write(\"epoch\\tglobal_step\\tloss\\tt_loss\\tt_acc\\tstrict_acc\\tf1\\tauc\\ts_acc\\ts_auc\\n\")\n",
    "    else:\n",
    "        with open(output_log_file, \"w\") as writer:\n",
    "            writer.write(\"epoch\\tglobal_step\\tloss\\tt_loss\\tt_acc\\taspect_P\\taspect_R\\taspect_F\\ts_acc_4\\ts_acc_3\\ts_acc_2\\n\")\n",
    "\n",
    "    return device, n_gpu, output_log_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d647c55",
   "metadata": {},
   "source": [
    "### Pass arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5552c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_init = ['--task_name', 'sentihood_NLI_M', \n",
    "             '--data_dir', '../datasets/sentihood/',\n",
    "             '--output_dir', '../results/sentihood/QACGLONG-reproduce1/',\n",
    "             '--model_type', 'QACGLONG',\n",
    "             '--do_lower_case',\n",
    "             '--max_seq_length', '2048',\n",
    "             '--train_batch_size', '64',\n",
    "             '--eval_batch_size', '128',\n",
    "             '--learning_rate', '2e-5',\n",
    "             '--num_train_epochs', '1',\n",
    "             '--vocab_file', 'BERT-Google/vocab.txt',\n",
    "             '--bert_config_file', 'Longformer/config.json',\n",
    "             '--init_checkpoint', 'Longformer/pytorch_model.bin',\n",
    "             '--seed', '123',\n",
    "             '--evaluate_interval', '200']\n",
    "\n",
    "args = parser.parse_args(args_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d731a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/23/2022 23:53:16 - INFO - __main__ -   device cpu n_gpu 0 distributed training False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_log_file= ../results/sentihood/QACGLONG-reproduce1/log.txt\n"
     ]
    }
   ],
   "source": [
    "device, n_gpu, output_log_file= system_setups(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ca882e",
   "metadata": {},
   "source": [
    "### Get model, optimizer and tokenizer (but \"model\" more useful here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45521a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelOptimizerTokenizer(model_type, vocab_file,\n",
    "                               bert_config_file=None, init_checkpoint=None,\n",
    "                               label_list=None,\n",
    "                               do_lower_case=True,\n",
    "                               num_train_steps=None,\n",
    "                               learning_rate=None,\n",
    "                               base_learning_rate=None,\n",
    "                               warmup_proportion=None,\n",
    "                               init_lrp=False):\n",
    "\n",
    "    # this is the model we develop\n",
    "    tokenizer = FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case, pretrain=False)\n",
    "    if bert_config_file is not None:\n",
    "        bert_config = BertConfig.from_json_file(bert_config_file)\n",
    "    else:\n",
    "        # default?\n",
    "        print(\"else implemented\")\n",
    "        bert_config = BertConfig(\n",
    "            hidden_size=768,\n",
    "            num_hidden_layers=12,\n",
    "            num_attention_heads=12,\n",
    "            intermediate_size=3072,\n",
    "            hidden_act=\"gelu\",\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "            max_position_embeddings=512,\n",
    "            type_vocab_size=2,\n",
    "            initializer_range=0.02\n",
    "        )\n",
    "    logger.info(\"*** Model Config ***\")\n",
    "    logger.info(bert_config.to_json_string())\n",
    "    # overwrite the vocab size to be exact. this also save space incase\n",
    "    # vocab size is shrinked.\n",
    "    bert_config.vocab_size = len(tokenizer.vocab)\n",
    "    # model and optimizer\n",
    "    logger.info(\"model = QACGLONG\")\n",
    "    model = QACGBertForSequenceClassification1(\n",
    "                    bert_config, len(label_list),\n",
    "                    init_weight=True,\n",
    "                    init_lrp=init_lrp)\n",
    "\n",
    "    if init_checkpoint is not None:\n",
    "        print(init_checkpoint)\n",
    "        logger.info(\"retraining with saved model.\")\n",
    "        # only load fields that are avaliable\n",
    "        model.bert.load_state_dict(torch.load(init_checkpoint, map_location='cpu'), strict=False)\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    block_list = []\n",
    "    \n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() \n",
    "            if not any(nd in n for nd in no_decay) and not any(bl in n for bl in block_list)], 'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in model.named_parameters() \n",
    "            if any(nd in n for nd in no_decay) and not any(bl in n for bl in block_list)], 'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "\n",
    "    optimizer = BERTAdam(optimizer_parameters,\n",
    "                        lr=learning_rate,\n",
    "                        warmup=warmup_proportion,\n",
    "                        t_total=num_train_steps)\n",
    "    return model, optimizer, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bbde94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/23/2022 23:53:18 - INFO - __main__ -   *** Model Config ***\n",
      "05/23/2022 23:53:18 - INFO - __main__ -   {\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"full_pooler\": false,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"sep_token_id\": 2,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "05/23/2022 23:53:18 - INFO - __main__ -   model = QACGLONG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_weight = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/23/2022 23:53:20 - INFO - __main__ -   retraining with saved model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longformer/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "processor = processors[args.task_name]()\n",
    "label_list = processor.get_labels()\n",
    "\n",
    "# training setup\n",
    "train_examples = None\n",
    "num_train_steps = None\n",
    "train_examples = processor.get_train_examples(args.data_dir)\n",
    "num_train_steps = int(len(train_examples) / args.train_batch_size * args.num_train_epochs)\n",
    "\n",
    "# model and optimizer\n",
    "model1, optimizer, tokenizer = getModelOptimizerTokenizer(model_type=args.model_type,\n",
    "                                   vocab_file=args.vocab_file,\n",
    "                                   bert_config_file=args.bert_config_file,\n",
    "                                   init_checkpoint=args.init_checkpoint,\n",
    "                                   label_list=label_list,\n",
    "                                   do_lower_case=True,\n",
    "                                   num_train_steps=num_train_steps,\n",
    "                                   learning_rate=args.learning_rate,\n",
    "                                   base_learning_rate=args.base_learning_rate,\n",
    "                                   warmup_proportion=args.warmup_proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35ee8a",
   "metadata": {},
   "source": [
    "### Obtain training features (extended length to 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35bac0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_id_map_sentihood = {'location - 1 - general':0,\n",
    "                            'location - 1 - price':1,\n",
    "                            'location - 1 - safety':2,\n",
    "                            'location - 1 - transit location':3,\n",
    "                            'location - 2 - general':4,\n",
    "                            'location - 2 - price':5,\n",
    "                            'location - 2 - safety':6,\n",
    "                            'location - 2 - transit location':7}\n",
    "\n",
    "context_id_map_semeval= {'price':0,\n",
    "                         'anecdotes':1,\n",
    "                         'food':2,\n",
    "                         'ambience':3,\n",
    "                         'service':4}\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, seq_len,\n",
    "                 context_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.seq_len = seq_len\n",
    "        # extra fields to hold context\n",
    "        self.context_ids = context_ids            \n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
    "                                 tokenizer, max_context_length,\n",
    "                                 # if this is true, the context will not be\n",
    "                                 # appened into the inputs\n",
    "                                 context_standalone, args):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(tqdm(examples)):\n",
    "        temp = example.text_a * 1000\n",
    "        tokens_a = tokenizer.tokenize(temp)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "        # tokens of context\n",
    "        tokens_context = None\n",
    "        if example.text_b:\n",
    "            tokens_context = tokens_b\n",
    "\n",
    "        if tokens_b and not context_standalone:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        if tokens_b and not context_standalone:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        context_ids = []\n",
    "        if tokens_context:\n",
    "            # let us encode context into single int\n",
    "            if args.task_name == \"sentihood_NLI_M\":\n",
    "                context_ids = [context_id_map_sentihood[example.text_b]]\n",
    "            else:\n",
    "                context_ids = [context_id_map_semeval[example.text_b]]\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        seq_len = len(input_ids)\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        while len(context_ids) < max_context_length:\n",
    "            context_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(context_ids) == max_context_length\n",
    "\n",
    "        label_id = label_map[example.label]\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(\n",
    "                        input_ids=input_ids,\n",
    "                        input_mask=input_mask,\n",
    "                        segment_ids=segment_ids,\n",
    "                        label_id=label_id,\n",
    "                        seq_len=seq_len,\n",
    "                        # newly added context part\n",
    "                        context_ids=context_ids))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7199e11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:18<00:00,  5.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "train_features = convert_examples_to_features(\n",
    "        train_examples[:100], label_list, args.max_seq_length,\n",
    "        tokenizer, args.max_context_length,\n",
    "        args.context_standalone, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2e3e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "seq_len = torch.tensor([[f.seq_len] for f in train_features], dtype=torch.long)\n",
    "context_ids = torch.tensor([f.context_ids for f in train_features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "612095bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_lens = max(seq_len)[0]\n",
    "input_ids = input_ids[:10,:max_seq_lens]\n",
    "input_mask = input_mask[:10,:max_seq_lens]\n",
    "segment_ids = segment_ids[:10,:max_seq_lens]\n",
    "label_ids = label_ids[:10]\n",
    "seq_len = seq_len[:10, :]\n",
    "context_ids = context_ids[:10,:]\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "input_mask = input_mask.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "label_ids = label_ids.to(device)\n",
    "seq_len = seq_len.to(device)\n",
    "context_ids = context_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ed97d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QACGBertForSequenceClassification1(\n",
       "  (bert): ContextBertModel1(\n",
       "    (embeddings): BERTEmbeddings1(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BERTLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ContextBERTEncoder1(\n",
       "      (context_layer): ModuleList(\n",
       "        (0): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (1): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (3): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (4): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (5): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (6): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (7): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (8): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (9): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (10): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (11): Linear(in_features=1536, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer): ModuleList(\n",
       "        (0): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler1): ContextBERTPooler1(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (context_embeddings): Embedding(8, 768)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f685d0",
   "metadata": {},
   "source": [
    "### Simply run model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886f6018",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model1(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask, seq_lens=seq_len, device=device, labels=label_ids, context_ids=context_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5828dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510b487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b1d1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../datasets/train.csv')\n",
    "train_data = train_data[['DOCUMENT', 'TARGET_ENTITY', 'TRUE_SENTIMENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "782bda72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT</th>\n",
       "      <th>TARGET_ENTITY</th>\n",
       "      <th>TRUE_SENTIMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany's Landesbank Baden Wuertemberg won EU ...</td>\n",
       "      <td>Landesbank Baden Wuertemberg</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Philippine National Police (PNP) identifie...</td>\n",
       "      <td>Rolando Mendoza</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sirleaf  70  acknowledged before the commissio...</td>\n",
       "      <td>Charles Taylor</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sawyer logged off and asked her sister  Mari  ...</td>\n",
       "      <td>Sawyers</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Candi Holyfield said in the protective order t...</td>\n",
       "      <td>Candi Holyfield</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            DOCUMENT  \\\n",
       "0  Germany's Landesbank Baden Wuertemberg won EU ...   \n",
       "1  The Philippine National Police (PNP) identifie...   \n",
       "2  Sirleaf  70  acknowledged before the commissio...   \n",
       "3  Sawyer logged off and asked her sister  Mari  ...   \n",
       "4  Candi Holyfield said in the protective order t...   \n",
       "\n",
       "                  TARGET_ENTITY TRUE_SENTIMENT  \n",
       "0  Landesbank Baden Wuertemberg       Negative  \n",
       "1               Rolando Mendoza        Neutral  \n",
       "2                Charles Taylor       Negative  \n",
       "3                       Sawyers        Neutral  \n",
       "4               Candi Holyfield        Neutral  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5015a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = {\n",
    "    \"sentihood_NLI_M\":Sentihood_NLI_M_Processor,\n",
    "    \"semeval_NLI_M\":Semeval_NLI_M_Processor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d3bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def system_setups(args):\n",
    "    # system related setups\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "    logger.info(\"device %s n_gpu %d distributed training %r\", device, n_gpu, bool(args.local_rank != -1))\n",
    "\n",
    "    if args.accumulate_gradients < 1:\n",
    "        raise ValueError(\"Invalid accumulate_gradients parameter: {}, should be >= 1\".format(\n",
    "                            args.accumulate_gradients))\n",
    "\n",
    "    args.train_batch_size = int(args.train_batch_size / args.accumulate_gradients)\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    if args.bert_config_file is not None:\n",
    "        bert_config = BertConfig.from_json_file(args.bert_config_file)\n",
    "        if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "            raise ValueError(\n",
    "                \"Cannot use sequence length {} because the BERT model was only trained up to sequence length {}\".format(\n",
    "                args.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "    # not preloading\n",
    "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n",
    "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    output_log_file = os.path.join(args.output_dir, \"log.txt\")\n",
    "    print(\"output_log_file=\",output_log_file)\n",
    "\n",
    "    if args.task_name == \"sentihood_NLI_M\":\n",
    "        with open(output_log_file, \"w\") as writer:\n",
    "            writer.write(\"epoch\\tglobal_step\\tloss\\tt_loss\\tt_acc\\tstrict_acc\\tf1\\tauc\\ts_acc\\ts_auc\\n\")\n",
    "    else:\n",
    "        with open(output_log_file, \"w\") as writer:\n",
    "            writer.write(\"epoch\\tglobal_step\\tloss\\tt_loss\\tt_acc\\taspect_P\\taspect_R\\taspect_F\\ts_acc_4\\ts_acc_3\\ts_acc_2\\n\")\n",
    "\n",
    "    return device, n_gpu, output_log_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cabcd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_init100 = ['--task_name', 'sentihood_NLI_M', \n",
    "             '--data_dir', '../datasets/sentihood/',\n",
    "             '--output_dir', '../results/sentihood/QACGLONG-reproduce101/',\n",
    "             '--model_type', 'QACGLONG',\n",
    "             '--do_lower_case',\n",
    "             '--max_seq_length', '2048',\n",
    "             '--train_batch_size', '64',\n",
    "             '--eval_batch_size', '16',\n",
    "             '--learning_rate', '2e-5',\n",
    "             '--num_train_epochs', '1',\n",
    "             '--vocab_file', 'BERT-Google/vocab.txt',\n",
    "             '--bert_config_file', 'Longformer/config.json',\n",
    "             '--init_checkpoint', 'Longformer/pytorch_model.bin',\n",
    "             '--seed', '123',\n",
    "             '--evaluate_interval', '15',\n",
    "             '--accumulate_gradients', '8',\n",
    "             '--gradient_accumulation_steps', '8']\n",
    "\n",
    "args100 = parser.parse_args(args_init100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abbb1b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/31/2022 14:25:54 - INFO - __main__ -   device cpu n_gpu 0 distributed training False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_log_file= ../results/sentihood/QACGLONG-reproduce101/log.txt\n"
     ]
    }
   ],
   "source": [
    "device, n_gpu, output_log_file= system_setups(args100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "521d813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fbcbc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "        \n",
    "label_list = list(train_data['TRUE_SENTIMENT'].unique())\n",
    "\n",
    "train_examples = []\n",
    "for (i, line) in enumerate(train_data.values):\n",
    "    text_a = convert_to_unicode(str(line[0]))\n",
    "    text_b = convert_to_unicode(str(line[1]))\n",
    "    label = convert_to_unicode(str(line[2]))\n",
    "    train_examples.append(InputExample(text_a=text_a, text_b=text_b, label=label))\n",
    "\n",
    "num_train_steps = int(len(train_examples) / 64 * 1)\n",
    "tokenizer = FullTokenizer(vocab_file=args100.vocab_file, do_lower_case=True, pretrain=False)\n",
    "bert_config = BertConfig.from_json_file(args100.bert_config_file)\n",
    "bert_config.vocab_size = len(tokenizer.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a49bd738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3355/3355 [00:17<00:00, 192.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, trange\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, seq_len,\n",
    "                 context_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.seq_len = seq_len\n",
    "        # extra fields to hold context\n",
    "        self.context_ids = context_ids\n",
    "\n",
    "max_seq_length = args100.max_seq_length\n",
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "    label_map[label] = i\n",
    "\n",
    "context_id_map = {}\n",
    "for (i, entity) in enumerate(list(train_data['TARGET_ENTITY'].unique())):\n",
    "    context_id_map[entity] = i\n",
    "\n",
    "features = []\n",
    "for (ex_index, example) in enumerate(tqdm(train_examples)):\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = tokenizer.tokenize(example.text_b)\n",
    "    tokens_context = tokenizer.tokenize(example.text_b)\n",
    "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    \n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "    \n",
    "    for token in tokens_b:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(1)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    context_ids = [context_id_map[example.text_b]]\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    seq_len = len(input_ids)\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "    \n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(context_ids) == 1\n",
    "\n",
    "    label_id = label_map[example.label]\n",
    "    features.append(InputFeatures(\n",
    "                        input_ids=input_ids,\n",
    "                        input_mask=input_mask,\n",
    "                        segment_ids=segment_ids,\n",
    "                        label_id=label_id,\n",
    "                        seq_len=seq_len,\n",
    "                        context_ids=context_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1b32d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = features\n",
    "input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "seq_len = torch.tensor([[f.seq_len] for f in train_features], dtype=torch.long)\n",
    "context_ids = torch.tensor([f.context_ids for f in train_features], dtype=torch.long)\n",
    "\n",
    "max_seq_lens = max(seq_len)[0]\n",
    "input_ids = input_ids[:10,:max_seq_lens]\n",
    "input_mask = input_mask[:10,:max_seq_lens]\n",
    "segment_ids = segment_ids[:10,:max_seq_lens]\n",
    "label_ids = label_ids[:10]\n",
    "seq_len = seq_len[:10, :]\n",
    "context_ids = context_ids[:10,:]\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "input_mask = input_mask.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "label_ids = label_ids.to(device)\n",
    "seq_len = seq_len.to(device)\n",
    "context_ids = context_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0221998a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_weight = True\n"
     ]
    }
   ],
   "source": [
    "model = QACGBertForSequenceClassification1(\n",
    "                    bert_config, len(label_list),\n",
    "                    init_weight=True,\n",
    "                    init_lrp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56b416bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QACGBertForSequenceClassification1(\n",
       "  (bert): ContextBertModel1(\n",
       "    (embeddings): BERTEmbeddings1(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BERTLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ContextBERTEncoder1(\n",
       "      (context_layer): ModuleList(\n",
       "        (0): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (1): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (2): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (3): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (4): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (5): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (6): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (7): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (8): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (9): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (10): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (11): Linear(in_features=1536, out_features=768, bias=True)\n",
       "      )\n",
       "      (layer): ModuleList(\n",
       "        (0): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ContextBERTLayer1(\n",
       "          (attention): ContextBERTAttention1(\n",
       "            (self): ContextBERTSelfAttention1(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (context_for_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (lambda_q_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_q_query_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_context_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_k_key_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "              (lambda_act): Sigmoid()\n",
       "              (quasi_act): Sigmoid()\n",
       "            )\n",
       "            (output): BERTSelfOutput1(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BERTLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BERTIntermediate1(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BERTOutput1(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler1): ContextBERTPooler1(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (context_embeddings): Embedding(2375, 768)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89f82d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/31/2022 14:26:36 - INFO - model.QACGLONG -   Initializing global attention on CLS token...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/08/gnkh3z114gd2wd6cgc3mm8sr0000gn/T/ipykernel_9443/147632788.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/torch1/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/internal project/Quasi-Attention-ABSA-main/code/model/QACGLONG.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, seq_lens, device, labels, context_ids, global_attention_mask, head_mask, position_ids)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_new_attention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mce\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             self.bert(input_ids, token_type_ids, attention_mask, device, context_ids, \n\u001b[0m\u001b[1;32m   1387\u001b[0m                         \u001b[0mglobal_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m                         \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch1/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/internal project/Quasi-Attention-ABSA-main/code/model/QACGLONG.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, device, context_ids, global_attention_mask, head_mask, position_ids)\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_new_attention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mce\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m             self.encoder(embedding_output, extended_attention_mask, device,\n\u001b[0m\u001b[1;32m   1312\u001b[0m                         \u001b[0mcontext_embedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                         \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch1/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/internal project/Quasi-Attention-ABSA-main/code/model/QACGLONG.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, device, context_embeddings, head_mask, padding_len)\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;31m# BERT encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_attention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mce\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m                     layer_module(hidden_states, attention_mask, device, \n\u001b[0m\u001b[1;32m   1150\u001b[0m                                 \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                                 \u001b[0mis_index_masked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_masked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch1/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/internal project/Quasi-Attention-ABSA-main/code/model/QACGLONG.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, device, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, context_embedded)\u001b[0m\n\u001b[1;32m   1089\u001b[0m                 context_embedded=None):\n\u001b[1;32m   1090\u001b[0m         \u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_attention_probs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             self.attention(hidden_states, attention_mask, device, \n\u001b[0m\u001b[1;32m   1092\u001b[0m                            \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m                            \u001b[0mis_index_masked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_masked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/torch1/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/internal project/Quasi-Attention-ABSA-main/code/model/QACGLONG.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, attention_mask, device, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, context_embedded)\u001b[0m\n\u001b[1;32m    995\u001b[0m                 context_embedded=None):\n\u001b[1;32m    996\u001b[0m         \u001b[0mself_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_attention_probs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m             self.self.forward(input_tensor, attention_mask, device,\n\u001b[0m\u001b[1;32m    998\u001b[0m                             \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                            \u001b[0mis_index_masked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_masked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/internal project/Quasi-Attention-ABSA-main/code/model/QACGLONG.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, device, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, context_embedded)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_global_attn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             global_attn_output, global_attn_probs = self._compute_global_attn_output_from_hidden(\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0mcontext_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/internal project/Quasi-Attention-ABSA-main/code/model/QACGLONG.py\u001b[0m in \u001b[0;36m_compute_global_attn_output_from_hidden\u001b[0;34m(self, hidden_states, context_embedded, max_num_global_attn_indices, layer_head_mask, is_local_index_global_attn_nonzero, is_index_global_attn_nonzero, is_local_index_no_global_attn_nonzero, is_index_masked)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;31m# global attn output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0mglobal_attn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_attn_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_value_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         assert list(global_attn_output.size()) == [\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask, seq_lens=seq_len, device=device, labels=label_ids, context_ids=context_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dcc3bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [5],\n",
       "        [7],\n",
       "        [8]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb3746e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8690, -0.9473,  0.2629,  ...,  0.1287,  0.2890, -1.3049]],\n",
       "\n",
       "        [[ 0.5599, -0.8460,  0.1368,  ...,  0.4884, -0.4717, -0.5029]],\n",
       "\n",
       "        [[-1.8438, -0.4552,  0.4576,  ..., -0.0440, -1.6519,  0.7680]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.9285,  0.2917, -0.7634,  ...,  2.1030,  0.4175,  1.2674]],\n",
       "\n",
       "        [[ 0.0657, -0.4176, -0.3156,  ...,  1.0859, -0.7780,  0.2519]],\n",
       "\n",
       "        [[-1.9695, -0.1400, -0.2129,  ...,  1.5484,  0.7385, -0.0243]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Embedding(2375, 768)(context_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9188fcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2375"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d74c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
