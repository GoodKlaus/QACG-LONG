{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16be7af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "from transformers import LongformerTokenizerFast, LongformerForSequenceClassification, Trainer, TrainingArguments, LongformerConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e691b",
   "metadata": {},
   "source": [
    "# Applying 20 NewsGroups Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ad312c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the test data using the same vectorizer\n"
     ]
    }
   ],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "data_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "labels = data_train.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english', use_idf=True)\n",
    "X_train = vectorizer.fit_transform(data_train.data)\n",
    "\n",
    "target_names = data_train.target_names\n",
    "\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "X_test = vectorizer.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9976bd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongformerForSequenceClassification(\n",
       "  (longformer): LongformerModel(\n",
       "    (embeddings): LongformerEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): LongformerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): LongformerClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NewsGroupsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length=2048)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', num_labels=20, gradient_checkpointing=False, attention_window = 512)\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(data_train.data, truncation=True, padding=True, max_length=2048)\n",
    "valid_encodings = tokenizer(data_test.data, truncation=True, padding=True, max_length=2048)\n",
    "    \n",
    "train_dataset = NewsGroupsDataset(train_encodings, data_train.target)\n",
    "valid_dataset = NewsGroupsDataset(valid_encodings, data_test.target)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eaa7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc,}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = '../results/sentihood/NewsGroups/',\n",
    "    num_train_epochs =10,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 8,    \n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 4,\n",
    "    fp16 = True,\n",
    "    logging_dir='../results/sentihood/NewsGroups/logs',\n",
    "    dataloader_num_workers = 0,\n",
    "    run_name = 'longformer-classification-updated-rtx3090_paper_replication_2_warm'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b06b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n",
      "/home/haoyu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11314\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 880\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "/home/haoyu/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9/880 01:54 < 3:57:17, 0.06 it/s, Epoch 0.09/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.996900</td>\n",
       "      <td>3.001111</td>\n",
       "      <td>0.054567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/471 00:24 < 01:01, 5.48 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7532\n",
      "  Batch size = 4\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "/home/haoyu/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7532\n",
      "  Batch size = 4\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-56ab32a352c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1495\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1625\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2283\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2284\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   2285\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2286\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2458\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2672\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=valid_dataset,          \n",
    "    compute_metrics=compute_metrics,     \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb683a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in model/20newsgroups-longformer/config.json\n",
      "Model weights saved in model/20newsgroups-longformer/pytorch_model.bin\n",
      "tokenizer config file saved in model/20newsgroups-longformer/tokenizer_config.json\n",
      "Special tokens file saved in model/20newsgroups-longformer/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('model/20newsgroups-longformer/tokenizer_config.json',\n",
       " 'model/20newsgroups-longformer/special_tokens_map.json',\n",
       " 'model/20newsgroups-longformer/vocab.json',\n",
       " 'model/20newsgroups-longformer/merges.txt',\n",
       " 'model/20newsgroups-longformer/added_tokens.json',\n",
       " 'model/20newsgroups-longformer/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"model/20newsgroups-longformer\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1343beb1",
   "metadata": {},
   "source": [
    "# Obtain newsgroup predictions for PerSent dataset from fine-tuned Longformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8869c044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOCUMENT_INDEX</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TARGET_ENTITY</th>\n",
       "      <th>DOCUMENT</th>\n",
       "      <th>MASKED_DOCUMENT</th>\n",
       "      <th>TRUE_SENTIMENT</th>\n",
       "      <th>Paragraph0</th>\n",
       "      <th>Paragraph1</th>\n",
       "      <th>Paragraph2</th>\n",
       "      <th>Paragraph3</th>\n",
       "      <th>...</th>\n",
       "      <th>Paragraph6</th>\n",
       "      <th>Paragraph7</th>\n",
       "      <th>Paragraph8</th>\n",
       "      <th>Paragraph9</th>\n",
       "      <th>Paragraph10</th>\n",
       "      <th>Paragraph11</th>\n",
       "      <th>Paragraph12</th>\n",
       "      <th>Paragraph13</th>\n",
       "      <th>Paragraph14</th>\n",
       "      <th>Paragraph15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3360</td>\n",
       "      <td>AS VALUES DROP MORE HOMEOWNERS WALKING AWAY FR...</td>\n",
       "      <td>Benjamin Koellmann</td>\n",
       "      <td>In 2006  Benjamin Koellmann bought a condomini...</td>\n",
       "      <td>In 2006  [TGT] bought a condominium in Miami B...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3361</td>\n",
       "      <td>Paraguay leader sacks chief of armed forces</td>\n",
       "      <td>Fernando Lugo</td>\n",
       "      <td>Lugo  a former Catholic bishop who assumed off...</td>\n",
       "      <td>Lugo  a former Catholic bishop who assumed off...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3362</td>\n",
       "      <td>Tennis: Nadal hopes to see Spain win World Cup</td>\n",
       "      <td>Rafael Nadal</td>\n",
       "      <td>Spanish Wimbledon winner Rafael Nadal said Sun...</td>\n",
       "      <td>[TGT] said Sunday [TGT] would love to go to th...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3363</td>\n",
       "      <td>White House picks new cyber coordinator</td>\n",
       "      <td>Howard A. Schmidt</td>\n",
       "      <td>In a letter posted on the White House web site...</td>\n",
       "      <td>In a letter posted on the White House web site...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3364</td>\n",
       "      <td>MORRIS PICKS HIS QB BUT KEEPS IT QUIET</td>\n",
       "      <td>MORRIS</td>\n",
       "      <td>TAMPA  At least Raheem Morris finally has the ...</td>\n",
       "      <td>TAMPA  At least Raheem Morris finally has the ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOCUMENT_INDEX                                              TITLE  \\\n",
       "0            3360  AS VALUES DROP MORE HOMEOWNERS WALKING AWAY FR...   \n",
       "1            3361        Paraguay leader sacks chief of armed forces   \n",
       "2            3362     Tennis: Nadal hopes to see Spain win World Cup   \n",
       "3            3363            White House picks new cyber coordinator   \n",
       "4            3364             MORRIS PICKS HIS QB BUT KEEPS IT QUIET   \n",
       "\n",
       "        TARGET_ENTITY                                           DOCUMENT  \\\n",
       "0  Benjamin Koellmann  In 2006  Benjamin Koellmann bought a condomini...   \n",
       "1       Fernando Lugo  Lugo  a former Catholic bishop who assumed off...   \n",
       "2        Rafael Nadal  Spanish Wimbledon winner Rafael Nadal said Sun...   \n",
       "3   Howard A. Schmidt  In a letter posted on the White House web site...   \n",
       "4              MORRIS  TAMPA  At least Raheem Morris finally has the ...   \n",
       "\n",
       "                                     MASKED_DOCUMENT TRUE_SENTIMENT  \\\n",
       "0  In 2006  [TGT] bought a condominium in Miami B...        Neutral   \n",
       "1  Lugo  a former Catholic bishop who assumed off...       Positive   \n",
       "2  [TGT] said Sunday [TGT] would love to go to th...       Positive   \n",
       "3  In a letter posted on the White House web site...       Positive   \n",
       "4  TAMPA  At least Raheem Morris finally has the ...       Positive   \n",
       "\n",
       "  Paragraph0 Paragraph1 Paragraph2 Paragraph3  ... Paragraph6 Paragraph7  \\\n",
       "0    Neutral    Neutral    Neutral    Neutral  ...        NaN        NaN   \n",
       "1   Positive   Positive    Neutral    Neutral  ...        NaN        NaN   \n",
       "2   Positive   Positive   Positive    Neutral  ...        NaN        NaN   \n",
       "3   Positive    Neutral   Positive    Neutral  ...        NaN        NaN   \n",
       "4    Neutral   Positive    Neutral    Neutral  ...        NaN        NaN   \n",
       "\n",
       "  Paragraph8 Paragraph9 Paragraph10 Paragraph11 Paragraph12 Paragraph13  \\\n",
       "0        NaN        NaN         NaN         NaN         NaN         NaN   \n",
       "1        NaN        NaN         NaN         NaN         NaN         NaN   \n",
       "2        NaN        NaN         NaN         NaN         NaN         NaN   \n",
       "3        NaN        NaN         NaN         NaN         NaN         NaN   \n",
       "4        NaN        NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "  Paragraph14 Paragraph15  \n",
       "0         NaN         NaN  \n",
       "1         NaN         NaN  \n",
       "2         NaN         NaN  \n",
       "3         NaN         NaN  \n",
       "4         NaN         NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Remember to add aspect information for all three data splits: train.csv, dev.csv, random_test.csv\n",
    "Current utilized data split is dev.csv\n",
    "'''\n",
    "\n",
    "train_data = pd.read_csv(\"../datasets/persent/dev.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "abbebd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 219 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 150 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 377 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 191 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 158 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 323 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 254 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 215 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 221 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 228 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 149 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 672 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 643 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 947 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 376 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 1115 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 1032 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 1090 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 1023 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 981 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 527 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 267 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 1331 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 539 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 182 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 246 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 209 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 203 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 237 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 32 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 235 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 462 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 81 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 365 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 415 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 16 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 214 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 1254 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 180 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 66 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 39 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 185 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 110 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 240 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 305 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 316 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 687 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 645 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 325 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 499 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 1744 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 317 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 677 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 128 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 179 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 227 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 352 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 155 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 733 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 168 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 716 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 603 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 165 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 108 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 517 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 77 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 143 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 494 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 747 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 739 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 262 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 533 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 333 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 861 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 362 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 1071 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 380 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 136 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 266 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 211 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 697 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 840 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 268 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 253 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 274 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 206 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 356 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 1008 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 368 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 1039 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 364 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 218 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 823 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 177 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 1184 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 847 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 247 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 552 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 555 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 294 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 766 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 586 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 256 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 286 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 245 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 236 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 196 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 401 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 249 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 215 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 204 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 208 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 611 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 812 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 200 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 483 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 436 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 375 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 633 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 802 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 166 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 896 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 197 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 626 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 298 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 397 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 271 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 127 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 609 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 331 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 756 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Initializing global attention on CLS token...\n",
      "Input ids are automatically padded from 146 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-759a3485f7ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prob_max'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DOCUMENT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_max_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DOCUMENT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4432\u001b[0m         \"\"\"\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4435\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1135\u001b[0m                 \u001b[0;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m                 \u001b[0;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1138\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-759a3485f7ef>\u001b[0m in \u001b[0;36mget_max_prob\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_max_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0mglobal_attention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1887\u001b[0;31m         outputs = self.longformer(\n\u001b[0m\u001b[1;32m   1888\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1705\u001b[0m         )\n\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1707\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1708\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1287\u001b[0m                 )\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1290\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m     ):\n\u001b[0;32m-> 1213\u001b[0;31m         self_attn_outputs = self.attention(\n\u001b[0m\u001b[1;32m   1214\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m         )\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_names = data_train.target_names\n",
    "\n",
    "def get_prediction(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=2048, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    return target_names[probs.argmax()]\n",
    "\n",
    "def get_max_prob(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=2048, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    return max(probs[0].tolist())\n",
    "\n",
    "train_data['prob_max'] = train_data['DOCUMENT'].apply(get_max_prob)\n",
    "train_data['topic'] = train_data['DOCUMENT'].apply(get_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82988f",
   "metadata": {},
   "source": [
    "# Map label to 6 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "434bd090",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_groups = {'computer': ['comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x'], 'recreation': ['rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey'], 'science': ['sci.crypt', 'sci.electronics', 'sci.med', 'sci.space'], 'sale': ['misc.forsale'], 'politics': ['talk.politics.misc', 'talk.politics.guns', 'talk.politics.mideast'], 'religion': ['talk.religion.misc', 'alt.atheism', 'soc.religion.christian']}\n",
    "# train_data = pd.read_csv('../datasets/persent/random_test_longformer_topic.csv')\n",
    "\n",
    "topic_mapping = {}\n",
    "for key, val in topic_groups.items():\n",
    "    for sub in val:\n",
    "        topic_mapping[sub] = key\n",
    "        \n",
    "train_data['group'] = train_data['topic'].apply(lambda x: topic_mapping[x])\n",
    "\n",
    "def group_w_threshold(row):\n",
    "    if row['prob_max'] <0.5:\n",
    "        return 'general'\n",
    "    else:\n",
    "        return row['group']\n",
    "\n",
    "train_data['group'] = train_data.apply(group_w_threshold, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf1ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to switch the prefix of the file name: train, dev and random_test\n",
    "# Current prefix is dev\n",
    "\n",
    "train_data.to_csv('../datasets/persent/dev_longformer_topic_nocopy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f922e",
   "metadata": {},
   "source": [
    "# Create 4-aspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a6889af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data4 = train_data.loc[train_data['group'].isin(['politics', 'general', 'recreation', 'science'])]\n",
    "\n",
    "train_data4.reset_index(inplace=True)\n",
    "train_data4.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "train_data4_new = train_data4[['DOCUMENT_INDEX', 'TITLE', 'TARGET_ENTITY', 'DOCUMENT', 'MASKED_DOCUMENT']]\n",
    "train_data4_new = train_data4_new.loc[train_data4_new.index.repeat(4)]\n",
    "groups = ['politics', 'general', 'recreation', 'science'] * (train_data4_new.shape[0] // 4)\n",
    "train_data4_new['group'] = groups\n",
    "train_data4_new['prob_max'] = np.nan\n",
    "train_data4_new['sentiment'] = np.nan\n",
    "train_data4_new.reset_index(inplace=True)\n",
    "train_data4_new.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "for i in range(train_data4.shape[0]):\n",
    "    temp_ind = train_data4_new.loc[(train_data4_new['DOCUMENT_INDEX']==train_data4['DOCUMENT_INDEX'][i]) & (train_data4_new['group']==train_data4['group'][i])].index[0]\n",
    "    train_data4_new.loc[temp_ind, 'sentiment'] = train_data4['TRUE_SENTIMENT'][i]\n",
    "    train_data4_new.loc[temp_ind, 'prob_max'] = train_data4['prob_max'][i]\n",
    "    \n",
    "train_data4_new['sentiment'].fillna('None', inplace=True)\n",
    "train_data4_new['context'] = '[TGT] - '+ train_data4_new['group']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31526db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to switch the prefix of the file name: train, dev and random_test\n",
    "# Current prefix is dev\n",
    "\n",
    "train_data4_new.to_csv('../datasets/persent/dev_longformer_4topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478ba9e",
   "metadata": {},
   "source": [
    "# Create 7-aspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77a9d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data7 = train_data[['DOCUMENT_INDEX', 'TITLE', 'TARGET_ENTITY', 'DOCUMENT', 'MASKED_DOCUMENT', 'group', 'TRUE_SENTIMENT', 'prob_max']]\n",
    "train_data7_new = train_data7.loc[train_data7.index.repeat(7)]\n",
    "groups = ['politics', 'recreation', 'computer', 'religion', 'science', 'sale', 'general'] * (train_data7_new.shape[0] // 7)\n",
    "train_data7_new['group'] = groups\n",
    "train_data7_new['prob_max'] = np.nan\n",
    "train_data7_new['sentiment'] = np.nan\n",
    "train_data7_new.reset_index(inplace=True)\n",
    "train_data7_new.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "for i in range(train_data7.shape[0]):\n",
    "    temp_ind = train_data7_new.loc[(train_data7_new['DOCUMENT_INDEX']==train_data7['DOCUMENT_INDEX'][i]) & (train_data7_new['group']==train_data7['group'][i])].index[0]\n",
    "    train_data7_new.loc[temp_ind, 'sentiment'] = train_data7['TRUE_SENTIMENT'][i]\n",
    "    train_data7_new.loc[temp_ind, 'prob_max'] = train_data7['prob_max'][i]\n",
    "    \n",
    "train_data7_new['sentiment'].fillna('None', inplace=True)\n",
    "train_data7_new['context'] = '[TGT] - '+ train_data7_new['group']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461afec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to switch the prefix of the file name: train, dev and random_test\n",
    "# Current prefix is dev\n",
    "\n",
    "train_data7_new.to_csv('../datasets/persent/dev_longformer_7topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1fbe93",
   "metadata": {},
   "source": [
    "# Create paragraph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c0e4d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_para_generation(data, data_w_para):\n",
    "    \n",
    "    data['paras'] = data['MASKED_DOCUMENT'].apply(lambda x: x.split('\\n'))\n",
    "    paras = list(data['paras'])\n",
    "    \n",
    "    paras1, sents, aspects = [], [], []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        current_doc_id =  data['DOCUMENT_INDEX'][i]\n",
    "        if len(paras[i]) > 16:\n",
    "            sents.extend(data_w_para.iloc[data_w_para.loc[data_w_para['DOCUMENT_INDEX']==current_doc_id].index[0], 6:22])\n",
    "            aspects.extend([data['group'][i]]*16)\n",
    "            paras1.extend(paras[i][:16])\n",
    "        else:\n",
    "            sents.extend(data_w_para.iloc[data_w_para.loc[data_w_para['DOCUMENT_INDEX']==current_doc_id].index[0], 6:6+len(paras[i])])\n",
    "            aspects.extend([data['group'][i]]*len(paras[i]))\n",
    "            paras1.extend(paras[i])\n",
    "    df2 = pd.DataFrame({'Paragraph':paras1, 'Sentiment':sents, 'Aspects':aspects})                            \n",
    "    df2['Word_Count'] = df2['Paragraph'].apply(lambda x: len(x.split()))\n",
    "    df2['Contains_Target'] = df2['Paragraph'].apply(lambda x: '[TGT]' in x)\n",
    "    df2 = df2.loc[(df2['Word_Count']<=128) & (df2['Contains_Target'])]\n",
    "    df2.reset_index(inplace=True, drop=True)\n",
    "    df2['Aspects'] = df2['Aspects'].astype(str)\n",
    "    df2.drop(columns=['Word_Count', 'Contains_Target'], inplace=True)\n",
    "    df2['Context'] = '[TGT] - '+ df2['Aspects']\n",
    "    \n",
    "    df2.dropna(inplace=True)\n",
    "    df2.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return df2\n",
    "\n",
    "data_para4 = aspect_para_generation(train_data4_new, train_data)\n",
    "data_para7 = aspect_para_generation(train_data7_new, train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd80e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to switch the prefix of the file name: train, dev and random_test\n",
    "# Current prefix is dev\n",
    "\n",
    "data_para4.to_csv('../datasets/persent/dev_para_4aspects_128tokens.csv', index=False)\n",
    "data_para7.to_csv('../datasets/persent/dev_para_7aspects_128tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11312fa3",
   "metadata": {},
   "source": [
    "# Create dataset for pure Longformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "118ddf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_longformer_generation(data):\n",
    "    df1 = data.loc[data.index.repeat(4)]\n",
    "\n",
    "    labels = ['None', 'Neutral', 'Negative', 'Positive']\n",
    "    label_map = dict(zip(labels, range(4)))\n",
    "\n",
    "    df1['sentiment'] = labels * (df1.shape[0] // 4)\n",
    "    df1['label'] = np.nan\n",
    "    df1.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        temp_label = label_map[data.loc[i, 'sentiment']]\n",
    "        df1.loc[i * 4 + temp_label, 'label'] = 1\n",
    "        \n",
    "    df1['label'].fillna(0, inplace=True)\n",
    "    df1['auxi_sent'] = 'the polarity of the aspect ' + df1['group'] + ' of [TGT] is ' + df1['sentiment'] + '.'\n",
    "    return df1\n",
    "\n",
    "data_long4 = aspect_longformer_generation(train_data4_new)\n",
    "data_long7 = aspect_longformer_generation(train_data7_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to switch the prefix of the file name: train, dev and random_test\n",
    "# Current prefix is dev\n",
    "\n",
    "data_long4.to_csv('../datasets/persent/dev_longformer_4topics_auxiliary.csv', index=False)\n",
    "data_long7.to_csv('../datasets/persent/dev_longformer_7topics_auxiliary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c5182a",
   "metadata": {},
   "source": [
    "# View data distribution regarding aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "21460fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = pd.read_csv('../datasets/persent/dev_longformer_topic_nocopy.csv')\n",
    "df2 = pd.read_csv('../datasets/persent/train_longformer_topic_nocopy.csv')\n",
    "df3 = pd.read_csv('../datasets/persent/random_test_longformer_topic_nocopy.csv')\n",
    "\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "df_new = df.groupby(by=['group', 'TRUE_SENTIMENT']).count()\n",
    "df_new.reset_index(inplace=True)\n",
    "df_new = df_new[['group', 'TRUE_SENTIMENT', 'DOCUMENT_INDEX']]\n",
    "df_new.columns = ['aspect', 'sentiment', 'count']\n",
    "\n",
    "df_new2 = df.groupby(by=['group']).count()\n",
    "df_new2.reset_index(inplace=True)\n",
    "df_new2 = df_new2[['group', 'DOCUMENT_INDEX']]\n",
    "df_new2.columns = ['aspect', 'count']\n",
    "df_new2.sort_values(by=['count'], inplace=True)\n",
    "\n",
    "fig, ax  = plt.subplots(1,1, figsize = (10,5))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.barplot(x='aspect', y='count', data=df_new, hue='sentiment', order=list(df_new2.aspect))\n",
    "# plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=45)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60538ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
